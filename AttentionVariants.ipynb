{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "aa2d5b42",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/butlerju/Library/Python/3.9/lib/python/site-packages/urllib3/__init__.py:35: NotOpenSSLWarning: urllib3 v2 only supports OpenSSL 1.1.1+, currently the 'ssl' module is compiled with 'LibreSSL 2.8.3'. See: https://github.com/urllib3/urllib3/issues/3020\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import requests, math"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "1b7e6b4b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================\n",
    "# 1. Load Tiny Shakespeare text\n",
    "# =============================\n",
    "url = \"https://raw.githubusercontent.com/karpathy/char-rnn/master/data/tinyshakespeare/input.txt\"\n",
    "text = requests.get(url).text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "63891fde",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total words: 202651\n",
      "Vocab size: 25670\n",
      "Sample tokens: ['First', 'Citizen:', 'Before', 'we', 'proceed', 'any', 'further,', 'hear', 'me', 'speak.', 'All:', 'Speak,', 'speak.', 'First', 'Citizen:', 'You', 'are', 'all', 'resolved', 'rather']\n",
      "Train words: 182385\n",
      "Val words: 20266\n"
     ]
    }
   ],
   "source": [
    "# =============================\n",
    "# 2. Word-level tokenization\n",
    "# =============================\n",
    "# Simple whitespace tokenizer (teaching-friendly)\n",
    "words = text.split()\n",
    "\n",
    "# Build vocab\n",
    "vocab = sorted(set(words))\n",
    "stoi = {w: i for i, w in enumerate(vocab)}       # word → index\n",
    "itos = {i: w for w, i in stoi.items()}           # index → word\n",
    "vocab_size = len(vocab)\n",
    "\n",
    "print(\"Total words:\", len(words))\n",
    "print(\"Vocab size:\", vocab_size)\n",
    "print(\"Sample tokens:\", words[:20])\n",
    "\n",
    "# =============================\n",
    "# 3. Convert entire corpus to integer IDs\n",
    "# =============================\n",
    "data = torch.tensor([stoi[w] for w in words], dtype=torch.long)\n",
    "\n",
    "# Train/validation split\n",
    "split = int(0.9 * len(data))\n",
    "train_data = data[:split]\n",
    "val_data   = data[split:]\n",
    "\n",
    "print(\"Train words:\", len(train_data))\n",
    "print(\"Val words:\", len(val_data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e2db7fde",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================\n",
    "# 4. Batching function for training\n",
    "# =============================\n",
    "def get_batch(split, block_size, batch_size, device):\n",
    "    source = train_data if split == \"train\" else val_data\n",
    "    # pick random starting word positions\n",
    "    ids = torch.randint(0, len(source) - block_size - 1, (batch_size,))\n",
    "    x = torch.stack([source[i:i+block_size] for i in ids]).to(device)\n",
    "    y = torch.stack([source[i+1:i+block_size+1] for i in ids]).to(device)\n",
    "    return x, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "8cafb4b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# 2. Attention variants (simple)\n",
    "# ============================================================\n",
    "\n",
    "class ScaledDotProductAttention(nn.Module):\n",
    "    def __init__(self, n_embd, n_head):\n",
    "        super().__init__()\n",
    "        self.n_head = n_head\n",
    "        self.d = n_embd // n_head\n",
    "        self.qkv = nn.Linear(n_embd, 3*n_embd, bias=False)\n",
    "        self.out = nn.Linear(n_embd, n_embd)\n",
    "\n",
    "    def forward(self, x):\n",
    "        B, T, C = x.shape\n",
    "        qkv = self.qkv(x).reshape(B, T, 3, self.n_head, self.d).permute(2,0,3,1,4)\n",
    "        q, k, v = qkv[0], qkv[1], qkv[2]\n",
    "\n",
    "        att = (q @ k.transpose(-2,-1)) / math.sqrt(self.d)\n",
    "        mask = torch.tril(torch.ones(T, T, device=x.device)) == 1\n",
    "        att = att.masked_fill(~mask, float('-inf'))\n",
    "        att = att.softmax(dim=-1)\n",
    "\n",
    "        out = att @ v\n",
    "        out = out.transpose(1,2).reshape(B,T,C)\n",
    "        return self.out(out)\n",
    "\n",
    "class FlashAttention(nn.Module):\n",
    "    \"\"\"Uses PyTorch 2 scaled_dot_product_attention (Flash when available).\"\"\"\n",
    "    def __init__(self, n_embd, n_head):\n",
    "        super().__init__()\n",
    "        self.n_head = n_head\n",
    "        self.d = n_embd // n_head\n",
    "        self.qkv = nn.Linear(n_embd, 3*n_embd, bias=False)\n",
    "        self.out = nn.Linear(n_embd, n_embd)\n",
    "\n",
    "    def forward(self, x):\n",
    "        B, T, C = x.shape\n",
    "        qkv = self.qkv(x).reshape(B, T, 3, self.n_head, self.d).permute(2,0,3,1,4)\n",
    "        q, k, v = qkv[0], qkv[1], qkv[2]\n",
    "        out = F.scaled_dot_product_attention(q, k, v, is_causal=True)\n",
    "        out = out.transpose(1,2).reshape(B,T,C)\n",
    "        return self.out(out)\n",
    "\n",
    "# ============================\n",
    "# Local (sliding-window) attention\n",
    "# ============================\n",
    "class LocalAttention(nn.Module):\n",
    "    \"\"\"\n",
    "    Causal sliding-window attention.\n",
    "    Each position attends only to the previous `window` tokens (and itself).\n",
    "    \"\"\"\n",
    "    def __init__(self, n_embd, n_head, window: int = 64):\n",
    "        super().__init__()\n",
    "        assert n_embd % n_head == 0\n",
    "        self.n_head = n_head\n",
    "        self.d = n_embd // n_head\n",
    "        self.window = window\n",
    "        self.qkv = nn.Linear(n_embd, 3 * n_embd, bias=False)\n",
    "        self.out = nn.Linear(n_embd, n_embd)\n",
    "\n",
    "    def forward(self, x):\n",
    "        B, T, C = x.shape\n",
    "        qkv = self.qkv(x).reshape(B, T, 3, self.n_head, self.d).permute(2, 0, 3, 1, 4)\n",
    "        q, k, v = qkv[0], qkv[1], qkv[2]     # (B, H, T, Dh)\n",
    "\n",
    "        # raw attention scores\n",
    "        att = (q @ k.transpose(-2, -1)) / math.sqrt(self.d)  # (B, H, T, T)\n",
    "\n",
    "        # build causal+local mask: j<=i and i-j<=window\n",
    "        device = x.device\n",
    "        i = torch.arange(T, device=device).unsqueeze(1)\n",
    "        j = torch.arange(T, device=device).unsqueeze(0)\n",
    "        causal = j <= i\n",
    "        local = (i - j) <= self.window\n",
    "        mask = causal & local                 # (T, T) bool\n",
    "\n",
    "        att = att.masked_fill(~mask, float('-inf'))\n",
    "        att = att.softmax(dim=-1)\n",
    "        out = att @ v                         # (B, H, T, Dh)\n",
    "        out = out.transpose(1, 2).reshape(B, T, C)\n",
    "        return self.out(out)\n",
    "\n",
    "# ============================\n",
    "# Linear (kernelized) attention (pedagogical)\n",
    "# ============================\n",
    "class LinearAttention(nn.Module):\n",
    "    \"\"\"\n",
    "    Simple causal linear attention with φ(x)=ELU(x)+1 feature map.\n",
    "    Pedagogical prefix-scan implementation (not the most optimized).\n",
    "    \"\"\"\n",
    "    def __init__(self, n_embd, n_head):\n",
    "        super().__init__()\n",
    "        assert n_embd % n_head == 0\n",
    "        self.n_head = n_head\n",
    "        self.d = n_embd // n_head\n",
    "        self.Wq = nn.Linear(n_embd, n_embd, bias=False)\n",
    "        self.Wk = nn.Linear(n_embd, n_embd, bias=False)\n",
    "        self.Wv = nn.Linear(n_embd, n_embd, bias=False)\n",
    "        self.out = nn.Linear(n_embd, n_embd)\n",
    "        self.eps = 1e-6\n",
    "\n",
    "    @staticmethod\n",
    "    def phi(x):\n",
    "        return F.elu(x, alpha=1.0) + 1.0\n",
    "\n",
    "    def forward(self, x):\n",
    "        B, T, C = x.shape\n",
    "        H, Dh = self.n_head, self.d\n",
    "\n",
    "        q = self.Wq(x).reshape(B, T, H, Dh).transpose(1, 2)   # (B,H,T,Dh)\n",
    "        k = self.Wk(x).reshape(B, T, H, Dh).transpose(1, 2)\n",
    "        v = self.Wv(x).reshape(B, T, H, Dh).transpose(1, 2)\n",
    "\n",
    "        qφ = self.phi(q)\n",
    "        kφ = self.phi(k)\n",
    "\n",
    "        # prefix accumulators\n",
    "        K_accum  = torch.zeros(B, H, Dh, device=x.device, dtype=x.dtype)\n",
    "        KV_accum = torch.zeros(B, H, Dh, Dh, device=x.device, dtype=x.dtype)\n",
    "\n",
    "        outs = []\n",
    "        for t in range(T):\n",
    "            kt = kφ[:, :, t, :]                                    # (B,H,Dh)\n",
    "            vt = v[:, :, t, :]                                     # (B,H,Dh)\n",
    "            # accumulate\n",
    "            KV_accum = KV_accum + torch.einsum(\"bhd,bhe->bhde\", kt, vt)\n",
    "            K_accum  = K_accum  + kt\n",
    "            # query\n",
    "            qt = qφ[:, :, t, :]                                    # (B,H,Dh)\n",
    "            num = torch.einsum(\"bhd,bhde->bhe\", qt, KV_accum)      # (B,H,Dh)\n",
    "            den = torch.einsum(\"bhd,bhd->bh\", qt, K_accum).unsqueeze(-1) + self.eps\n",
    "            yt = num / den\n",
    "            outs.append(yt)\n",
    "\n",
    "        y = torch.stack(outs, dim=2).transpose(1, 2).reshape(B, T, C)  # (B,T,C)\n",
    "        return self.out(y)\n",
    "\n",
    "# ============================\n",
    "# Multi-Query Attention (MQA)\n",
    "# ============================\n",
    "class MultiQueryAttention(nn.Module):\n",
    "    \"\"\"\n",
    "    Multi-Query Attention: many query heads, shared K and V across heads.\n",
    "    Cuts memory bandwidth for K/V compared with full MHA.\n",
    "    \"\"\"\n",
    "    def __init__(self, n_embd, n_head):\n",
    "        super().__init__()\n",
    "        assert n_embd % n_head == 0\n",
    "        self.n_head = n_head\n",
    "        self.d = n_embd // n_head\n",
    "        self.Wq = nn.Linear(n_embd, n_embd, bias=False)  # (B,T,C) -> (B,T,H*Dh)\n",
    "        self.Wk = nn.Linear(n_embd, self.d, bias=False)  # shared K: (B,T,Dh)\n",
    "        self.Wv = nn.Linear(n_embd, self.d, bias=False)  # shared V: (B,T,Dh)\n",
    "        self.out = nn.Linear(n_embd, n_embd)\n",
    "\n",
    "    def forward(self, x):\n",
    "        B, T, C = x.shape\n",
    "        H, Dh = self.n_head, self.d\n",
    "\n",
    "        q = self.Wq(x).reshape(B, T, H, Dh).transpose(1, 2)      # (B,H,T,Dh)\n",
    "\n",
    "        # shared K/V (expand over heads)\n",
    "        k = self.Wk(x).unsqueeze(1).expand(B, H, T, Dh).contiguous()\n",
    "        v = self.Wv(x).unsqueeze(1).expand(B, H, T, Dh).contiguous()\n",
    "\n",
    "        # PyTorch's fused scaled dot-product attention (Flash when available)\n",
    "        y = F.scaled_dot_product_attention(q, k, v, is_causal=True)  # (B,H,T,Dh)\n",
    "        y = y.transpose(1, 2).reshape(B, T, C)\n",
    "        return self.out(y)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "de7cc866",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# 3. Tiny Transformer\n",
    "# ============================================================\n",
    "\n",
    "class Block(nn.Module):\n",
    "    def __init__(self, n_embd, n_head, attn_class):\n",
    "        super().__init__()\n",
    "        self.ln1 = nn.LayerNorm(n_embd)\n",
    "        self.attn = attn_class(n_embd, n_head)\n",
    "        self.ln2 = nn.LayerNorm(n_embd)\n",
    "        self.ff = nn.Sequential(\n",
    "            nn.Linear(n_embd, 4*n_embd),\n",
    "            nn.GELU(),\n",
    "            nn.Linear(4*n_embd, n_embd)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x + self.attn(self.ln1(x))\n",
    "        x = x + self.ff(self.ln2(x))\n",
    "        return x\n",
    "\n",
    "class TinyTransformer(nn.Module):\n",
    "    def __init__(self, vocab_size, block_size, n_layer=4, n_embd=256, n_head=4, attn_class=ScaledDotProductAttention):\n",
    "        super().__init__()\n",
    "        self.block_size = block_size\n",
    "        self.token = nn.Embedding(vocab_size, n_embd)\n",
    "        self.pos   = nn.Embedding(block_size, n_embd)\n",
    "        self.blocks = nn.Sequential(*[Block(n_embd, n_head, attn_class) for _ in range(n_layer)])\n",
    "        self.ln_f = nn.LayerNorm(n_embd)\n",
    "        self.head = nn.Linear(n_embd, vocab_size)\n",
    "\n",
    "    def forward(self, idx, targets=None):\n",
    "        B, T = idx.shape\n",
    "        pos = torch.arange(0, T, device=idx.device)\n",
    "        x = self.token(idx) + self.pos(pos)[None,:,:]\n",
    "        x = self.blocks(x)\n",
    "        x = self.ln_f(x)\n",
    "        logits = self.head(x)\n",
    "        if targets is None:\n",
    "            return logits\n",
    "        loss = F.cross_entropy(logits.view(-1, vocab_size), targets.view(-1))\n",
    "        return logits, loss\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e11f7310",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training...\n",
      "step 0 | loss 10.314\n",
      "step 100 | loss 6.919\n",
      "step 200 | loss 5.653\n",
      "step 300 | loss 4.630\n",
      "step 400 | loss 3.747\n"
     ]
    }
   ],
   "source": [
    "# ============================================================\n",
    "# 4. Training\n",
    "# ============================================================\n",
    "\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "block_size = 256\n",
    "batch_size = 64\n",
    "\n",
    "# Choose model: ScaledDotProductAttention OR FlashAttention\n",
    "attn_type = FlashAttention   # change this line\n",
    "\n",
    "model = TinyTransformer(vocab_size, block_size, attn_class=attn_type).to(device)\n",
    "optimizer = torch.optim.AdamW(model.parameters())\n",
    "\n",
    "steps = 1000\n",
    "print(\"Training...\")\n",
    "\n",
    "for step in range(steps):\n",
    "    xb, yb = get_batch(\"train\", block_size, batch_size, device)\n",
    "    logits, loss = model(xb, yb)\n",
    "\n",
    "    optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "    if step % 100 == 0:\n",
    "        print(f\"step {step} | loss {loss.item():.3f}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d74f907a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# 5. Sampling\n",
    "# ============================================================\n",
    "\n",
    "\n",
    "def sample(model, start=\"ROMEO:\", steps=100):\n",
    "    model.eval()\n",
    "\n",
    "    # Tokenize start prompt into words\n",
    "    start_words = start.split()\n",
    "\n",
    "    # Map words → IDs (use 0 for unknown words to avoid KeyErrors)\n",
    "    start_ids = [stoi.get(w, 0) for w in start_words]\n",
    "\n",
    "    # Build initial sequence tensor\n",
    "    idx = torch.tensor([start_ids], device=device)\n",
    "\n",
    "    for _ in range(steps):\n",
    "        # Crop context to block_size so position embeddings never overflow\n",
    "        idx_cond = idx[:, -model.block_size:]\n",
    "\n",
    "        with torch.no_grad():\n",
    "            logits = model(idx_cond)[:, -1, :]  # final word's logits\n",
    "\n",
    "        # Greedy decoding: pick highest‑probability word\n",
    "        next_id = torch.argmax(logits, dim=-1, keepdim=True)\n",
    "\n",
    "        # Append to sequence\n",
    "        idx = torch.cat([idx, next_id], dim=1)\n",
    "\n",
    "    # Convert token IDs back to words\n",
    "    result_words = [itos[int(i)] for i in idx[0]]\n",
    "    return \" \".join(result_words)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "print(\"\\n=== SAMPLE ===\")\n",
    "print(sample(model))\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
