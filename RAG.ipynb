{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3ba67f26",
   "metadata": {},
   "source": [
    "# Retrieval Augmented Generation (RAG)\n",
    "\n",
    "**Note:** These notes and code are based off of the code provided in [this GitHub repo](https://github.com/dinocodesx/simple-local-rag/tree/master). I highly suggest you take a look at it and his other works.\n",
    "\n",
    "## What is RAG?\n",
    "* RAG is a method to improve the outputs of large language models\n",
    "    * **Prevents hallucinations:** RAG provides factual information as inputs to an LLM. This prevents (or at least reduces) hallucinations\n",
    "    * **Allows LLMs to work with custom data:** RAG provides a quick method to have LLMs produce an outout on data outside of its training scope. This is much faster than the alternative which is to continue the training of the LLM on the custom data.\n",
    "* Overview: RAG combines the generative abilities of LLMs with relevant text from trusted sources. These can be used to make sure the information that are produced by the LLMs are correct or can be used to generate outputs using ideas or data the LLM was not trained on.\n",
    "\n",
    "## Breakdown\n",
    "1. **Retrieval**: An algorithm, based on embedding, is used to search a provided text to provide information within the text which is related to the query being passed to the LLM. In this example the provided text is a PDF copy of a textbook but in practice it can be any text retrieved from any source that is taken to be factual or true.\n",
    "2. **Augmentation**: Create a custom prompt for an LLM containing both the user query and the related information retrieved from the provided text which is taken to be factual.\n",
    "3. **Generation:** Use the LLM to generate an answer to the user query.\n",
    "\n",
    "## How it Works\n",
    "1. **Retrieve the known text**: Import the text into a format readable by Python. This can be from importing a PDF, scraping webpages, or any other method to import text into Python.\n",
    "2. **Chunk the text:** The next step is to divide the text into small(ish) pieces called chunks. There is no correct way to do this but you want chunks which are large enough that they will contain enough relevant information to augment the LLM's knowledge, but not so large that they contain unrelated information (or are so long they slow down the LLM's processing of the prompt).\n",
    "3. **Embedding:** Once the text is chunked, embed the chunks using an embedding method. This can be Word2Vec which we have discussed in class or a newer embedding method such as the one used in this notebook. This means that all of the chunks of text are now vectors, whose directions can tell us how related their contents are.\n",
    "4. **Retrieval:** Embed the user query in the same embedding space as the text. Determine which chunks are most related to the query using a similarity metric (which vectors point in similar directions to query). Save the top k chunks which are the most related.\n",
    "5. **Prompt Engineering:** Pass the user query and the chunks of related text to an LLM. Use a custom prompt to tell the LLM to answer the query using the related text as known truth.\n",
    "\n",
    "**NOTE:** If is possible to run the below notebook on a normal laptop, but it runs much faster (and you have access to larger LLM models) with an Nvidia GPU. If you do not have a GPU on your laptop or desktop you could transfer this notebook to Google Colab (note the education account is free) or the Kaggle."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "087e7d24",
   "metadata": {},
   "source": [
    "## Getting Started with Hugging Face\n",
    "\n",
    "Hugging Face contains a collection of pre-trained LLMs of various sizes. We will be using one of these models later in the code. First, you need to make an account with [Hugging Face](https://huggingface.co/). In order to use the LLMs you must have an account an a token from that account associated with your computer. After you create an account run the below code cell."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "22ccf452",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Requirement already satisfied: huggingface_hub in /Users/butlerju/Library/Python/3.13/lib/python/site-packages (0.36.0)\n",
      "Requirement already satisfied: filelock in /Users/butlerju/Library/Python/3.13/lib/python/site-packages (from huggingface_hub) (3.20.3)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in /Users/butlerju/Library/Python/3.13/lib/python/site-packages (from huggingface_hub) (2025.10.0)\n",
      "Requirement already satisfied: packaging>=20.9 in /Users/butlerju/Library/Python/3.13/lib/python/site-packages (from huggingface_hub) (26.0)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /Users/butlerju/Library/Python/3.13/lib/python/site-packages (from huggingface_hub) (6.0.3)\n",
      "Requirement already satisfied: requests in /Users/butlerju/Library/Python/3.13/lib/python/site-packages (from huggingface_hub) (2.32.5)\n",
      "Requirement already satisfied: tqdm>=4.42.1 in /Users/butlerju/Library/Python/3.13/lib/python/site-packages (from huggingface_hub) (4.67.1)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /Users/butlerju/Library/Python/3.13/lib/python/site-packages (from huggingface_hub) (4.15.0)\n",
      "Requirement already satisfied: hf-xet<2.0.0,>=1.1.3 in /Users/butlerju/Library/Python/3.13/lib/python/site-packages (from huggingface_hub) (1.2.0)\n",
      "Requirement already satisfied: charset_normalizer<4,>=2 in /Users/butlerju/Library/Python/3.13/lib/python/site-packages (from requests->huggingface_hub) (3.4.4)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /Users/butlerju/Library/Python/3.13/lib/python/site-packages (from requests->huggingface_hub) (3.11)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /Users/butlerju/Library/Python/3.13/lib/python/site-packages (from requests->huggingface_hub) (2.6.3)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /Users/butlerju/Library/Python/3.13/lib/python/site-packages (from requests->huggingface_hub) (2026.1.4)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a2667a6f90fb40099b7c5f9979c8536f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(HTML(value='<center> <img\\nsrc=https://huggingface.co/front/assets/huggingface_logo-noborder.sv…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Install the huggingface_hub package to interact with Hugging Face from Python.\n",
    "# Replace pip3 with pip as needed.\n",
    "!pip3 install huggingface_hub\n",
    "\n",
    "# Import the login function and call it to authenticate your Hugging Face account.\n",
    "from huggingface_hub import login\n",
    "\n",
    "# Call the login function to authenticate. This will prompt for your Hugging \n",
    "# Face token in the console below. There is a link to the webpage where you can\n",
    "# retrieve your token. In theory you should be able to paste it directly, but if \n",
    "# that doesn't work, try typing it out manually.\n",
    "login() \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a2582fd2",
   "metadata": {},
   "source": [
    "## RAG Code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "7916c446",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Requirement already satisfied: fitz in /Users/butlerju/Library/Python/3.13/lib/python/site-packages (0.0.1.dev2)\n",
      "Requirement already satisfied: frontend in /Users/butlerju/Library/Python/3.13/lib/python/site-packages (0.0.3)\n",
      "Requirement already satisfied: tools in /Users/butlerju/Library/Python/3.13/lib/python/site-packages (1.0.21)\n",
      "Requirement already satisfied: pymupdf in /Users/butlerju/Library/Python/3.13/lib/python/site-packages (1.26.7)\n",
      "Requirement already satisfied: spacy in /Users/butlerju/Library/Python/3.13/lib/python/site-packages (3.8.11)\n",
      "Requirement already satisfied: sentence_transformers in /Users/butlerju/Library/Python/3.13/lib/python/site-packages (5.2.0)\n",
      "Requirement already satisfied: tf_keras in /Users/butlerju/Library/Python/3.13/lib/python/site-packages (2.20.1)\n",
      "Requirement already satisfied: bitsandbytes in /Users/butlerju/Library/Python/3.13/lib/python/site-packages (0.49.1)\n",
      "Requirement already satisfied: accelerate in /Users/butlerju/Library/Python/3.13/lib/python/site-packages (1.12.0)\n",
      "Requirement already satisfied: configobj in /Users/butlerju/Library/Python/3.13/lib/python/site-packages (from fitz) (5.0.9)\n",
      "Requirement already satisfied: configparser in /Users/butlerju/Library/Python/3.13/lib/python/site-packages (from fitz) (7.2.0)\n",
      "Requirement already satisfied: httplib2 in /Users/butlerju/Library/Python/3.13/lib/python/site-packages (from fitz) (0.31.1)\n",
      "Requirement already satisfied: nibabel in /Users/butlerju/Library/Python/3.13/lib/python/site-packages (from fitz) (5.3.3)\n",
      "Requirement already satisfied: nipype in /Users/butlerju/Library/Python/3.13/lib/python/site-packages (from fitz) (1.10.0)\n",
      "Requirement already satisfied: numpy in /Users/butlerju/Library/Python/3.13/lib/python/site-packages (from fitz) (2.4.1)\n",
      "Requirement already satisfied: pandas in /Users/butlerju/Library/Python/3.13/lib/python/site-packages (from fitz) (3.0.0)\n",
      "Requirement already satisfied: pyxnat in /Users/butlerju/Library/Python/3.13/lib/python/site-packages (from fitz) (1.6.4)\n",
      "Requirement already satisfied: scipy in /Users/butlerju/Library/Python/3.13/lib/python/site-packages (from fitz) (1.17.0)\n",
      "Requirement already satisfied: starlette>=0.12.0 in /Users/butlerju/Library/Python/3.13/lib/python/site-packages (from frontend) (0.52.1)\n",
      "Requirement already satisfied: uvicorn>=0.7.1 in /Users/butlerju/Library/Python/3.13/lib/python/site-packages (from frontend) (0.40.0)\n",
      "Requirement already satisfied: itsdangerous>=1.1.0 in /Users/butlerju/Library/Python/3.13/lib/python/site-packages (from frontend) (2.2.0)\n",
      "Requirement already satisfied: aiofiles in /Users/butlerju/Library/Python/3.13/lib/python/site-packages (from frontend) (25.1.0)\n",
      "Requirement already satisfied: spacy-legacy<3.1.0,>=3.0.11 in /Users/butlerju/Library/Python/3.13/lib/python/site-packages (from spacy) (3.0.12)\n",
      "Requirement already satisfied: spacy-loggers<2.0.0,>=1.0.0 in /Users/butlerju/Library/Python/3.13/lib/python/site-packages (from spacy) (1.0.5)\n",
      "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /Users/butlerju/Library/Python/3.13/lib/python/site-packages (from spacy) (1.0.15)\n",
      "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /Users/butlerju/Library/Python/3.13/lib/python/site-packages (from spacy) (2.0.13)\n",
      "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in /Users/butlerju/Library/Python/3.13/lib/python/site-packages (from spacy) (3.0.12)\n",
      "Requirement already satisfied: thinc<8.4.0,>=8.3.4 in /Users/butlerju/Library/Python/3.13/lib/python/site-packages (from spacy) (8.3.10)\n",
      "Requirement already satisfied: wasabi<1.2.0,>=0.9.1 in /Users/butlerju/Library/Python/3.13/lib/python/site-packages (from spacy) (1.1.3)\n",
      "Requirement already satisfied: srsly<3.0.0,>=2.4.3 in /Users/butlerju/Library/Python/3.13/lib/python/site-packages (from spacy) (2.5.2)\n",
      "Requirement already satisfied: catalogue<2.1.0,>=2.0.6 in /Users/butlerju/Library/Python/3.13/lib/python/site-packages (from spacy) (2.0.10)\n",
      "Requirement already satisfied: weasel<0.5.0,>=0.4.2 in /Users/butlerju/Library/Python/3.13/lib/python/site-packages (from spacy) (0.4.3)\n",
      "Requirement already satisfied: typer-slim<1.0.0,>=0.3.0 in /Users/butlerju/Library/Python/3.13/lib/python/site-packages (from spacy) (0.21.1)\n",
      "Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in /Users/butlerju/Library/Python/3.13/lib/python/site-packages (from spacy) (4.67.1)\n",
      "Requirement already satisfied: requests<3.0.0,>=2.13.0 in /Users/butlerju/Library/Python/3.13/lib/python/site-packages (from spacy) (2.32.5)\n",
      "Requirement already satisfied: pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4 in /Users/butlerju/Library/Python/3.13/lib/python/site-packages (from spacy) (2.12.5)\n",
      "Requirement already satisfied: jinja2 in /Users/butlerju/Library/Python/3.13/lib/python/site-packages (from spacy) (3.1.6)\n",
      "Requirement already satisfied: setuptools in /Users/butlerju/Library/Python/3.13/lib/python/site-packages (from spacy) (80.10.1)\n",
      "Requirement already satisfied: packaging>=20.0 in /Users/butlerju/Library/Python/3.13/lib/python/site-packages (from spacy) (26.0)\n",
      "Requirement already satisfied: annotated-types>=0.6.0 in /Users/butlerju/Library/Python/3.13/lib/python/site-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy) (0.7.0)\n",
      "Requirement already satisfied: pydantic-core==2.41.5 in /Users/butlerju/Library/Python/3.13/lib/python/site-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy) (2.41.5)\n",
      "Requirement already satisfied: typing-extensions>=4.14.1 in /Users/butlerju/Library/Python/3.13/lib/python/site-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy) (4.15.0)\n",
      "Requirement already satisfied: typing-inspection>=0.4.2 in /Users/butlerju/Library/Python/3.13/lib/python/site-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy) (0.4.2)\n",
      "Requirement already satisfied: charset_normalizer<4,>=2 in /Users/butlerju/Library/Python/3.13/lib/python/site-packages (from requests<3.0.0,>=2.13.0->spacy) (3.4.4)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /Users/butlerju/Library/Python/3.13/lib/python/site-packages (from requests<3.0.0,>=2.13.0->spacy) (3.11)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /Users/butlerju/Library/Python/3.13/lib/python/site-packages (from requests<3.0.0,>=2.13.0->spacy) (2.6.3)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /Users/butlerju/Library/Python/3.13/lib/python/site-packages (from requests<3.0.0,>=2.13.0->spacy) (2026.1.4)\n",
      "Requirement already satisfied: blis<1.4.0,>=1.3.0 in /Users/butlerju/Library/Python/3.13/lib/python/site-packages (from thinc<8.4.0,>=8.3.4->spacy) (1.3.3)\n",
      "Requirement already satisfied: confection<1.0.0,>=0.0.1 in /Users/butlerju/Library/Python/3.13/lib/python/site-packages (from thinc<8.4.0,>=8.3.4->spacy) (0.1.5)\n",
      "Requirement already satisfied: click>=8.0.0 in /Users/butlerju/Library/Python/3.13/lib/python/site-packages (from typer-slim<1.0.0,>=0.3.0->spacy) (8.3.1)\n",
      "Requirement already satisfied: cloudpathlib<1.0.0,>=0.7.0 in /Users/butlerju/Library/Python/3.13/lib/python/site-packages (from weasel<0.5.0,>=0.4.2->spacy) (0.23.0)\n",
      "Requirement already satisfied: smart-open<8.0.0,>=5.2.1 in /Users/butlerju/Library/Python/3.13/lib/python/site-packages (from weasel<0.5.0,>=0.4.2->spacy) (7.5.0)\n",
      "Requirement already satisfied: wrapt in /Users/butlerju/Library/Python/3.13/lib/python/site-packages (from smart-open<8.0.0,>=5.2.1->weasel<0.5.0,>=0.4.2->spacy) (2.0.1)\n",
      "Requirement already satisfied: transformers<6.0.0,>=4.41.0 in /Users/butlerju/Library/Python/3.13/lib/python/site-packages (from sentence_transformers) (4.57.6)\n",
      "Requirement already satisfied: torch>=1.11.0 in /Users/butlerju/Library/Python/3.13/lib/python/site-packages (from sentence_transformers) (2.10.0)\n",
      "Requirement already satisfied: scikit-learn in /Users/butlerju/Library/Python/3.13/lib/python/site-packages (from sentence_transformers) (1.8.0)\n",
      "Requirement already satisfied: huggingface-hub>=0.20.0 in /Users/butlerju/Library/Python/3.13/lib/python/site-packages (from sentence_transformers) (0.36.0)\n",
      "Requirement already satisfied: filelock in /Users/butlerju/Library/Python/3.13/lib/python/site-packages (from transformers<6.0.0,>=4.41.0->sentence_transformers) (3.20.3)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /Users/butlerju/Library/Python/3.13/lib/python/site-packages (from transformers<6.0.0,>=4.41.0->sentence_transformers) (6.0.3)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /Users/butlerju/Library/Python/3.13/lib/python/site-packages (from transformers<6.0.0,>=4.41.0->sentence_transformers) (2026.1.15)\n",
      "Requirement already satisfied: tokenizers<=0.23.0,>=0.22.0 in /Users/butlerju/Library/Python/3.13/lib/python/site-packages (from transformers<6.0.0,>=4.41.0->sentence_transformers) (0.22.2)\n",
      "Requirement already satisfied: safetensors>=0.4.3 in /Users/butlerju/Library/Python/3.13/lib/python/site-packages (from transformers<6.0.0,>=4.41.0->sentence_transformers) (0.7.0)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in /Users/butlerju/Library/Python/3.13/lib/python/site-packages (from huggingface-hub>=0.20.0->sentence_transformers) (2025.10.0)\n",
      "Requirement already satisfied: hf-xet<2.0.0,>=1.1.3 in /Users/butlerju/Library/Python/3.13/lib/python/site-packages (from huggingface-hub>=0.20.0->sentence_transformers) (1.2.0)\n",
      "Requirement already satisfied: tensorflow<2.21,>=2.20 in /Users/butlerju/Library/Python/3.13/lib/python/site-packages (from tf_keras) (2.20.0)\n",
      "Requirement already satisfied: absl-py>=1.0.0 in /Users/butlerju/Library/Python/3.13/lib/python/site-packages (from tensorflow<2.21,>=2.20->tf_keras) (2.3.1)\n",
      "Requirement already satisfied: astunparse>=1.6.0 in /Users/butlerju/Library/Python/3.13/lib/python/site-packages (from tensorflow<2.21,>=2.20->tf_keras) (1.6.3)\n",
      "Requirement already satisfied: flatbuffers>=24.3.25 in /Users/butlerju/Library/Python/3.13/lib/python/site-packages (from tensorflow<2.21,>=2.20->tf_keras) (25.12.19)\n",
      "Requirement already satisfied: gast!=0.5.0,!=0.5.1,!=0.5.2,>=0.2.1 in /Users/butlerju/Library/Python/3.13/lib/python/site-packages (from tensorflow<2.21,>=2.20->tf_keras) (0.7.0)\n",
      "Requirement already satisfied: google_pasta>=0.1.1 in /Users/butlerju/Library/Python/3.13/lib/python/site-packages (from tensorflow<2.21,>=2.20->tf_keras) (0.2.0)\n",
      "Requirement already satisfied: libclang>=13.0.0 in /Users/butlerju/Library/Python/3.13/lib/python/site-packages (from tensorflow<2.21,>=2.20->tf_keras) (18.1.1)\n",
      "Requirement already satisfied: opt_einsum>=2.3.2 in /Users/butlerju/Library/Python/3.13/lib/python/site-packages (from tensorflow<2.21,>=2.20->tf_keras) (3.4.0)\n",
      "Requirement already satisfied: protobuf>=5.28.0 in /Users/butlerju/Library/Python/3.13/lib/python/site-packages (from tensorflow<2.21,>=2.20->tf_keras) (6.33.4)\n",
      "Requirement already satisfied: six>=1.12.0 in /Users/butlerju/Library/Python/3.13/lib/python/site-packages (from tensorflow<2.21,>=2.20->tf_keras) (1.17.0)\n",
      "Requirement already satisfied: termcolor>=1.1.0 in /Users/butlerju/Library/Python/3.13/lib/python/site-packages (from tensorflow<2.21,>=2.20->tf_keras) (3.3.0)\n",
      "Requirement already satisfied: grpcio<2.0,>=1.24.3 in /Users/butlerju/Library/Python/3.13/lib/python/site-packages (from tensorflow<2.21,>=2.20->tf_keras) (1.76.0)\n",
      "Requirement already satisfied: tensorboard~=2.20.0 in /Users/butlerju/Library/Python/3.13/lib/python/site-packages (from tensorflow<2.21,>=2.20->tf_keras) (2.20.0)\n",
      "Requirement already satisfied: keras>=3.10.0 in /Users/butlerju/Library/Python/3.13/lib/python/site-packages (from tensorflow<2.21,>=2.20->tf_keras) (3.13.1)\n",
      "Requirement already satisfied: h5py>=3.11.0 in /Users/butlerju/Library/Python/3.13/lib/python/site-packages (from tensorflow<2.21,>=2.20->tf_keras) (3.15.1)\n",
      "Requirement already satisfied: ml_dtypes<1.0.0,>=0.5.1 in /Users/butlerju/Library/Python/3.13/lib/python/site-packages (from tensorflow<2.21,>=2.20->tf_keras) (0.5.4)\n",
      "Requirement already satisfied: markdown>=2.6.8 in /Users/butlerju/Library/Python/3.13/lib/python/site-packages (from tensorboard~=2.20.0->tensorflow<2.21,>=2.20->tf_keras) (3.10.1)\n",
      "Requirement already satisfied: pillow in /Users/butlerju/Library/Python/3.13/lib/python/site-packages (from tensorboard~=2.20.0->tensorflow<2.21,>=2.20->tf_keras) (12.1.0)\n",
      "Requirement already satisfied: tensorboard-data-server<0.8.0,>=0.7.0 in /Users/butlerju/Library/Python/3.13/lib/python/site-packages (from tensorboard~=2.20.0->tensorflow<2.21,>=2.20->tf_keras) (0.7.2)\n",
      "Requirement already satisfied: werkzeug>=1.0.1 in /Users/butlerju/Library/Python/3.13/lib/python/site-packages (from tensorboard~=2.20.0->tensorflow<2.21,>=2.20->tf_keras) (3.1.5)\n",
      "Requirement already satisfied: sympy>=1.13.3 in /Users/butlerju/Library/Python/3.13/lib/python/site-packages (from torch>=1.11.0->sentence_transformers) (1.14.0)\n",
      "Requirement already satisfied: networkx>=2.5.1 in /Users/butlerju/Library/Python/3.13/lib/python/site-packages (from torch>=1.11.0->sentence_transformers) (3.6.1)\n",
      "Requirement already satisfied: psutil in /Users/butlerju/Library/Python/3.13/lib/python/site-packages (from accelerate) (7.2.1)\n",
      "Requirement already satisfied: wheel<1.0,>=0.23.0 in /Users/butlerju/Library/Python/3.13/lib/python/site-packages (from astunparse>=1.6.0->tensorflow<2.21,>=2.20->tf_keras) (0.46.3)\n",
      "Requirement already satisfied: rich in /Users/butlerju/Library/Python/3.13/lib/python/site-packages (from keras>=3.10.0->tensorflow<2.21,>=2.20->tf_keras) (14.2.0)\n",
      "Requirement already satisfied: namex in /Users/butlerju/Library/Python/3.13/lib/python/site-packages (from keras>=3.10.0->tensorflow<2.21,>=2.20->tf_keras) (0.1.0)\n",
      "Requirement already satisfied: optree in /Users/butlerju/Library/Python/3.13/lib/python/site-packages (from keras>=3.10.0->tensorflow<2.21,>=2.20->tf_keras) (0.18.0)\n",
      "Requirement already satisfied: anyio<5,>=3.6.2 in /Users/butlerju/Library/Python/3.13/lib/python/site-packages (from starlette>=0.12.0->frontend) (4.12.1)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /Users/butlerju/Library/Python/3.13/lib/python/site-packages (from sympy>=1.13.3->torch>=1.11.0->sentence_transformers) (1.3.0)\n",
      "Requirement already satisfied: h11>=0.8 in /Users/butlerju/Library/Python/3.13/lib/python/site-packages (from uvicorn>=0.7.1->frontend) (0.16.0)\n",
      "Requirement already satisfied: markupsafe>=2.1.1 in /Users/butlerju/Library/Python/3.13/lib/python/site-packages (from werkzeug>=1.0.1->tensorboard~=2.20.0->tensorflow<2.21,>=2.20->tf_keras) (3.0.3)\n",
      "Requirement already satisfied: pyparsing<4,>=3.0.4 in /Users/butlerju/Library/Python/3.13/lib/python/site-packages (from httplib2->fitz) (3.3.2)\n",
      "Requirement already satisfied: prov>=1.5.2 in /Users/butlerju/Library/Python/3.13/lib/python/site-packages (from nipype->fitz) (2.1.1)\n",
      "Requirement already satisfied: pydot>=1.2.3 in /Users/butlerju/Library/Python/3.13/lib/python/site-packages (from nipype->fitz) (4.0.1)\n",
      "Requirement already satisfied: python-dateutil>=2.2 in /Users/butlerju/Library/Python/3.13/lib/python/site-packages (from nipype->fitz) (2.9.0.post0)\n",
      "Requirement already satisfied: rdflib>=5.0.0 in /Users/butlerju/Library/Python/3.13/lib/python/site-packages (from nipype->fitz) (7.5.0)\n",
      "Requirement already satisfied: simplejson>=3.8.0 in /Users/butlerju/Library/Python/3.13/lib/python/site-packages (from nipype->fitz) (3.20.2)\n",
      "Requirement already satisfied: traits>=6.2 in /Users/butlerju/Library/Python/3.13/lib/python/site-packages (from nipype->fitz) (7.1.0)\n",
      "Requirement already satisfied: acres in /Users/butlerju/Library/Python/3.13/lib/python/site-packages (from nipype->fitz) (0.5.0)\n",
      "Requirement already satisfied: etelemetry>=0.3.1 in /Users/butlerju/Library/Python/3.13/lib/python/site-packages (from nipype->fitz) (0.3.1)\n",
      "Requirement already satisfied: looseversion!=1.2 in /Users/butlerju/Library/Python/3.13/lib/python/site-packages (from nipype->fitz) (1.3.0)\n",
      "Requirement already satisfied: puremagic in /Users/butlerju/Library/Python/3.13/lib/python/site-packages (from nipype->fitz) (1.30)\n",
      "Requirement already satisfied: ci-info>=0.2 in /Users/butlerju/Library/Python/3.13/lib/python/site-packages (from etelemetry>=0.3.1->nipype->fitz) (0.3.0)\n",
      "Requirement already satisfied: lxml>=4.3 in /Users/butlerju/Library/Python/3.13/lib/python/site-packages (from pyxnat->fitz) (6.0.2)\n",
      "Requirement already satisfied: pathlib>=1.0 in /Users/butlerju/Library/Python/3.13/lib/python/site-packages (from pyxnat->fitz) (1.0.1)\n",
      "Requirement already satisfied: markdown-it-py>=2.2.0 in /Users/butlerju/Library/Python/3.13/lib/python/site-packages (from rich->keras>=3.10.0->tensorflow<2.21,>=2.20->tf_keras) (4.0.0)\n",
      "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /Users/butlerju/Library/Python/3.13/lib/python/site-packages (from rich->keras>=3.10.0->tensorflow<2.21,>=2.20->tf_keras) (2.19.2)\n",
      "Requirement already satisfied: mdurl~=0.1 in /Users/butlerju/Library/Python/3.13/lib/python/site-packages (from markdown-it-py>=2.2.0->rich->keras>=3.10.0->tensorflow<2.21,>=2.20->tf_keras) (0.1.2)\n",
      "Requirement already satisfied: joblib>=1.3.0 in /Users/butlerju/Library/Python/3.13/lib/python/site-packages (from scikit-learn->sentence_transformers) (1.5.3)\n",
      "Requirement already satisfied: threadpoolctl>=3.2.0 in /Users/butlerju/Library/Python/3.13/lib/python/site-packages (from scikit-learn->sentence_transformers) (3.6.0)\n"
     ]
    }
   ],
   "source": [
    "###################\n",
    "## INSTALLATIONS ##\n",
    "###################\n",
    "# Install required packages. These are likely new installations for most of you. If you\n",
    "# have issues with any of the libraries, please let me know. You may also want to make\n",
    "# a virtual environment for this installation process. If you do so, add numpy and\n",
    "# torch to the install list.\n",
    "\n",
    "# Change from pip3 to pip as needed. NOTE: Your Python version must be 3.10 or higher for\n",
    "# the bitsandbytes library to work properly.\n",
    "! pip3 install fitz frontend tools pymupdf spacy sentence_transformers tf_keras bitsandbytes accelerate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "1c797816",
   "metadata": {},
   "outputs": [],
   "source": [
    "#############\n",
    "## IMPORTS ##\n",
    "#############\n",
    "# Alphabetical ordered imports for all libraries used in this notebook.\n",
    "import numpy as np\n",
    "import os\n",
    "import pandas as pd\n",
    "# Imports PDFs into Python\n",
    "import pymupdf \n",
    "import random\n",
    "# Regular expressions (used for text cleaning)\n",
    "import re\n",
    "# Used to pull the pdf from the web\n",
    "import requests\n",
    "# Used to create sentence embeddings\n",
    "from sentence_transformers import SentenceTransformer, util\n",
    "# Used to split the text into chunks/sentences\n",
    "from spacy.lang.en import English \n",
    "# For displaying long text outputs\n",
    "import textwrap\n",
    "# For timing code execution\n",
    "from time import perf_counter as timer\n",
    "# Transformer libraries for LLMs\n",
    "import torch\n",
    "# Progress bar for loops\n",
    "from tqdm.auto import tqdm\n",
    "# Hugging Face transformers \n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM, BitsAndBytesConfig\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d958ef2",
   "metadata": {},
   "source": [
    "### Retrieval\n",
    "\n",
    "First, we need to be able to retrieve relevant text from a document in order to augment the results from the LLM. Below we will use the concepts of embedding to determine which portions of text are the most relevant to our question on the text."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "0dd0cadc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "File human-nutrition-text.pdf exists.\n"
     ]
    }
   ],
   "source": [
    "###############################\n",
    "## DOWNLOAD THE PDF DOCUMENT ##\n",
    "###############################\n",
    "\n",
    "# We will be using the human nutrition textbook provided in the original GitHub\n",
    "# as our document for this example. If you have your own PDF document you would \n",
    "# like to use, feel free to change this. You will get better results if the document\n",
    "# is longer and more complex. You can also use multiple documents if you like.\n",
    "\n",
    "# If you want to change the document you will also need to change the below code \n",
    "# depending on if your document is already downloaded or not.\n",
    "\n",
    "# Get PDF document\n",
    "pdf_path = \"human-nutrition-text.pdf\"\n",
    "\n",
    "# Download PDF if it doesn't already exist\n",
    "if not os.path.exists(pdf_path):\n",
    "  print(\"File doesn't exist, downloading...\")\n",
    "\n",
    "  # The URL of the PDF you want to download\n",
    "  url = \"https://pressbooks.oer.hawaii.edu/humannutrition2/open/download?type=pdf\"\n",
    "\n",
    "  # The local filename to save the downloaded file\n",
    "  filename = pdf_path\n",
    "\n",
    "  # Send a GET request to the URL\n",
    "  response = requests.get(url)\n",
    "\n",
    "  # Check if the request was successful\n",
    "  if response.status_code == 200:\n",
    "      # Open a file in binary write mode and save the content to it\n",
    "      with open(filename, \"wb\") as file:\n",
    "          file.write(response.content)\n",
    "      print(f\"The file has been downloaded and saved as {filename}\")\n",
    "  else:\n",
    "      print(f\"Failed to download the file. Status code: {response.status_code}\")\n",
    "else:\n",
    "  print(f\"File {pdf_path} exists.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c3c4402c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ef8b2b53dcfc4e49898063ba419c1d7d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "[{'page_number': -41,\n",
       "  'page_char_count': 29,\n",
       "  'page_word_count': 4,\n",
       "  'page_sentence_count_raw': 1,\n",
       "  'page_token_count': 7.25,\n",
       "  'text': 'Human Nutrition: 2020 Edition'},\n",
       " {'page_number': -40,\n",
       "  'page_char_count': 0,\n",
       "  'page_word_count': 1,\n",
       "  'page_sentence_count_raw': 1,\n",
       "  'page_token_count': 0.0,\n",
       "  'text': ''}]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "################################\n",
    "## IMPORT AND FORMAT PDF TEXT ##\n",
    "################################\n",
    "\n",
    "def text_formatter(text: str) -> str:\n",
    "    \"\"\"Performs minor formatting on text.\"\"\"\n",
    "    # note: this might be different for each doc (best to experiment)\n",
    "    cleaned_text = text.replace(\"\\n\", \" \").strip() \n",
    "\n",
    "    # Other potential text formatting functions can go here. If you change the document\n",
    "    # being used, you may need to modify this function.\n",
    "    return cleaned_text\n",
    "\n",
    "# Open PDF and get lines/pages\n",
    "# Note: this only focuses on text, rather than images/figures etc\n",
    "def open_and_read_pdf(pdf_path: str) -> list[dict]:\n",
    "    \"\"\"\n",
    "    Opens a PDF file, reads its text content page by page, and collects statistics.\n",
    "\n",
    "    Parameters:\n",
    "        pdf_path (str): The file path to the PDF document to be opened and read.\n",
    "\n",
    "    Returns:\n",
    "        list[dict]: A list of dictionaries, each containing the page number\n",
    "        (adjusted), character count, word count, sentence count, token count, and the extracted text\n",
    "        for each page.\n",
    "    \"\"\"\n",
    "    doc = pymupdf.open(pdf_path)  # open a document\n",
    "    pages_and_texts = []\n",
    "    for page_number, page in tqdm(enumerate(doc)):  # iterate the document pages\n",
    "        text = page.get_text()  # get plain text encoded as UTF-8\n",
    "        text = text_formatter(text)\n",
    "        pages_and_texts.append({\"page_number\": page_number - 41,  # adjust page numbers since our PDF starts on page 42\n",
    "                                \"page_char_count\": len(text),\n",
    "                                \"page_word_count\": len(text.split(\" \")),\n",
    "                                \"page_sentence_count_raw\": len(text.split(\". \")),\n",
    "                                \"page_token_count\": len(text) / 4,  # 1 token = ~4 chars, see: https://help.openai.com/en/articles/4936856-what-are-tokens-and-how-to-count-them\n",
    "                                \"text\": text})\n",
    "    return pages_and_texts\n",
    "\n",
    "# Read the PDF and get the pages and texts, show the first two entries\n",
    "pages_and_texts = open_and_read_pdf(pdf_path=pdf_path)\n",
    "pages_and_texts[:2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "1afa318d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'page_number': 566,\n",
       "  'page_char_count': 114,\n",
       "  'page_word_count': 24,\n",
       "  'page_sentence_count_raw': 1,\n",
       "  'page_token_count': 28.5,\n",
       "  'text': 'Image by  Allison  Calabrese /  CC BY 4.0  Figure 9.13 Niacin Deficiency, Pellagra  566  |  Water-Soluble Vitamins'},\n",
       " {'page_number': 183,\n",
       "  'page_char_count': 165,\n",
       "  'page_word_count': 37,\n",
       "  'page_sentence_count_raw': 2,\n",
       "  'page_token_count': 41.25,\n",
       "  'text': 'Sodium  levels in  milligrams is  a required  listing on a  Nutrition  Facts label.  Sodium on the Nutrition Facts Panel  Figure 3.10 Nutrition Label  Sodium  |  183'},\n",
       " {'page_number': 215,\n",
       "  'page_char_count': 195,\n",
       "  'page_word_count': 33,\n",
       "  'page_sentence_count_raw': 2,\n",
       "  'page_token_count': 48.75,\n",
       "  'text': 'An interactive or media element has been  excluded from this version of the text. You can  view it online here:  http://pressbooks.oer.hawaii.edu/ humannutrition2/?p=162  \\xa0 Water Concerns  |  215'}]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "##########################\n",
    "## DISPLAY RANDOM PAGES ##\n",
    "##########################\n",
    "\n",
    "# Change the k value to display more or fewer random pages\n",
    "random.sample(pages_and_texts, k=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "a9e4e4b7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>page_number</th>\n",
       "      <th>page_char_count</th>\n",
       "      <th>page_word_count</th>\n",
       "      <th>page_sentence_count_raw</th>\n",
       "      <th>page_token_count</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>1208.00000</td>\n",
       "      <td>1208.000000</td>\n",
       "      <td>1208.000000</td>\n",
       "      <td>1208.000000</td>\n",
       "      <td>1208.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>562.50000</td>\n",
       "      <td>1148.004139</td>\n",
       "      <td>198.299669</td>\n",
       "      <td>9.972682</td>\n",
       "      <td>287.001035</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>348.86387</td>\n",
       "      <td>560.382275</td>\n",
       "      <td>95.759336</td>\n",
       "      <td>6.187226</td>\n",
       "      <td>140.095569</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>-41.00000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>260.75000</td>\n",
       "      <td>762.000000</td>\n",
       "      <td>134.000000</td>\n",
       "      <td>4.000000</td>\n",
       "      <td>190.500000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>562.50000</td>\n",
       "      <td>1231.500000</td>\n",
       "      <td>214.500000</td>\n",
       "      <td>10.000000</td>\n",
       "      <td>307.875000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>864.25000</td>\n",
       "      <td>1603.500000</td>\n",
       "      <td>271.000000</td>\n",
       "      <td>14.000000</td>\n",
       "      <td>400.875000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>1166.00000</td>\n",
       "      <td>2308.000000</td>\n",
       "      <td>429.000000</td>\n",
       "      <td>32.000000</td>\n",
       "      <td>577.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       page_number  page_char_count  page_word_count  page_sentence_count_raw  \\\n",
       "count   1208.00000      1208.000000      1208.000000              1208.000000   \n",
       "mean     562.50000      1148.004139       198.299669                 9.972682   \n",
       "std      348.86387       560.382275        95.759336                 6.187226   \n",
       "min      -41.00000         0.000000         1.000000                 1.000000   \n",
       "25%      260.75000       762.000000       134.000000                 4.000000   \n",
       "50%      562.50000      1231.500000       214.500000                10.000000   \n",
       "75%      864.25000      1603.500000       271.000000                14.000000   \n",
       "max     1166.00000      2308.000000       429.000000                32.000000   \n",
       "\n",
       "       page_token_count  \n",
       "count       1208.000000  \n",
       "mean         287.001035  \n",
       "std          140.095569  \n",
       "min            0.000000  \n",
       "25%          190.500000  \n",
       "50%          307.875000  \n",
       "75%          400.875000  \n",
       "max          577.000000  "
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "######################################\n",
    "## STATISTICAL SUMMARY OF PDF PAGES ##\n",
    "#######################################\n",
    "\n",
    "# Convert the list of dictionaries to a DataFrame and display summary statistics\n",
    "# This helps us understand the distribution of text lengths across the pages.\n",
    "\n",
    "# This is important because we will be embedding the text later. Embedding models\n",
    "# have maximum input lengths, so we need to understand how long the text chunks are.\n",
    "# The embedding model we will be using later has a maximum input length of 384 tokens.\n",
    "# The text is currently chunked by the page, but it seems that the average number\n",
    "# of tokens per page is 287, but the 75% percentile is 400 tokens, which is above \n",
    "# the input limit. This means we will need to further chunk the text later. Note that \n",
    "# 1 token is approximately 4 characters, or 0.75 words on average.\n",
    "\n",
    "pd.DataFrame(pages_and_texts).describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "89b5ac0f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[This is a sentence., This another sentence.]\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "dd2f283cbfa3493690e299b05cd261fc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1208 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "[{'page_number': 819,\n",
       "  'page_char_count': 746,\n",
       "  'page_word_count': 134,\n",
       "  'page_sentence_count_raw': 8,\n",
       "  'page_token_count': 186.5,\n",
       "  'text': 'breastfed infants, 6 months of age is a good time to introduce  sources of highly bioavailable iron and zinc such as baby meats.  Iron-fortified cereals and beans can boost the iron intake as well.  Fluids  Infants have a high need for fluids, 1.5 milliliters per kilocalorie  consumed compared to 1.0 milliliters per kilocalorie consumed for  adults. This is because children have larger body surface area per  unit of body weight and a higher metabolic rate. Therefore, they are  at greater risk of dehydration. However, parents or other caregivers  can meet an infant’s fluid needs with breast milk or formula. As  solids are introduced, parents must make sure that young children  continue to drink fluids throughout the day.  Infancy  |  819',\n",
       "  'sentences': ['breastfed infants, 6 months of age is a good time to introduce  sources of highly bioavailable iron and zinc such as baby meats.',\n",
       "   ' Iron-fortified cereals and beans can boost the iron intake as well.',\n",
       "   ' Fluids  Infants have a high need for fluids, 1.5 milliliters per kilocalorie  consumed compared to 1.0 milliliters per kilocalorie consumed for  adults.',\n",
       "   'This is because children have larger body surface area per  unit of body weight and a higher metabolic rate.',\n",
       "   'Therefore, they are  at greater risk of dehydration.',\n",
       "   'However, parents or other caregivers  can meet an infant’s fluid needs with breast milk or formula.',\n",
       "   'As  solids are introduced, parents must make sure that young children  continue to drink fluids throughout the day.',\n",
       "   ' Infancy  |  819'],\n",
       "  'page_sentence_count_spacy': 8}]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "###################################\n",
    "## SPLIT THE TEXT INTO SENTENCES ##\n",
    "###################################\n",
    "# We will use the spaCy library to split the text into sentences. Later we will group\n",
    "# the sentences into chunks that fit within the embedding model's input limits.\n",
    "nlp = English()\n",
    "\n",
    "# Add a sentencizer pipeline, see https://spacy.io/api/sentencizer/\n",
    "nlp.add_pipe(\"sentencizer\")\n",
    "\n",
    "# EXAMPLE\n",
    "# Create a document instance as an example\n",
    "doc = nlp(\"This is a sentence. This another sentence.\")\n",
    "# Access the sentences of the document\n",
    "print(list(doc.sents))\n",
    "\n",
    "# For every page, split the text into sentences and store them back in the \n",
    "# pages_and_texts list\n",
    "for item in tqdm(pages_and_texts):\n",
    "    item[\"sentences\"] = list(nlp(item[\"text\"]).sents)\n",
    "\n",
    "    # Make sure all sentences are strings\n",
    "    item[\"sentences\"] = [str(sentence) for sentence in item[\"sentences\"]]\n",
    "\n",
    "    # Count the sentences\n",
    "    item[\"page_sentence_count_spacy\"] = len(item[\"sentences\"])\n",
    "\n",
    "# Display a random page to see the sentences.\n",
    "random.sample(pages_and_texts, k=1)    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "7e43562c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Inspect an example\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "e6c7fff2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>page_number</th>\n",
       "      <th>page_char_count</th>\n",
       "      <th>page_word_count</th>\n",
       "      <th>page_sentence_count_raw</th>\n",
       "      <th>page_token_count</th>\n",
       "      <th>page_sentence_count_spacy</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>1208.00000</td>\n",
       "      <td>1208.000000</td>\n",
       "      <td>1208.000000</td>\n",
       "      <td>1208.000000</td>\n",
       "      <td>1208.000000</td>\n",
       "      <td>1208.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>562.50000</td>\n",
       "      <td>1148.004139</td>\n",
       "      <td>198.299669</td>\n",
       "      <td>9.972682</td>\n",
       "      <td>287.001035</td>\n",
       "      <td>10.319536</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>348.86387</td>\n",
       "      <td>560.382275</td>\n",
       "      <td>95.759336</td>\n",
       "      <td>6.187226</td>\n",
       "      <td>140.095569</td>\n",
       "      <td>6.300843</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>-41.00000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>260.75000</td>\n",
       "      <td>762.000000</td>\n",
       "      <td>134.000000</td>\n",
       "      <td>4.000000</td>\n",
       "      <td>190.500000</td>\n",
       "      <td>5.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>562.50000</td>\n",
       "      <td>1231.500000</td>\n",
       "      <td>214.500000</td>\n",
       "      <td>10.000000</td>\n",
       "      <td>307.875000</td>\n",
       "      <td>10.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>864.25000</td>\n",
       "      <td>1603.500000</td>\n",
       "      <td>271.000000</td>\n",
       "      <td>14.000000</td>\n",
       "      <td>400.875000</td>\n",
       "      <td>15.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>1166.00000</td>\n",
       "      <td>2308.000000</td>\n",
       "      <td>429.000000</td>\n",
       "      <td>32.000000</td>\n",
       "      <td>577.000000</td>\n",
       "      <td>28.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       page_number  page_char_count  page_word_count  page_sentence_count_raw  \\\n",
       "count   1208.00000      1208.000000      1208.000000              1208.000000   \n",
       "mean     562.50000      1148.004139       198.299669                 9.972682   \n",
       "std      348.86387       560.382275        95.759336                 6.187226   \n",
       "min      -41.00000         0.000000         1.000000                 1.000000   \n",
       "25%      260.75000       762.000000       134.000000                 4.000000   \n",
       "50%      562.50000      1231.500000       214.500000                10.000000   \n",
       "75%      864.25000      1603.500000       271.000000                14.000000   \n",
       "max     1166.00000      2308.000000       429.000000                32.000000   \n",
       "\n",
       "       page_token_count  page_sentence_count_spacy  \n",
       "count       1208.000000                1208.000000  \n",
       "mean         287.001035                  10.319536  \n",
       "std          140.095569                   6.300843  \n",
       "min            0.000000                   0.000000  \n",
       "25%          190.500000                   5.000000  \n",
       "50%          307.875000                  10.000000  \n",
       "75%          400.875000                  15.000000  \n",
       "max          577.000000                  28.000000  "
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Check updated statistics after sentence splitting. There is a new column\n",
    "# for the number of sentences based on spaCy's sentencizer.\n",
    "pd.DataFrame(pages_and_texts).describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "96fa8e86",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "78f61dd194ff4cf693f7315a7d6e8795",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1208 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "##############\n",
    "## CHUNKING ##\n",
    "##############\n",
    "# Define split size to turn groups of sentences into chunks. This number may need\n",
    "# to be adjusted based on the document and the embedding model being used. Try different\n",
    "# values and see how it affects the chunk sizes and the results.\n",
    "num_sentence_chunk_size = 10\n",
    "\n",
    "# Create a function that recursively splits a list into desired sizes\n",
    "def split_list(input_list: list,\n",
    "               slice_size: int) -> list[list[str]]:\n",
    "    \"\"\"\n",
    "    Splits the input_list into sublists of size slice_size (or as close as possible).\n",
    "\n",
    "    For example, a list of 17 sentences would be split into two lists of [[10], [7]]\n",
    "    \"\"\"\n",
    "    return [input_list[i:i + slice_size] for i in range(0, len(input_list), slice_size)]\n",
    "\n",
    "# Loop through pages and texts and split sentences into chunks\n",
    "for item in tqdm(pages_and_texts):\n",
    "    item[\"sentence_chunks\"] = split_list(input_list=item[\"sentences\"],\n",
    "                                         slice_size=num_sentence_chunk_size)\n",
    "    item[\"num_chunks\"] = len(item[\"sentence_chunks\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "e97d0a66",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>page_number</th>\n",
       "      <th>page_char_count</th>\n",
       "      <th>page_word_count</th>\n",
       "      <th>page_sentence_count_raw</th>\n",
       "      <th>page_token_count</th>\n",
       "      <th>page_sentence_count_spacy</th>\n",
       "      <th>num_chunks</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>1208.00000</td>\n",
       "      <td>1208.000000</td>\n",
       "      <td>1208.000000</td>\n",
       "      <td>1208.000000</td>\n",
       "      <td>1208.000000</td>\n",
       "      <td>1208.000000</td>\n",
       "      <td>1208.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>562.50000</td>\n",
       "      <td>1148.004139</td>\n",
       "      <td>198.299669</td>\n",
       "      <td>9.972682</td>\n",
       "      <td>287.001035</td>\n",
       "      <td>10.319536</td>\n",
       "      <td>1.525662</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>348.86387</td>\n",
       "      <td>560.382275</td>\n",
       "      <td>95.759336</td>\n",
       "      <td>6.187226</td>\n",
       "      <td>140.095569</td>\n",
       "      <td>6.300843</td>\n",
       "      <td>0.644397</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>-41.00000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>260.75000</td>\n",
       "      <td>762.000000</td>\n",
       "      <td>134.000000</td>\n",
       "      <td>4.000000</td>\n",
       "      <td>190.500000</td>\n",
       "      <td>5.000000</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>562.50000</td>\n",
       "      <td>1231.500000</td>\n",
       "      <td>214.500000</td>\n",
       "      <td>10.000000</td>\n",
       "      <td>307.875000</td>\n",
       "      <td>10.000000</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>864.25000</td>\n",
       "      <td>1603.500000</td>\n",
       "      <td>271.000000</td>\n",
       "      <td>14.000000</td>\n",
       "      <td>400.875000</td>\n",
       "      <td>15.000000</td>\n",
       "      <td>2.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>1166.00000</td>\n",
       "      <td>2308.000000</td>\n",
       "      <td>429.000000</td>\n",
       "      <td>32.000000</td>\n",
       "      <td>577.000000</td>\n",
       "      <td>28.000000</td>\n",
       "      <td>3.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       page_number  page_char_count  page_word_count  page_sentence_count_raw  \\\n",
       "count   1208.00000      1208.000000      1208.000000              1208.000000   \n",
       "mean     562.50000      1148.004139       198.299669                 9.972682   \n",
       "std      348.86387       560.382275        95.759336                 6.187226   \n",
       "min      -41.00000         0.000000         1.000000                 1.000000   \n",
       "25%      260.75000       762.000000       134.000000                 4.000000   \n",
       "50%      562.50000      1231.500000       214.500000                10.000000   \n",
       "75%      864.25000      1603.500000       271.000000                14.000000   \n",
       "max     1166.00000      2308.000000       429.000000                32.000000   \n",
       "\n",
       "       page_token_count  page_sentence_count_spacy   num_chunks  \n",
       "count       1208.000000                1208.000000  1208.000000  \n",
       "mean         287.001035                  10.319536     1.525662  \n",
       "std          140.095569                   6.300843     0.644397  \n",
       "min            0.000000                   0.000000     0.000000  \n",
       "25%          190.500000                   5.000000     1.000000  \n",
       "50%          307.875000                  10.000000     1.000000  \n",
       "75%          400.875000                  15.000000     2.000000  \n",
       "max          577.000000                  28.000000     3.000000  "
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Generate new statistics after chunking, now with chunk counts\n",
    "pd.DataFrame(pages_and_texts).describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "47b5c674",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2a83c28ad2c647ea9bec2fa0a3ac4d7f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1208 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "1843"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "##########################\n",
    "## SPLITTING THE CHUNKS ##\n",
    "##########################\n",
    "# Split each chunk into its own item. The data is currently separated by pages, but that is not\n",
    "# needed now that we have the chunks.\n",
    "pages_and_chunks = []\n",
    "for item in tqdm(pages_and_texts):\n",
    "    for sentence_chunk in item[\"sentence_chunks\"]:\n",
    "        chunk_dict = {}\n",
    "        chunk_dict[\"page_number\"] = item[\"page_number\"]\n",
    "\n",
    "        # Join the sentences together into a paragraph-like structure, aka a chunk (so they are a single string)\n",
    "        joined_sentence_chunk = \"\".join(sentence_chunk).replace(\"  \", \" \").strip()\n",
    "        joined_sentence_chunk = re.sub(r'\\.([A-Z])', r'. \\1', joined_sentence_chunk) # \".A\" -> \". A\" for any full-stop/capital letter combo\n",
    "        chunk_dict[\"sentence_chunk\"] = joined_sentence_chunk\n",
    "\n",
    "        # Get stats about the chunk\n",
    "        chunk_dict[\"chunk_char_count\"] = len(joined_sentence_chunk)\n",
    "        chunk_dict[\"chunk_word_count\"] = len([word for word in joined_sentence_chunk.split(\" \")])\n",
    "        chunk_dict[\"chunk_token_count\"] = len(joined_sentence_chunk) / 4 # 1 token = ~4 characters\n",
    "\n",
    "        pages_and_chunks.append(chunk_dict)\n",
    "\n",
    "# How many chunks do we have?\n",
    "len(pages_and_chunks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "d196b4a3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>page_number</th>\n",
       "      <th>chunk_char_count</th>\n",
       "      <th>chunk_word_count</th>\n",
       "      <th>chunk_token_count</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>1843.000000</td>\n",
       "      <td>1843.000000</td>\n",
       "      <td>1843.000000</td>\n",
       "      <td>1843.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>583.381443</td>\n",
       "      <td>734.442756</td>\n",
       "      <td>112.333152</td>\n",
       "      <td>183.610689</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>347.788670</td>\n",
       "      <td>447.541546</td>\n",
       "      <td>71.220313</td>\n",
       "      <td>111.885387</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>-41.000000</td>\n",
       "      <td>12.000000</td>\n",
       "      <td>3.000000</td>\n",
       "      <td>3.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>280.500000</td>\n",
       "      <td>315.000000</td>\n",
       "      <td>44.000000</td>\n",
       "      <td>78.750000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>586.000000</td>\n",
       "      <td>746.000000</td>\n",
       "      <td>114.000000</td>\n",
       "      <td>186.500000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>890.000000</td>\n",
       "      <td>1118.500000</td>\n",
       "      <td>173.000000</td>\n",
       "      <td>279.625000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>1166.000000</td>\n",
       "      <td>1831.000000</td>\n",
       "      <td>297.000000</td>\n",
       "      <td>457.750000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       page_number  chunk_char_count  chunk_word_count  chunk_token_count\n",
       "count  1843.000000       1843.000000       1843.000000        1843.000000\n",
       "mean    583.381443        734.442756        112.333152         183.610689\n",
       "std     347.788670        447.541546         71.220313         111.885387\n",
       "min     -41.000000         12.000000          3.000000           3.000000\n",
       "25%     280.500000        315.000000         44.000000          78.750000\n",
       "50%     586.000000        746.000000        114.000000         186.500000\n",
       "75%     890.000000       1118.500000        173.000000         279.625000\n",
       "max    1166.000000       1831.000000        297.000000         457.750000"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Generate a final set of statistics about the chunks\n",
    "pd.DataFrame(pages_and_chunks).describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "a9beb27e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sentence: The Sentences Transformers library provides an easy and open-source way to create embeddings.\n",
      "Embedding: [-2.07983255e-02  3.03164460e-02 -2.01218147e-02  6.86484799e-02\n",
      " -2.55256239e-02 -8.47688317e-03 -2.07214689e-04 -6.32378086e-02\n",
      "  2.81606764e-02 -3.33354436e-02  3.02633625e-02  5.30721843e-02\n",
      " -5.03527448e-02  2.62288898e-02  3.33314165e-02 -4.51577418e-02\n",
      "  3.63045111e-02 -1.37122045e-03 -1.20170712e-02  1.14947269e-02\n",
      "  5.04510924e-02  4.70856726e-02  2.11913958e-02  5.14606722e-02\n",
      " -2.03747228e-02 -3.58889922e-02 -6.67759916e-04 -2.94394121e-02\n",
      "  4.95859236e-02 -1.05640236e-02 -1.52013823e-02 -1.31756917e-03\n",
      "  4.48197611e-02  1.56023633e-02  8.60379600e-07 -1.21388992e-03\n",
      " -2.37977933e-02 -9.09364375e-04  7.34490482e-03 -2.53936765e-03\n",
      "  5.23370318e-02 -4.68043648e-02  1.66215319e-02  4.71579246e-02\n",
      " -4.15599458e-02  9.01898893e-04  3.60277742e-02  3.42214964e-02\n",
      "  9.68227163e-02  5.94829470e-02 -1.64984688e-02 -3.51249278e-02\n",
      "  5.92519110e-03 -7.07932282e-04 -2.41031311e-02  3.49740349e-02\n",
      " -2.94746663e-02  6.04266766e-03 -9.80652776e-03  2.83218026e-02\n",
      " -1.85375810e-02  3.63212638e-02  1.30292689e-02 -3.71233188e-02\n",
      "  5.27256615e-02 -1.19707352e-02 -7.18082488e-02  1.24431932e-02\n",
      " -6.70563895e-03  7.42155090e-02  1.16356257e-02 -1.74534731e-02\n",
      " -1.82405524e-02 -1.88931394e-02  2.82414947e-02  1.32828997e-02\n",
      " -3.51910181e-02  8.87302333e-04  5.79572581e-02  3.22093628e-02\n",
      " -3.48578999e-03  4.13768664e-02  1.44358641e-02 -3.28044146e-02\n",
      " -9.79077537e-03 -3.16493027e-02  4.23870385e-02 -4.70847152e-02\n",
      " -2.08936725e-02 -1.91249549e-02 -1.22627318e-02  1.01605738e-02\n",
      "  3.91921066e-02 -2.61895210e-02  1.09028360e-02  1.35723026e-02\n",
      " -5.79267219e-02 -3.21499594e-02 -5.75720193e-03 -2.43515428e-02\n",
      "  5.23416735e-02  5.46117919e-03 -2.30996553e-02  2.57174601e-03\n",
      " -6.63347021e-02  3.54125462e-02 -1.03907436e-02  2.25410685e-02\n",
      " -1.84573662e-02 -2.42007319e-02 -4.78365831e-02 -4.79227258e-03\n",
      " -5.34138009e-02  3.01790871e-02 -1.56130511e-02 -5.51477261e-02\n",
      " -3.91873941e-02  5.92153408e-02 -3.47647220e-02  9.68125649e-03\n",
      "  2.13416144e-02  2.30416898e-02  1.91713367e-02  2.77378485e-02\n",
      " -7.73504423e-03  1.04446234e-02 -2.67719980e-02 -2.40200069e-02\n",
      " -1.92290023e-02  3.91511992e-03 -2.54714917e-02  3.61943766e-02\n",
      "  5.12867086e-02 -8.41691904e-03 -3.13830599e-02  1.47482930e-02\n",
      "  2.13940199e-02 -3.84900607e-02  2.01944839e-02  1.20765958e-02\n",
      " -3.12058371e-03  7.84033537e-03  3.30339745e-03 -4.94357795e-02\n",
      "  5.83886802e-02  3.26136826e-03  4.84480336e-03 -4.50682491e-02\n",
      "  2.45682541e-02  3.55427973e-02 -5.32506257e-02  9.21152681e-02\n",
      "  2.04395074e-02 -3.36951800e-02 -6.19803406e-02 -2.11038589e-02\n",
      "  7.82358646e-02  5.11908233e-02  5.93169928e-02 -1.25146777e-04\n",
      "  4.96349819e-02 -1.55722536e-02 -3.35680833e-03  1.82015616e-02\n",
      " -2.73444280e-02 -1.08772777e-02  1.41476393e-02  1.09877419e-02\n",
      "  4.32557613e-03  8.23312029e-02 -9.85381776e-04  7.58791342e-02\n",
      "  9.44996253e-03  2.37687659e-02  1.61927976e-02  6.24993294e-02\n",
      "  4.75922488e-02 -3.92629579e-03  9.07524526e-02  4.49874848e-02\n",
      " -3.47131491e-02  2.14077327e-02 -3.35604325e-02  4.93850000e-02\n",
      "  1.08669391e-02  2.63447426e-02 -3.26089300e-02  8.00303742e-02\n",
      "  9.29758977e-03  7.16578122e-03 -2.79172584e-02 -3.06821391e-02\n",
      "  4.01057396e-03 -4.93907370e-02 -3.13769677e-03  4.00537923e-02\n",
      " -3.97855043e-02  5.48014082e-02  1.36592807e-05 -8.38372484e-02\n",
      " -1.21547822e-02  3.40949409e-02  3.22414166e-03  6.11845106e-02\n",
      "  5.60067073e-02  9.62871592e-03  2.54616085e-02 -4.64169160e-02\n",
      " -3.98900807e-02  7.68132135e-02  2.28409171e-02 -2.26568840e-02\n",
      " -1.91193409e-02 -6.53029233e-02  4.56781276e-02 -4.43661446e-03\n",
      "  1.49631966e-02 -2.15078779e-02  2.74249353e-03  1.90358069e-02\n",
      "  5.91888987e-02 -2.47569103e-02  3.66145186e-02  5.63084520e-02\n",
      " -8.86446144e-03 -1.74324624e-02 -1.03289355e-03  2.47666780e-02\n",
      "  1.30762998e-02  5.04632629e-02 -5.28493151e-03  5.92397340e-02\n",
      "  6.29906803e-02 -4.36782800e-02 -4.97830138e-02  5.56296296e-02\n",
      " -2.44854204e-02 -8.26755092e-02  2.04910785e-02 -1.06446169e-01\n",
      "  6.64844736e-03  2.97304206e-02 -2.36440040e-02 -8.84619821e-03\n",
      "  2.45564873e-03 -3.35233510e-02  7.52212629e-02 -5.89880161e-02\n",
      " -3.67809013e-02  3.41542959e-02  5.41131012e-02 -1.74905080e-02\n",
      "  1.33921150e-02  4.71682958e-02  1.46116149e-02 -2.12311093e-02\n",
      " -6.55338317e-02  1.23857511e-02  2.76074987e-02 -8.02159030e-03\n",
      " -4.59636338e-02 -8.22437368e-03  9.16961953e-03 -1.56399161e-02\n",
      "  7.54619390e-03  1.58313685e-03 -3.03958487e-02 -5.10670952e-02\n",
      "  1.96313541e-02  1.26263676e-02 -1.51741877e-03  2.02891417e-02\n",
      "  1.37817021e-02  1.49110993e-02  2.50766389e-02 -3.62870730e-02\n",
      "  1.08085638e-02  2.74129654e-03  1.81510597e-02  5.39871976e-02\n",
      " -4.74542454e-02 -4.28730547e-02 -2.89914068e-02  2.13235095e-02\n",
      " -3.85161415e-02  6.31923303e-02 -5.77975512e-02  3.77891888e-03\n",
      " -2.54394189e-02 -1.77309033e-04  9.08240583e-03  1.59095656e-02\n",
      "  4.11799997e-02 -3.94366644e-02 -9.64430254e-03  1.30791785e-02\n",
      "  6.87962621e-02  4.32192646e-02  7.53998873e-04  6.77741319e-02\n",
      "  4.93706539e-02 -3.47818225e-03 -1.06055364e-02  6.72494434e-03\n",
      " -1.39062526e-02  4.88276668e-02 -1.05734877e-02  3.50229023e-03\n",
      "  2.90220464e-03  2.40043923e-02  1.20271519e-02 -2.09797714e-02\n",
      " -2.39111539e-02  3.26579623e-02 -1.01324031e-03 -5.92755619e-03\n",
      " -7.40532251e-03  3.63141974e-03 -2.26698406e-02 -2.21242476e-02\n",
      "  3.86996157e-02  1.72321592e-02  3.85920219e-02 -5.04711159e-02\n",
      " -3.42144296e-02 -4.00443412e-02 -3.57910506e-02 -4.62562032e-02\n",
      "  6.70232326e-02 -4.61647473e-03 -3.29680182e-03  2.08444167e-02\n",
      " -5.14250947e-03 -5.00849821e-02  2.22503264e-02  4.66933064e-02\n",
      "  1.36209084e-02  1.77530590e-02  4.28080745e-03 -2.79332120e-02\n",
      " -1.93420351e-02 -3.87860090e-02 -3.09555009e-02 -6.64133579e-02\n",
      " -1.13433963e-02  1.64268352e-02  1.77628472e-02 -2.28223158e-03\n",
      " -3.30087207e-02 -1.36265764e-03 -2.17933860e-02 -2.67507806e-02\n",
      " -1.26375733e-02  1.61870813e-03 -4.95672971e-02  7.85446316e-02\n",
      "  4.10963073e-02  9.65920836e-03 -1.14643043e-02  1.68856571e-03\n",
      "  5.37663065e-02  2.05527805e-03 -4.11200412e-02  1.46329580e-02\n",
      " -3.75564620e-02 -3.35689895e-02  5.19253779e-03 -6.33088648e-02\n",
      "  3.32963206e-02  8.76120105e-03  1.33861660e-03 -3.95747460e-03\n",
      " -1.61677916e-02  8.26746821e-02  4.75945212e-02 -3.43055539e-02\n",
      "  2.50881687e-02 -3.50977257e-02  3.68657559e-02  4.12649801e-03\n",
      "  4.16018665e-02 -1.35181621e-01 -4.76338342e-02 -1.20026488e-02\n",
      " -3.48892398e-02  3.25455442e-02 -2.93576461e-03 -4.85055475e-03\n",
      " -1.04223736e-01  2.78610326e-02  1.41570065e-02  3.94395255e-02\n",
      " -3.88806239e-02 -1.42463511e-02 -5.19983657e-02  8.92734528e-03\n",
      " -1.99770387e-02 -2.51725242e-02 -3.41299511e-02  1.93041116e-02\n",
      " -5.20207696e-02 -6.72000572e-02 -9.46365576e-03 -1.25587801e-03\n",
      " -5.66048734e-02  2.62098759e-02  9.91581753e-03  4.38286401e-02\n",
      "  2.26638024e-03 -3.11896168e-02 -6.25467375e-02 -3.87793221e-02\n",
      " -6.83939308e-02  4.93721068e-02  5.85507229e-02 -4.08729389e-02\n",
      " -1.98637936e-02 -2.12633703e-02  4.98037115e-02 -4.51748893e-02\n",
      " -2.37142481e-02  2.32674778e-02  1.00594856e-01  9.87117831e-03\n",
      " -1.38015989e-02 -5.21041974e-02  9.08207707e-03  1.72427781e-02\n",
      "  5.91431409e-02  2.62336694e-02 -7.04639591e-03 -1.50032360e-02\n",
      " -3.76662356e-03  6.28255447e-03 -5.23980334e-02 -4.96638604e-02\n",
      "  3.06610614e-02 -3.33644543e-03  2.34912056e-02 -8.58830288e-02\n",
      " -4.62448820e-02  5.59701286e-02  3.09032359e-04  2.01728884e-02\n",
      " -2.98051047e-03  1.76645815e-02  1.54668940e-02 -7.41718486e-02\n",
      "  7.34992279e-03 -1.05014984e-02  2.45247204e-02  1.36879394e-02\n",
      " -1.17804119e-02  4.51545306e-02  3.29039171e-02 -3.50391353e-03\n",
      " -2.71314550e-02 -5.27362861e-02 -4.60163951e-02  2.22848430e-02\n",
      "  2.62271818e-02  5.56152966e-03  1.45788575e-02 -2.97145825e-02\n",
      "  3.57042588e-02  2.22534202e-02  3.89617495e-02 -7.92634785e-02\n",
      " -9.01092030e-03  2.19012294e-02 -5.49059315e-03  8.69957451e-03\n",
      "  4.33031246e-02 -2.12631263e-02  1.13292430e-02 -6.33699223e-02\n",
      "  3.63723598e-02  2.67443098e-02 -6.64251968e-02  1.70499571e-02\n",
      " -2.79691499e-02  2.36352836e-03 -1.81952417e-02  1.52955735e-02\n",
      " -8.50440655e-03  1.16647175e-02 -9.75922197e-02 -2.92093325e-02\n",
      " -5.42547740e-02  3.61235142e-02  3.25116329e-02  8.26979615e-03\n",
      " -2.96502083e-04  1.11555271e-02 -3.85188200e-02  2.36162022e-02\n",
      "  9.85914469e-03  5.73998168e-02  4.86060753e-02 -1.37578379e-02\n",
      " -6.19225716e-03  1.11972978e-02 -3.37175652e-02 -1.10515831e-02\n",
      " -7.08334148e-02 -1.01816403e-02 -3.66011225e-02 -1.55561306e-02\n",
      " -2.13110298e-02 -1.02760550e-02 -4.35734019e-02  5.55186123e-02\n",
      " -3.76547910e-02  5.29252551e-02 -3.45224552e-02 -2.43005366e-03\n",
      "  7.25552961e-02  4.45072539e-03  4.71416973e-02 -9.43885557e-03\n",
      " -1.98979136e-02  5.71899638e-02  8.60541090e-02 -5.25058173e-02\n",
      " -1.39550231e-02  1.17373550e-02  1.33974710e-02 -4.73053008e-02\n",
      " -5.41673563e-02  4.62725535e-02 -2.58969665e-02  1.51415411e-02\n",
      "  3.38944420e-02 -3.78258945e-03 -5.76043613e-02 -1.60082150e-02\n",
      "  2.42738668e-02  3.37361433e-02 -1.96820870e-02 -2.53463425e-02\n",
      " -4.75617275e-02 -5.68755828e-02 -2.28194688e-02  3.83187905e-02\n",
      " -1.78331174e-02  1.35964369e-02  7.86004355e-04  9.74015146e-03\n",
      "  3.34299020e-02 -2.60134544e-02 -7.38569070e-03  3.56451124e-02\n",
      " -2.68532094e-02 -7.53625184e-02 -2.66983416e-02 -4.46457603e-33\n",
      " -3.31646390e-02  1.41703272e-02 -3.92908938e-02 -3.46318223e-02\n",
      " -5.88668603e-03 -1.18211526e-02  1.53951207e-02  1.18473386e-02\n",
      "  1.07756797e-02  3.62140276e-02  7.87954312e-03 -2.31845845e-02\n",
      "  1.07622882e-02  1.72346495e-02  9.54133633e-04  2.83640325e-02\n",
      "  2.37419102e-02 -1.48058683e-02  1.24198385e-03  3.52358958e-03\n",
      "  2.33735479e-02  5.58308028e-02  5.38328253e-02 -3.74079756e-02\n",
      " -2.11805645e-02  1.52717257e-04 -7.27788312e-03  5.50551387e-03\n",
      "  3.05823125e-02  4.54633720e-02 -3.35787013e-02  3.16142328e-02\n",
      " -2.56390101e-03  3.96355912e-02 -1.47573594e-02  5.67167960e-02\n",
      " -5.62787205e-02 -5.04594902e-03  3.56153920e-02 -2.76199616e-02\n",
      " -2.32293084e-02 -4.63291369e-02 -3.70919369e-02 -4.23188768e-02\n",
      "  3.70306894e-02  7.88719207e-03  3.85176502e-02  1.74774847e-03\n",
      "  5.62754879e-03  6.18116139e-03 -6.90268874e-02 -9.42963921e-03\n",
      " -7.74665643e-03  1.68349631e-02  1.22767324e-02  2.26406250e-02\n",
      "  1.21008372e-02  1.11743445e-02  1.21539067e-02 -1.16862934e-02\n",
      " -4.41612266e-02  2.30047628e-02  2.20672023e-02 -5.87504283e-02\n",
      " -3.96427922e-02  6.83135837e-02 -3.29948291e-02 -3.66774909e-02\n",
      " -3.53655405e-02  1.76184960e-02  6.95645064e-03  5.92692792e-02\n",
      "  4.12156023e-02  7.98109248e-02 -5.36565948e-03  1.14239193e-02\n",
      " -2.96388958e-02 -1.15412064e-02  2.22812835e-02  7.93187786e-03\n",
      "  2.60355454e-02  1.28212143e-02  1.71345845e-02 -6.90185232e-03\n",
      " -1.07603082e-02  1.35714486e-02 -9.90822096e-04 -6.16075061e-02\n",
      "  4.40513603e-02 -8.26544012e-04 -2.78340615e-02 -1.23618226e-02\n",
      "  1.34629123e-02 -3.85745205e-02  1.08699896e-03  2.18712818e-02\n",
      " -3.32398005e-02  1.84616074e-02 -5.10105444e-03  3.74665186e-02\n",
      " -3.67545150e-03 -2.19246838e-02 -4.96484432e-03 -9.59842466e-03\n",
      "  2.33591553e-02  1.04876682e-02  4.38722856e-02 -1.51423775e-02\n",
      " -6.30309507e-02  8.23259354e-03 -1.09131029e-02 -4.06409279e-02\n",
      " -6.21691123e-02  2.21324991e-02 -2.71434747e-02  4.05540019e-02\n",
      " -8.09449144e-03 -1.76410144e-03  3.01527008e-02 -5.42275887e-03\n",
      " -4.69822139e-02 -1.73768606e-02  4.11631316e-02  3.20635252e-02\n",
      " -2.22944804e-02 -1.58162117e-02 -4.50720377e-02  5.69485612e-02\n",
      "  4.71596234e-02 -5.78058809e-02  1.32475514e-02 -4.71288385e-03\n",
      "  1.66824535e-07  4.81090210e-02  5.03628142e-02  5.45263179e-02\n",
      "  2.07568966e-02 -1.19080208e-02 -6.37489278e-03  5.26361587e-03\n",
      "  7.21949488e-02 -2.21762881e-02  2.20102575e-02 -9.90495901e-04\n",
      " -1.37163354e-02  6.89197378e-03  2.46912967e-02 -1.39462024e-01\n",
      "  2.56988336e-03 -4.64827307e-02 -4.04967442e-02 -6.08555451e-02\n",
      " -1.53213646e-02  1.36129886e-01  9.45034847e-02  4.25741635e-02\n",
      "  4.67131287e-02 -2.30677836e-02 -1.20965634e-02  3.86673100e-02\n",
      "  2.11656070e-03 -2.51473244e-02 -1.15076406e-02 -3.46506126e-02\n",
      " -2.29534041e-02 -6.33842498e-03 -3.05175446e-02 -1.56236328e-02\n",
      "  1.39514217e-02  3.27474001e-04  2.00320641e-03  4.15105280e-03\n",
      " -2.22925339e-02 -3.62589359e-02 -2.36579720e-02 -1.87817030e-02\n",
      " -1.96289103e-02  4.52125818e-02 -8.12568367e-02 -2.14568581e-02\n",
      " -4.41543423e-02 -2.68475749e-02  2.01974157e-02  2.82991258e-03\n",
      " -1.95011497e-02 -3.45331021e-02  2.26913746e-02  3.78325880e-02\n",
      " -1.02544054e-02 -2.19755108e-03 -8.96744654e-02 -4.50031385e-02\n",
      "  8.09703860e-03 -2.05805227e-02 -2.02998426e-02 -2.09922548e-02\n",
      " -1.79405119e-02  5.81897087e-02 -7.63653498e-03  1.50847398e-02\n",
      "  1.78279801e-34  4.86179441e-02  4.22228128e-02  4.71595488e-02\n",
      "  5.89047670e-02  3.99784856e-02 -5.27071245e-02  1.56905316e-02\n",
      " -5.25109412e-04  1.13652414e-02 -6.56410232e-02 -2.20849104e-02]\n",
      "\n",
      "Sentence: Sentences can be embedded one by one or as a list of strings.\n",
      "Embedding: [ 4.31717634e-02 -5.38701713e-02 -3.78044732e-02  4.27236445e-02\n",
      " -2.35409085e-02  3.44860666e-02  2.89586447e-02  1.92816672e-03\n",
      "  2.41733305e-02 -3.17013115e-02  7.32855797e-02  1.25590274e-02\n",
      "  3.64620090e-02 -2.05251481e-02  2.81973239e-02 -6.87329099e-02\n",
      "  4.22232039e-02  9.31762333e-04  3.54034901e-02  1.41787371e-02\n",
      "  7.83991441e-03  2.31179334e-02 -4.84737474e-03  1.07173920e-02\n",
      "  4.39492613e-03  5.47803426e-03 -3.80338877e-02 -3.05487379e-03\n",
      "  5.72232157e-03 -6.78214356e-02 -4.88007367e-02 -1.45032229e-02\n",
      "  6.68007694e-03 -7.17479810e-02  1.64644871e-06  1.07564563e-02\n",
      " -3.60923223e-02 -2.37057228e-02 -5.22791110e-02  3.46110240e-02\n",
      " -5.42170182e-03  1.62612610e-02  1.96564663e-02  2.25396194e-02\n",
      " -2.25998508e-03  4.06341329e-02  8.17157477e-02  2.48179864e-02\n",
      "  5.31884506e-02  7.82715902e-02 -1.91813540e-02 -1.94087010e-02\n",
      " -2.62805372e-02 -2.44082939e-02  5.49405776e-02  1.90318562e-02\n",
      "  1.60811376e-02 -2.68895011e-02 -8.24695639e-03  7.33444616e-02\n",
      "  1.00123603e-02  2.93315221e-02  3.42855323e-03 -2.13270169e-02\n",
      " -1.62443391e-03 -5.56255877e-03 -7.64879808e-02 -5.85450046e-02\n",
      " -2.82272082e-02  7.51849730e-03  7.11225644e-02  1.95453409e-03\n",
      "  5.45931328e-03  3.22322967e-03  5.12800105e-02 -3.54105756e-02\n",
      " -5.03608435e-02  4.70519252e-02  5.15482109e-03  1.52287325e-02\n",
      " -1.06680766e-02  3.16299088e-02 -9.09037516e-03 -4.01327088e-02\n",
      " -4.35235649e-02 -1.94969270e-02  1.65604725e-02 -4.71169204e-02\n",
      " -3.92091572e-02 -3.07757389e-02 -2.94167455e-02 -4.20827232e-02\n",
      "  2.27073231e-03 -2.78329570e-02  1.69421807e-02  7.74497306e-03\n",
      " -5.23742214e-02 -4.50040735e-02  3.83605473e-02 -4.90786545e-02\n",
      "  5.06618693e-02  1.01615395e-02 -1.25021469e-02 -4.64556506e-03\n",
      " -1.54539961e-02  1.58862341e-02  1.18369786e-02 -3.59233096e-02\n",
      " -7.76226074e-02  3.43358889e-02 -2.14710217e-02 -6.86098114e-02\n",
      " -5.46236187e-02  7.83901364e-02 -3.00702471e-02 -3.37550268e-02\n",
      " -4.04999331e-02  4.80515175e-02  9.53902490e-03  2.31399927e-02\n",
      " -8.16115215e-02 -6.51691901e-03  1.54213458e-02  7.04258084e-02\n",
      " -1.25069022e-02 -2.48266608e-02 -1.71329342e-02  6.13106135e-03\n",
      "  5.44412881e-02 -1.40566118e-02 -6.24512834e-03  3.65788005e-02\n",
      "  7.36230835e-02 -6.05690759e-03 -3.61630358e-02 -1.42197742e-03\n",
      "  4.43166457e-02 -3.14519275e-03  3.18767391e-02 -1.30948098e-02\n",
      " -3.69524322e-02 -4.98030428e-03  1.30020257e-03 -2.05213539e-02\n",
      "  2.06277948e-02  5.93870878e-03 -3.07164481e-03 -3.97513509e-02\n",
      "  4.29890417e-02  6.49802908e-02 -6.76022694e-02  5.41655906e-02\n",
      "  1.52559846e-03 -3.72908860e-02 -4.02427092e-02 -2.28772461e-02\n",
      "  1.31769702e-01  4.87868674e-03  1.39470650e-02  4.92436066e-02\n",
      "  2.49219146e-02 -8.76095332e-03 -5.38762147e-03 -2.65596211e-02\n",
      " -1.19766416e-02 -2.32007094e-02 -2.67433226e-02  5.66907506e-03\n",
      "  2.21722424e-02  4.67294566e-02 -5.78487702e-02  8.22120532e-02\n",
      " -3.36831994e-03  8.09647888e-02  1.41423810e-02  1.02393121e-01\n",
      " -5.76837128e-03 -1.15876617e-02  4.90584895e-02  5.87829836e-02\n",
      "  6.50030971e-02  4.74621467e-02 -2.89464742e-02 -1.76580646e-03\n",
      "  3.32560912e-02  2.91198138e-02  6.03811406e-02  3.73549090e-04\n",
      "  1.06576504e-02 -5.96284419e-02 -7.28601217e-02  2.95080170e-02\n",
      "  9.54461657e-03 -2.71542221e-02 -5.63305393e-02  9.66683321e-04\n",
      " -4.77729626e-02  4.67576794e-02  4.87257075e-03 -6.57519922e-02\n",
      " -1.42248617e-02  3.99873480e-02 -1.09797334e-02  7.68941715e-02\n",
      " -4.00003716e-02  2.96825767e-02  2.81303506e-02 -5.55424765e-02\n",
      "  6.31263619e-03  5.00449985e-02  1.89884230e-02  5.38683906e-02\n",
      " -1.95981581e-02  1.08600864e-02  1.64150335e-02  1.44135477e-02\n",
      "  1.71448737e-02  2.17624828e-02 -4.98862192e-02  1.56104909e-02\n",
      "  4.83740587e-03  1.87053606e-02 -3.18545033e-03  2.66863629e-02\n",
      "  5.55552691e-02 -4.88005653e-02 -3.02928817e-02  2.52110381e-02\n",
      "  1.07265180e-02  1.88270174e-02 -1.50688207e-02  3.43832411e-02\n",
      "  4.15125154e-02  1.37788942e-02 -5.54848723e-02  1.43848630e-02\n",
      " -5.88139109e-02 -6.01676852e-02  2.69856658e-02 -5.46130091e-02\n",
      "  8.14633165e-03 -1.17758485e-02  1.57442559e-02  1.43906660e-03\n",
      " -2.64554489e-02 -4.48875725e-02  4.39732261e-02 -1.06210166e-04\n",
      " -2.25905571e-02  3.00296806e-02  1.97440330e-02  7.44077517e-03\n",
      " -1.93788987e-02  8.09799880e-03  4.34860103e-02 -1.08557520e-04\n",
      " -3.77225950e-02  2.67195739e-02 -4.63157855e-02 -1.53401541e-03\n",
      "  8.05311184e-03 -4.30901274e-02 -2.13849079e-02  1.20184952e-02\n",
      "  8.41401704e-03  2.48275860e-03 -3.09567004e-02 -9.05277357e-02\n",
      " -4.76694107e-02  1.22606140e-02 -1.36467181e-02 -2.63655242e-02\n",
      " -7.65553582e-03  8.72377027e-03  2.65725069e-02  8.39986606e-04\n",
      " -5.55930054e-03 -9.29537043e-03  3.19337286e-02  5.94645962e-02\n",
      "  1.83205605e-02 -7.56547228e-02 -5.59389628e-02 -1.20870844e-02\n",
      " -3.16261165e-02  3.62187289e-02  7.53613003e-03 -6.15653917e-02\n",
      " -2.30458528e-02 -3.51664727e-03  1.23332022e-02 -9.67634190e-03\n",
      "  4.96861748e-02 -8.42256323e-02  1.52395526e-02 -1.82445850e-02\n",
      "  7.70462453e-02  9.28717628e-02  4.03724872e-02  1.11732557e-01\n",
      " -1.03270076e-02 -2.54558567e-02  2.13153996e-02 -1.16186228e-03\n",
      "  2.82605435e-03  5.06967008e-02 -3.13697010e-02 -8.14279448e-03\n",
      "  1.38388053e-02  4.66889516e-02  5.09670600e-02  3.77154164e-02\n",
      " -2.94988565e-02  3.60631719e-02 -2.61167926e-03  2.72192847e-04\n",
      " -6.71806559e-02 -6.54026717e-02 -3.43590565e-02  1.91068705e-02\n",
      "  4.13294584e-02 -1.10970614e-02  4.51951511e-02 -5.93565181e-02\n",
      "  1.06963878e-02 -1.82229523e-02 -5.65814301e-02  1.20387087e-02\n",
      "  4.44773957e-02  1.87049937e-02  1.63810477e-02  5.51149994e-02\n",
      " -2.23332457e-02  2.12861635e-02 -1.20339962e-02  3.26752998e-02\n",
      "  1.47004202e-02 -8.16687196e-03  1.12904524e-02 -3.00620403e-02\n",
      " -2.34345850e-02 -2.68646218e-02 -1.28720538e-03 -7.67190084e-02\n",
      "  2.22615269e-03 -5.89474756e-03  2.63103619e-02  2.07126304e-03\n",
      " -6.91152886e-02 -1.43792024e-02  2.68787798e-02 -3.51540074e-02\n",
      " -2.69611720e-02  2.54717655e-03 -6.48881495e-02  3.18729132e-02\n",
      "  1.70126930e-02 -4.54004221e-02 -1.80615820e-02 -1.61115658e-02\n",
      "  5.70772924e-02 -2.78271595e-03 -6.45585060e-02  7.86598474e-02\n",
      "  2.29075160e-02  6.81843469e-03 -9.11739655e-03 -2.27726959e-02\n",
      " -4.76526208e-02  4.88430560e-02 -2.09893044e-02 -2.43693739e-02\n",
      " -5.01208287e-03  6.70254380e-02  6.91370806e-03  2.25842651e-02\n",
      "  2.51125023e-02 -6.92509487e-03  8.59392621e-03  2.38977112e-02\n",
      "  3.29738744e-02 -1.05310567e-01  1.22095319e-02 -1.22263366e-02\n",
      " -5.73769361e-02  1.84311680e-02  2.97158007e-02 -6.09429069e-02\n",
      " -6.55256361e-02  3.55713069e-02  5.64317731e-03  3.34649137e-03\n",
      " -3.59686799e-02 -8.83422978e-03 -6.97895214e-02  6.89778775e-02\n",
      " -4.88214009e-03  2.23995298e-02 -3.16053294e-02 -7.41181988e-03\n",
      "  3.19351107e-02 -5.18788211e-02  2.11601406e-02 -5.03339767e-02\n",
      "  9.10578668e-03  2.13354044e-02  1.66838616e-02  3.49020138e-02\n",
      " -6.38500229e-02 -6.75625168e-03 -1.27405347e-02 -4.63366359e-02\n",
      " -1.14779929e-02  2.08778754e-02  2.44822446e-02  3.66458856e-03\n",
      " -2.86097988e-03  2.29390413e-02  2.13746279e-02 -3.48901115e-02\n",
      " -3.00388020e-02  4.78870571e-02  5.83370365e-02 -9.70491860e-03\n",
      "  1.38235232e-02 -3.27485539e-02 -8.11463047e-04  9.54228546e-03\n",
      "  1.20401857e-02  1.97230279e-02 -4.74797009e-04 -1.39226047e-02\n",
      " -5.21069951e-02 -1.75592341e-02 -5.41699119e-02 -1.17970007e-02\n",
      " -1.71030667e-02 -3.50195244e-02  3.38661745e-02 -6.76587895e-02\n",
      " -2.27606203e-02  1.95607264e-02  5.50249740e-02  1.22029493e-02\n",
      " -1.75170589e-03  7.22440751e-03  1.16350288e-02 -1.61908139e-02\n",
      " -3.37755159e-02  3.22626606e-02 -2.03813706e-02 -2.33859662e-02\n",
      " -1.29991584e-02 -1.66799147e-02  1.03071611e-02 -1.46029927e-02\n",
      " -7.79070631e-02 -8.25812444e-02 -3.38809974e-02  3.81114595e-02\n",
      "  7.86003377e-03  2.41455194e-02 -2.75715552e-02  1.30867492e-02\n",
      " -7.88588822e-03  1.78650953e-02  5.37323095e-02 -3.01823318e-02\n",
      "  1.69455502e-02  1.19571378e-02  3.52548755e-04  4.90209274e-02\n",
      " -8.57212581e-03  1.71269232e-03  4.83873906e-03 -4.10080887e-02\n",
      " -4.68120351e-02 -2.32562260e-03 -5.16776554e-02  3.10031045e-02\n",
      "  1.60961822e-02 -1.00803319e-02 -3.72487796e-03 -3.53388451e-02\n",
      "  2.95961704e-02  2.89096870e-02 -7.59912208e-02 -5.02981134e-02\n",
      " -2.11783573e-02  3.20462212e-02 -3.84538323e-02  2.45102812e-02\n",
      " -2.04187948e-02  6.02109823e-03 -9.81939305e-03  3.74777876e-02\n",
      "  3.40838879e-02  1.28864506e-02  5.67342713e-02 -8.09704140e-02\n",
      " -8.93610343e-03  1.33352643e-02 -2.51566209e-02  2.58406717e-03\n",
      " -6.51802495e-02  1.34400697e-02 -2.04682350e-02  6.53375639e-03\n",
      "  4.56976192e-03  1.99271087e-02 -6.07340932e-02  1.40691791e-02\n",
      " -5.75334169e-02  9.79801919e-03  3.55392806e-02 -2.45283321e-02\n",
      " -4.73314896e-03 -2.77493745e-02  2.34282445e-02 -8.76619088e-05\n",
      "  7.30439136e-03  1.42028928e-02  4.92807068e-02 -3.16540226e-02\n",
      " -1.34901833e-02  3.08487434e-02  2.80401818e-02 -4.33068611e-02\n",
      " -4.42284495e-02  3.80739458e-02  9.48157467e-05 -4.34896946e-02\n",
      "  1.43868644e-02  2.44326517e-03 -4.84073535e-02  1.08956154e-02\n",
      " -9.87487100e-03  4.59295698e-02  3.96379791e-02 -2.60117389e-02\n",
      "  2.48134807e-02 -5.37149683e-02  5.62824495e-02  8.81360751e-03\n",
      "  5.25077395e-02 -1.47370910e-02 -1.74380615e-02  3.45084891e-02\n",
      "  3.75523642e-02 -4.70167063e-02 -1.94910970e-02  3.82632241e-02\n",
      " -5.67596778e-02 -1.78616575e-03  2.33403463e-02 -5.88216381e-33\n",
      " -4.87188511e-02 -2.76266262e-02 -3.38240825e-02  2.66188327e-02\n",
      " -3.39277014e-02 -8.49194825e-03 -1.91250183e-02  3.00252158e-02\n",
      "  3.40781510e-02  5.11157140e-02 -1.92480162e-02  2.85642445e-02\n",
      "  3.66040468e-02  1.68858096e-02  4.77258340e-02  1.23802386e-02\n",
      "  2.14844048e-02  4.93626634e-04  1.21273836e-02 -5.82144782e-02\n",
      "  1.62953902e-02 -7.14261131e-03  4.80092317e-02  2.51191035e-02\n",
      "  4.60097492e-02 -2.29840241e-02 -2.05696356e-02 -3.22225760e-03\n",
      "  4.00092006e-02  3.52309979e-02 -3.43153924e-02  2.75631133e-03\n",
      " -1.25138490e-02  1.97686404e-02  5.53489756e-03  1.03744507e-01\n",
      "  5.77608356e-03 -5.65426685e-02  4.19558734e-02 -3.78830880e-02\n",
      " -3.93442847e-02 -6.24309815e-02 -2.24397122e-03 -5.46548851e-02\n",
      "  4.56134826e-02 -5.69244707e-03  3.38917188e-02 -1.44447582e-02\n",
      "  2.72102235e-03  1.11191645e-02 -5.00661172e-02 -1.61127374e-02\n",
      "  1.72822969e-03  6.88878447e-02  1.16493376e-02  2.83171497e-02\n",
      "  6.97191898e-03  2.68372409e-02 -7.72082107e-03  2.16828398e-02\n",
      "  1.15183499e-02  8.72832760e-02 -6.27265172e-03 -6.44473061e-02\n",
      " -1.58233475e-02  4.03268375e-02 -1.69728119e-02 -1.61188487e-02\n",
      " -3.75576466e-02  7.02938363e-02 -3.30487229e-02  4.66324538e-02\n",
      "  1.18028168e-02  6.51075318e-02 -1.16979918e-02 -8.28344747e-03\n",
      " -5.46904542e-02 -2.00226326e-02  8.42718524e-04 -8.19521956e-03\n",
      "  2.08356697e-02  1.37454569e-02 -1.29920396e-03 -3.94575261e-02\n",
      " -2.00184975e-02 -1.53721059e-02  1.17271692e-02 -4.40111272e-02\n",
      "  5.39267957e-02 -2.33011059e-02 -2.24211570e-02 -3.65215680e-03\n",
      "  2.92213634e-02  7.56440032e-03 -2.90923528e-02  4.01517637e-02\n",
      " -2.00853460e-02 -1.79864257e-03 -1.26236575e-02  2.51076892e-02\n",
      " -4.69284542e-02 -3.08554582e-02 -3.63386876e-04  6.01784000e-03\n",
      "  3.97509076e-02  1.38546983e-02  2.49773543e-02  1.76975839e-02\n",
      " -9.31573808e-02 -9.83685162e-03  8.44925735e-03 -1.95390955e-02\n",
      " -3.26569676e-02  5.13739837e-03  5.80928382e-03  2.08537020e-02\n",
      " -5.97830629e-03  5.86810336e-02 -1.49496282e-02 -5.72965331e-02\n",
      " -5.98233938e-03  1.95200124e-03  2.72978144e-03  6.06999500e-03\n",
      " -2.00526081e-02 -1.31688649e-02 -4.06228825e-02  5.68997189e-02\n",
      "  4.44969498e-02 -1.24307657e-02  1.96967740e-02  3.80979739e-02\n",
      "  2.30237617e-07  1.10575417e-02  4.79512848e-02  6.18298501e-02\n",
      "  4.40278202e-02  6.17655087e-03  2.58291001e-03  3.38913947e-02\n",
      " -5.32941287e-03 -2.59283781e-02 -1.26143834e-02  2.46496014e-02\n",
      " -1.68774812e-03  1.17901829e-03  2.40443442e-02 -9.77310017e-02\n",
      "  1.97367538e-02 -5.52920699e-02 -6.17424697e-02 -4.87152152e-02\n",
      "  1.11090287e-03  1.18732132e-01  8.13258141e-02  3.32449339e-02\n",
      "  4.38326746e-02 -2.49559171e-02 -3.59626710e-02  1.66319348e-02\n",
      "  5.93760703e-03 -1.43972198e-02  4.46712598e-03 -6.01986647e-02\n",
      " -5.65912575e-02 -8.21539760e-03  5.83052402e-03 -1.69482380e-02\n",
      "  9.58632305e-03  1.46733737e-02  5.05845882e-02  3.06891631e-02\n",
      "  6.60468861e-02 -2.56551635e-02 -2.78858151e-02 -3.19173224e-02\n",
      " -3.39236371e-02  1.49903065e-02 -3.03336345e-02 -6.06494769e-03\n",
      " -4.81768698e-03  1.72136314e-02 -8.23372323e-03  1.55547913e-02\n",
      "  2.69106496e-02  5.44310082e-03 -1.06898835e-02 -7.82136526e-03\n",
      " -4.44506593e-02  2.55874004e-02 -5.74760959e-02 -2.05443967e-02\n",
      " -3.07850465e-02 -1.57855377e-02 -7.07540009e-03 -4.21312191e-02\n",
      "  3.79934870e-02  6.27765581e-02 -7.67782563e-03 -3.18353325e-02\n",
      "  1.99277792e-34  1.04834437e-02 -3.39326560e-02  3.93821336e-02\n",
      "  5.53066321e-02  9.42170154e-03  1.09728351e-02 -4.91939858e-02\n",
      "  2.95023900e-02 -8.85379873e-03 -5.96248396e-02 -2.37825476e-02]\n",
      "\n",
      "Sentence: Embeddings are one of the most powerful concepts in machine learning!\n",
      "Embedding: [-2.98611373e-02 -1.37521755e-02 -4.75402027e-02  2.72126701e-02\n",
      "  3.40054259e-02  3.16465758e-02  4.26963493e-02  3.29803373e-03\n",
      "  4.35718000e-02  2.53837090e-02  3.02527826e-02  3.21130604e-02\n",
      " -3.99912745e-02  1.28761381e-02  6.70219138e-02 -7.92899504e-02\n",
      "  4.68772501e-02  2.40266826e-02 -2.07997914e-02 -1.07433423e-02\n",
      " -1.19410409e-02 -5.39290756e-02  4.21055220e-02  2.23588031e-02\n",
      " -2.98949517e-02  8.35978985e-03  1.58385858e-02 -4.80236523e-02\n",
      "  1.88435055e-03 -1.67521890e-02 -2.15628576e-02 -3.88488173e-02\n",
      "  3.06273550e-02  4.20526601e-02  1.69483405e-06 -1.86929330e-02\n",
      " -1.24559263e-02  1.32128270e-02 -4.89039160e-02  1.34746274e-02\n",
      "  2.28873882e-02  8.81792232e-03  8.64926539e-03 -2.00950131e-02\n",
      " -3.15218158e-02 -2.53433269e-02  7.57319033e-02  3.62447277e-02\n",
      "  1.25290537e-02  3.09694856e-02  4.50760545e-03 -3.50042544e-02\n",
      " -4.42495686e-04 -9.76648089e-03  6.04545698e-02  4.03472371e-02\n",
      "  1.10735381e-02  6.56195777e-03 -5.84596768e-03  3.79783916e-03\n",
      " -4.46914956e-02  1.76404621e-02  2.45916191e-02 -3.60039738e-03\n",
      "  1.02473341e-01  3.73758227e-02  6.13311958e-03 -2.24676877e-02\n",
      "  1.46482578e-02  5.00537455e-02 -2.29907725e-02  1.12924520e-02\n",
      " -3.10552251e-02 -1.49508985e-02 -2.53137713e-03  3.20945531e-02\n",
      " -4.67056036e-02 -4.85886559e-02  2.98305415e-02  6.44215271e-02\n",
      " -3.12612914e-02  3.57407629e-02  4.16527018e-02 -5.52517958e-02\n",
      " -8.74622259e-03 -2.18630061e-02 -1.12744179e-02 -2.14435905e-02\n",
      " -1.32824443e-02 -2.04866547e-02 -1.00576496e-02  3.54763493e-02\n",
      " -7.47608813e-03 -3.70188504e-02  5.77893071e-02 -2.18169075e-02\n",
      "  4.36223438e-03  2.04380788e-02  3.36815491e-02 -4.92800996e-02\n",
      "  4.82793562e-02 -1.81001227e-03 -1.05118500e-02  4.13323306e-02\n",
      " -6.79834113e-02  1.75716132e-02 -4.43412811e-02  9.90841072e-03\n",
      " -3.81809697e-02  1.10827414e-02 -5.07280976e-02 -2.17451006e-02\n",
      " -1.03836320e-02  4.60333377e-02  1.55862551e-02 -4.21366766e-02\n",
      " -2.72146910e-02  3.22818533e-02 -4.24739420e-02  2.71206722e-02\n",
      " -7.41061047e-02  4.20107543e-02  2.02437844e-02  7.31811225e-02\n",
      " -8.97696614e-03 -2.31160093e-02 -3.93558852e-02 -1.46008441e-02\n",
      " -3.30911279e-02  1.12239877e-02  2.58575007e-03 -4.36851755e-03\n",
      "  1.85855795e-02  2.69934963e-02 -1.67215951e-02  3.69569845e-02\n",
      "  4.44489643e-02 -2.21723597e-02  6.72960142e-03  1.22935381e-02\n",
      "  1.71758961e-02 -2.36475538e-03  3.72263230e-02 -2.22871024e-02\n",
      "  2.94603575e-02 -2.33691614e-02  5.38466452e-03 -3.06582451e-02\n",
      " -2.38920022e-02 -2.63614617e-02 -2.01788936e-02  1.11245655e-01\n",
      " -1.99836958e-02 -3.54030430e-02  3.84142809e-02  2.53069345e-02\n",
      "  1.99551992e-02  5.53518310e-02 -1.99332405e-02 -2.16717646e-03\n",
      "  4.91092615e-02 -4.03530337e-02 -1.16977058e-02 -5.33113070e-02\n",
      "  8.29586387e-03 -5.08251414e-02 -2.65503209e-02 -1.53242899e-02\n",
      "  5.78818889e-03  2.46580946e-03 -3.44449431e-02 -1.85132131e-03\n",
      " -3.95730920e-02 -2.71690506e-02  4.93568107e-02  8.38368684e-02\n",
      "  5.43492772e-02  8.22261646e-02  1.23893917e-02 -4.79805609e-03\n",
      "  7.77375943e-04  2.98486482e-02 -1.85583737e-02  5.98795451e-02\n",
      " -6.82798121e-03  9.78117925e-04  2.85485331e-02 -7.64614856e-03\n",
      " -1.86620038e-02 -2.69287080e-02 -2.90332958e-02 -1.37871364e-02\n",
      " -2.57599959e-03 -2.20173597e-02 -1.70821175e-02 -3.81842181e-02\n",
      "  2.21506134e-02 -3.59232910e-02 -1.19439308e-02 -3.18307914e-02\n",
      " -4.80801351e-02  9.77637991e-03 -1.04859110e-03  4.15370800e-02\n",
      " -1.10974051e-02 -4.72830683e-02  1.90984048e-02 -5.31177223e-02\n",
      "  2.11324934e-02 -2.53073242e-03  5.61054721e-02 -1.33795859e-02\n",
      " -5.95867634e-03 -1.20307859e-02  4.63930182e-02 -2.81908493e-02\n",
      "  2.25355364e-02 -2.50469171e-03 -3.52452695e-02  2.55495254e-02\n",
      "  9.10397340e-03  3.25214234e-03  2.55969563e-03 -1.25624798e-02\n",
      " -3.51496935e-02 -4.28946428e-02 -2.32327706e-03  2.41020881e-02\n",
      " -5.16831037e-03  1.68740340e-02  5.52647561e-03  2.36792006e-02\n",
      "  5.65164164e-02 -3.47869173e-02 -6.34517595e-02 -7.45624769e-03\n",
      " -1.78447943e-02  5.35897948e-02  2.67291889e-02 -8.74199197e-02\n",
      "  1.04195848e-02 -4.13962523e-04 -3.04450374e-03  9.14244913e-03\n",
      "  2.91530751e-02 -5.81831448e-02  6.83463216e-02 -4.08618152e-02\n",
      " -9.09781829e-03 -3.40770148e-02  3.52411680e-02 -1.02627771e-02\n",
      " -5.72469784e-04 -2.73449393e-03  1.59635916e-02  4.49064607e-03\n",
      " -2.09051408e-02  3.02770585e-02  2.46118791e-02 -1.44067779e-02\n",
      "  1.73270088e-02  1.99036766e-03  4.23051752e-02 -2.39176024e-02\n",
      " -3.25546935e-02 -1.45940026e-02  3.95100825e-02 -6.04650676e-02\n",
      " -3.02065294e-02  1.67189855e-02 -2.26816535e-02 -2.61954386e-02\n",
      " -5.51318973e-02  1.44908121e-02 -1.99246705e-02  3.99752380e-03\n",
      "  3.12610194e-02 -4.90727723e-02 -9.49740293e-04  5.39497212e-02\n",
      " -9.10086650e-03 -2.69486401e-02 -3.63158658e-02 -1.38436668e-02\n",
      " -4.45622206e-02  5.49359433e-02  2.17600749e-03  2.23445403e-03\n",
      " -5.23019489e-03 -1.47893718e-02  3.60592492e-02  1.45263365e-02\n",
      "  8.39192979e-03 -6.10360913e-02 -7.89955165e-03 -2.98298337e-03\n",
      "  3.56566440e-03  8.33991989e-02 -2.61214972e-02  8.06721821e-02\n",
      "  3.63051891e-03  1.69974007e-02  2.58605126e-02  1.09438784e-03\n",
      " -4.57064137e-02  5.55678718e-02  2.00643893e-02  4.76660728e-02\n",
      " -4.91053239e-02 -1.86081231e-02  3.34405527e-02 -2.57310122e-02\n",
      " -3.16366646e-03  7.21444488e-02 -1.61519237e-02 -1.33932829e-02\n",
      " -6.06294535e-02 -2.82186940e-02 -8.91928934e-03 -2.71170679e-03\n",
      "  8.04919004e-03 -4.95209284e-02  7.89433941e-02  2.76428219e-02\n",
      " -5.42576564e-03 -3.06733348e-03 -4.11826633e-02  1.39171435e-02\n",
      "  3.04253325e-02  1.02855954e-02  1.06679080e-02 -5.56553528e-02\n",
      " -1.75083205e-02  2.03868803e-02  8.43302812e-03  3.82471606e-02\n",
      " -3.89099792e-02 -1.61303747e-02  3.18059176e-02 -7.32969493e-02\n",
      " -1.76502336e-02 -4.79874127e-02 -5.55042289e-02 -5.00743603e-03\n",
      "  4.46770078e-04  3.57333124e-02 -8.24696559e-04 -3.34324576e-02\n",
      " -3.32416557e-02 -2.46459320e-02  2.15332229e-02  3.90864629e-03\n",
      "  2.53471117e-02  6.02994533e-03 -7.81605951e-03  1.23765701e-02\n",
      " -1.71039551e-02  2.68102176e-02  2.83672381e-03 -1.27643459e-02\n",
      "  1.00510865e-01  1.03581268e-02 -3.55142727e-02  1.56616122e-02\n",
      " -9.85950083e-02  4.58441935e-02 -3.15230973e-02 -2.35782061e-02\n",
      " -2.78350189e-02 -7.76177912e-05 -2.82363780e-02 -1.92918926e-02\n",
      "  1.87389627e-02  5.71941361e-02  2.56912000e-02 -3.20030525e-02\n",
      "  1.99074168e-02 -3.15819085e-02 -4.02062759e-02  5.77630177e-02\n",
      "  1.72973480e-02 -5.37014119e-02 -1.25325769e-02 -1.45484200e-02\n",
      " -5.76174967e-02  1.09727625e-02 -2.04728693e-02  2.85540652e-02\n",
      " -5.04399315e-02  4.36991043e-02  1.75711196e-02 -1.02344742e-02\n",
      " -9.69772115e-02 -2.99995560e-02 -2.86678839e-02  2.24936306e-02\n",
      " -1.68121196e-02 -1.43673206e-02 -8.79594591e-03 -1.69043858e-02\n",
      "  2.41558217e-02 -6.53193220e-02 -4.10799757e-02 -2.34057885e-02\n",
      " -6.76064566e-02 -1.55690983e-02  3.62358578e-02  7.83160254e-02\n",
      " -4.97517176e-02 -7.08548203e-02 -5.01179285e-02 -8.56464496e-04\n",
      "  5.44897420e-03  4.34691366e-03  9.88052860e-02 -2.16415487e-02\n",
      " -1.87751427e-02  1.15070147e-02  2.63996571e-02  1.65235382e-02\n",
      " -2.24058125e-02 -4.31826636e-02  1.31803915e-01 -2.97034718e-02\n",
      "  2.65934579e-02 -1.38888061e-02 -1.67004131e-02  3.44145969e-02\n",
      " -8.94358568e-03  6.16000704e-02 -3.42303067e-02  2.46421341e-03\n",
      " -8.14090017e-03  5.80325387e-02  5.24207726e-02 -1.53281763e-02\n",
      "  4.01382558e-02  1.51407644e-02 -3.01475381e-03 -4.97021414e-02\n",
      " -4.24301391e-03  5.77287972e-02  3.17873731e-02  4.74007651e-02\n",
      "  2.95218453e-02 -1.50121795e-02 -2.47945748e-02 -7.11502209e-02\n",
      "  2.06847936e-02  3.11487652e-02 -5.86070167e-03  1.62786692e-02\n",
      " -3.93682569e-02  5.46506085e-02  3.26594710e-02 -1.87021680e-02\n",
      " -9.79863256e-02  4.33762325e-03 -5.58156595e-02 -1.34620974e-02\n",
      "  2.88453493e-02  1.58748459e-02 -3.32564786e-02  1.44415966e-03\n",
      " -5.51107936e-02  8.24673697e-02  2.38845441e-02 -2.04837620e-02\n",
      " -4.78581991e-03  3.78722474e-02 -4.87562455e-02  3.44646871e-02\n",
      "  1.10358065e-02  1.12450169e-02  1.33262984e-02 -3.46374959e-02\n",
      " -6.92220256e-02  7.30122672e-03 -6.57010311e-03  1.73203554e-02\n",
      "  5.23064006e-03  4.48132269e-02  3.89853194e-02 -1.99274272e-02\n",
      " -1.80921201e-02  3.25937495e-02 -2.02026516e-02  4.86217206e-04\n",
      " -8.88757780e-03 -1.91347711e-02  2.50686072e-02  4.74020019e-02\n",
      "  2.18658987e-03 -1.69988405e-02  3.62671316e-02  3.46247409e-03\n",
      "  4.21937741e-03  8.04172084e-02  3.10627017e-02 -1.04940415e-03\n",
      " -3.55466306e-02  4.34836820e-02 -3.06218360e-02 -3.03192697e-02\n",
      " -4.13175188e-02 -1.05258133e-02 -2.35242900e-02 -1.86771844e-02\n",
      "  4.42940602e-03  5.45055792e-02 -6.05017915e-02  2.48421934e-02\n",
      " -3.36966403e-02 -4.54169400e-02 -2.63173673e-02  6.98053697e-03\n",
      "  6.92872033e-02 -2.04490554e-02 -1.96813084e-02 -9.72571038e-03\n",
      " -1.21564427e-02  7.89342448e-03  1.84751651e-03 -6.93650991e-02\n",
      "  2.43357550e-02  4.00609486e-02  3.44012715e-02 -2.84281913e-02\n",
      " -1.09431092e-02  1.38742784e-02 -4.40583564e-03  1.19345053e-03\n",
      " -8.81165564e-02  1.15930280e-02 -2.56351493e-02  5.57525530e-02\n",
      "  1.26946107e-01  5.39565943e-02 -1.41437072e-02  1.27196163e-02\n",
      " -1.32236322e-02 -5.94483875e-02  2.86705457e-02  2.57284604e-02\n",
      " -8.33777804e-03  8.17427586e-04  5.93059510e-03  3.29110548e-02\n",
      "  4.12751883e-02 -5.77957230e-03 -1.71124246e-02  1.06227379e-02\n",
      " -2.19601672e-02 -4.97206971e-02  2.53765937e-02 -5.60257616e-33\n",
      " -1.35397827e-02 -3.77958938e-02 -2.67921807e-03 -3.69702262e-04\n",
      " -1.98267847e-02  5.47047378e-03  6.02681097e-03  1.93069149e-02\n",
      "  3.87985422e-03  2.97698136e-02 -1.88228227e-02  2.20031547e-03\n",
      "  8.17019399e-03  1.61965732e-02  3.17528546e-02 -6.83414843e-03\n",
      "  2.19252203e-02  4.37764189e-04  2.96859238e-02 -2.62556821e-02\n",
      "  6.49387622e-03  3.56025323e-02  1.58611883e-03 -4.76583913e-02\n",
      " -5.26239648e-02  3.78274620e-02  3.54342237e-02 -3.10348161e-02\n",
      "  7.96403922e-03  5.48469722e-02 -3.56443450e-02  9.10139643e-03\n",
      " -9.45987459e-03 -4.63287644e-02 -1.63906626e-02  6.32453263e-02\n",
      " -1.38588762e-02 -5.95725179e-02 -1.57990716e-02  2.01887656e-02\n",
      " -1.98292639e-02 -3.49211283e-02  2.27937773e-02 -5.91620989e-02\n",
      "  4.18854319e-02  1.20742747e-03  5.19158319e-02 -1.88435949e-02\n",
      " -3.12102418e-02  2.34932452e-02 -7.41029531e-02 -2.76587467e-04\n",
      " -1.51719823e-02  6.11714050e-02  1.25065088e-01 -1.28459092e-02\n",
      " -1.12671852e-02  1.51748408e-03 -8.09153691e-02  1.12689156e-02\n",
      " -1.97573714e-02  2.74268035e-02  9.40991379e-03 -9.58753098e-03\n",
      "  2.54851021e-02  6.81658983e-02 -1.83453541e-02 -1.00963958e-01\n",
      " -9.45251063e-03 -5.27003594e-03  1.98683795e-02  9.80847850e-02\n",
      "  3.15632895e-02  5.30423187e-02  3.75122540e-02 -6.64209276e-02\n",
      " -5.92880510e-02 -1.57074593e-02  1.76609587e-02 -5.81073500e-02\n",
      "  2.23231539e-02  1.29869757e-02 -3.30261029e-02  9.96876275e-04\n",
      " -9.87092499e-03 -3.12954895e-02  2.28523207e-03 -4.91250046e-02\n",
      "  1.47693958e-02 -1.83367953e-02 -4.16305400e-02  3.76897603e-02\n",
      "  3.35410051e-02 -7.97111541e-02  4.01300080e-02  1.59072038e-02\n",
      "  5.06089255e-03  4.28808965e-02  2.29759943e-02 -4.13351059e-02\n",
      " -3.10503170e-02 -5.26404604e-02 -4.95405272e-02 -2.94256099e-02\n",
      "  5.94923794e-02 -2.59803049e-02  3.02497149e-02  8.80403910e-03\n",
      " -4.84466963e-02 -2.00850721e-02  9.82172694e-03 -7.89194033e-02\n",
      "  4.52880515e-03 -9.34901088e-03  9.23309382e-03 -3.17362100e-02\n",
      "  2.10833251e-02  6.37130812e-03  3.36348191e-02  3.83613110e-02\n",
      " -4.55528088e-02  1.08116399e-03 -9.83317290e-03  7.70192361e-03\n",
      " -2.87618153e-02 -1.74959321e-02 -4.27814573e-03  2.81286947e-02\n",
      "  4.97339107e-02 -7.45570660e-02 -1.07008116e-02 -7.66054774e-03\n",
      "  2.33968862e-07  1.52483359e-02  8.39614123e-02  3.67242657e-02\n",
      " -3.69249247e-02  3.64752486e-02  4.26422507e-02 -4.39820951e-03\n",
      "  1.78133622e-02 -2.67077163e-02 -7.13895075e-03  5.59975989e-02\n",
      "  3.13966200e-02  2.13438552e-03  3.90370749e-02 -8.78529176e-02\n",
      " -2.21659914e-02 -2.47735493e-02 -1.18190842e-02 -7.89704919e-03\n",
      " -2.08857600e-02  4.30555716e-02  1.07643560e-01  4.40618806e-02\n",
      "  1.47962971e-02  2.44863015e-02 -3.86268161e-02  1.80743393e-02\n",
      " -1.47829566e-03  7.74167031e-02 -4.19565290e-02 -3.80529277e-02\n",
      "  3.61257344e-02  1.59035530e-03  1.95323993e-02 -2.00080611e-02\n",
      "  4.22538593e-02  3.06111127e-02 -3.53702251e-03  5.93102351e-03\n",
      " -2.23223660e-02 -2.07130816e-02 -3.62917851e-03  1.74654443e-02\n",
      " -4.08758633e-02  5.91595098e-02 -5.89099117e-02 -3.96754444e-02\n",
      " -3.33528481e-02  1.02161719e-02  6.97295601e-03  7.70388767e-02\n",
      " -1.86911132e-02 -1.82568599e-02 -2.42319200e-02 -3.40699218e-03\n",
      " -3.60554531e-02  4.33389284e-02 -3.48603502e-02  5.27769178e-02\n",
      "  2.89710425e-02 -4.98463474e-02 -1.94749329e-02  1.16398698e-02\n",
      " -3.04614920e-02  8.04637894e-02  6.56248182e-02 -2.84533612e-02\n",
      "  1.81615392e-34 -4.19155508e-03 -2.57881880e-02  5.17319627e-02\n",
      "  4.94421087e-02  1.32476473e-02 -4.21994217e-02 -1.12458663e-02\n",
      " -2.61519980e-02  5.51130809e-02  2.20024996e-02 -2.51170471e-02]\n",
      "\n",
      "Sentence: Learn to use embeddings well and you'll be well on your way to being an AI engineer.\n",
      "Embedding: [-2.20731217e-02  2.08950248e-02 -6.03005849e-02  8.43955111e-03\n",
      "  4.37651016e-02  1.55070527e-02  4.99907546e-02 -3.03232856e-02\n",
      "  4.94783744e-02  2.35512014e-02  3.29350308e-02  1.53877446e-02\n",
      " -6.68355525e-02  1.11002855e-01  6.92676380e-02 -2.31889263e-02\n",
      "  3.79102714e-02 -4.94143320e-03 -1.57800801e-02 -3.45476270e-02\n",
      " -2.65052915e-02 -2.47879568e-02 -1.86141282e-02  3.00362222e-02\n",
      " -2.81186495e-02 -8.75135791e-03 -3.30774253e-03 -2.06116326e-02\n",
      "  1.03315460e-02 -1.51484078e-02 -3.48330811e-02 -2.63248086e-02\n",
      "  2.06907485e-02  3.79109643e-02  1.81912890e-06 -2.44293688e-03\n",
      " -1.80559128e-03  5.61760366e-03 -2.79870182e-02  1.54703120e-02\n",
      "  3.06456983e-02  3.72601412e-02 -1.55611141e-02  2.54414491e-02\n",
      " -6.42072931e-02  3.16353031e-02  6.63442165e-02  3.80970836e-02\n",
      "  5.57845496e-02  5.31660505e-02 -9.69293155e-03 -3.61424498e-02\n",
      "  3.72434407e-02 -4.67829779e-03  5.14576100e-02  1.00057600e-02\n",
      "  4.90289740e-03  1.41562698e-02  4.95099761e-02  3.32950708e-03\n",
      " -3.21100801e-02  4.42386977e-02  3.27416174e-02 -7.90615007e-03\n",
      "  1.07809782e-01  7.32944235e-02  3.36702913e-02 -4.28345464e-02\n",
      "  1.05966479e-02  2.05653962e-02 -2.02670172e-02  1.04964012e-02\n",
      " -1.97615772e-02 -2.88944684e-05 -2.61862464e-02 -1.85174048e-02\n",
      " -3.44269238e-02 -4.08621430e-02  2.32571736e-02  2.14196034e-02\n",
      "  1.31321121e-02 -3.27211283e-02 -1.91425923e-02 -2.86572333e-02\n",
      " -1.16859516e-02  1.21911177e-02  1.05248373e-02 -3.39584760e-02\n",
      "  3.08908918e-03 -4.44889218e-02  2.65105348e-02  1.09536964e-02\n",
      "  2.51450650e-02 -6.48831856e-03  4.54358337e-03 -2.02784929e-02\n",
      " -1.03216078e-02  2.06590295e-02 -1.65313687e-02 -2.45611854e-02\n",
      "  5.47549501e-02  2.68261749e-02  2.95510460e-02  3.86754759e-02\n",
      " -7.76720196e-02  3.80055718e-02 -2.98364516e-02  7.96886459e-02\n",
      " -3.00943721e-02  7.57843722e-03 -6.89827055e-02 -2.92667113e-02\n",
      " -2.35580392e-02  3.48198265e-02  2.52938941e-02 -4.53817062e-02\n",
      " -1.57939326e-02  4.39031310e-02 -4.04336192e-02  8.32525920e-03\n",
      " -2.84665134e-02  4.94934022e-02  2.41277050e-02  3.02192159e-02\n",
      " -4.99590859e-02 -5.94533049e-02 -3.70175913e-02  1.30330371e-02\n",
      " -3.36469449e-02  3.45590003e-02 -1.44523019e-02  2.57639959e-02\n",
      "  4.61184606e-03  2.21552271e-02 -4.93462663e-03  9.66005325e-02\n",
      " -2.72445683e-03  5.65377064e-04 -3.24248634e-02  1.31681915e-02\n",
      "  4.41607572e-02 -7.03057088e-03  6.84260949e-02 -2.28166934e-02\n",
      " -2.81036785e-03 -4.23883311e-02 -1.33632636e-02 -5.96738271e-02\n",
      " -6.96125673e-03 -2.31901556e-02 -3.78850996e-02  9.80186611e-02\n",
      " -2.21728757e-02 -2.30062902e-02  3.22815105e-02  8.21806025e-03\n",
      " -7.04122754e-03  4.84080017e-02  4.23291251e-02 -2.59717042e-03\n",
      "  1.20471035e-04  1.67415012e-02  2.91197933e-02 -1.28741134e-02\n",
      " -2.41077952e-02 -3.29320729e-02 -3.50292865e-03 -3.19322310e-02\n",
      " -2.64171865e-02  2.30404269e-02  1.11637134e-02 -9.95999575e-03\n",
      " -1.75901521e-02 -2.00271793e-03  1.21594714e-02  4.67823260e-02\n",
      "  5.20882271e-02  7.21172020e-02  1.94978435e-02  1.15072597e-02\n",
      "  5.94161730e-03 -1.47833778e-02 -2.87724063e-02  6.72666356e-02\n",
      " -1.68758985e-02  5.25875343e-03 -3.39739285e-02  5.24596125e-02\n",
      " -2.59793326e-02 -4.41380404e-02  1.47450063e-03 -1.06599741e-02\n",
      " -1.51859671e-02 -1.55883259e-03  1.81505065e-02 -4.85410988e-02\n",
      "  3.67909460e-03 -6.59313202e-02 -1.49418693e-02 -3.23529467e-02\n",
      " -2.79948972e-02  1.71856470e-02 -7.87082128e-03  4.65691797e-02\n",
      "  1.47122983e-02 -7.40439445e-02 -6.52104691e-02 -5.22734970e-02\n",
      " -1.82343870e-02  5.20858988e-02  3.06305196e-02 -2.36037634e-02\n",
      "  2.42384747e-02 -1.83940101e-02 -4.83014388e-03 -2.13386565e-02\n",
      "  1.56584475e-02  9.87345725e-03 -4.25561927e-02  6.00459427e-03\n",
      " -3.14495387e-03  4.51518642e-03 -1.52780127e-03  1.13731576e-02\n",
      " -6.96354136e-02 -3.37258354e-02  1.33406948e-02  4.87282872e-03\n",
      " -7.81487674e-03  4.78049442e-02 -1.59711502e-02  3.14606912e-02\n",
      "  5.15920036e-02 -4.05122377e-02 -5.06460369e-02  9.99939721e-03\n",
      " -2.00730190e-02  4.21552658e-02  3.03183030e-02 -1.00431196e-01\n",
      " -4.12019864e-02  3.43990661e-02  3.29209231e-02  1.07404892e-03\n",
      "  3.70962024e-02 -6.94237649e-02  6.52393922e-02  8.31659138e-03\n",
      "  1.68036465e-02 -2.60705408e-02  8.14496446e-03 -1.48009285e-02\n",
      " -2.65670978e-02  3.29322442e-02 -7.37516535e-03  7.23975664e-03\n",
      " -2.69329920e-02  1.71755459e-02 -2.28083264e-02 -4.75338940e-03\n",
      "  2.88569257e-02  1.30799526e-04  5.44129126e-02 -1.43379085e-02\n",
      "  1.89892184e-02 -1.32732037e-02  4.01178338e-02 -7.29276091e-02\n",
      " -2.41210870e-02  3.16217132e-02 -1.68014448e-02  8.47542565e-03\n",
      " -5.23940809e-02 -1.43882744e-02 -1.46156540e-02  6.39909226e-03\n",
      "  2.15114113e-02 -5.18960245e-02 -4.30576690e-02  2.34075934e-02\n",
      "  2.30277167e-03 -2.48434469e-02 -4.38243374e-02 -2.16570012e-02\n",
      " -6.91595376e-02  1.76770426e-02  2.12066695e-02 -2.20293738e-02\n",
      " -1.06773414e-02  9.57425963e-03  2.21989341e-02  5.51471300e-02\n",
      "  1.03683192e-02 -8.14824626e-02 -7.94703420e-03 -1.85866561e-02\n",
      "  1.20494831e-02  7.51403570e-02 -1.41215539e-02  8.83924738e-02\n",
      "  3.12628709e-02  8.12258106e-03 -2.29444560e-02  3.96010689e-02\n",
      " -2.00168565e-02  9.16027948e-02 -2.06839293e-02  5.84991351e-02\n",
      " -4.32368368e-02 -4.74740798e-03 -9.51884501e-03  5.42946486e-03\n",
      "  1.19173827e-04  6.15376756e-02 -1.68788899e-03 -4.66320775e-02\n",
      " -2.01405473e-02  1.32406307e-02  9.70681757e-03  2.73847375e-02\n",
      "  3.06639634e-02  6.71582203e-03  8.71221125e-02 -1.97370094e-03\n",
      "  8.18300527e-03  5.44357067e-03 -5.76205701e-02  1.34820407e-02\n",
      "  5.06295916e-03 -2.10569277e-02  1.25937127e-02 -5.49785467e-03\n",
      " -1.44645572e-02 -2.92567331e-02  5.53298481e-02 -2.60098707e-02\n",
      " -2.82454048e-03 -2.30901670e-02  8.89703259e-03 -2.61565559e-02\n",
      "  9.08619899e-04 -6.16203584e-02 -7.56419972e-02 -1.05930977e-02\n",
      " -1.19564068e-02  6.71458617e-02 -1.96234714e-02 -5.00933863e-02\n",
      " -3.91228944e-02 -3.07007656e-02  7.18906820e-02  9.29253642e-03\n",
      " -6.34384807e-03  7.87006167e-04 -1.36483843e-02  2.87188608e-02\n",
      "  4.01224419e-02  1.28037557e-02  1.77381635e-02 -4.75981133e-03\n",
      "  5.47172949e-02  1.10807957e-03 -2.25790124e-02 -2.80284905e-03\n",
      " -1.13696098e-01  2.55904756e-02  4.00391145e-04 -4.39811721e-02\n",
      "  1.36319408e-02 -1.54137406e-02 -4.99015711e-02 -2.32889783e-02\n",
      " -1.62987784e-03  3.95835675e-02  1.89040620e-02 -3.02703176e-02\n",
      "  2.71437895e-02  1.05677743e-03 -4.21070009e-02  3.71960737e-02\n",
      "  3.54258232e-02 -6.98274076e-02 -2.20937934e-02 -4.01496291e-02\n",
      " -1.90164261e-02 -2.69835312e-02 -1.51213026e-02  3.33365574e-02\n",
      " -9.74889621e-02  1.73102077e-02  6.21021865e-03 -2.59430660e-03\n",
      " -1.10152990e-01 -6.10185526e-02 -1.36549780e-02 -1.35035859e-02\n",
      " -6.72574043e-02 -4.05120896e-03 -6.64984435e-03  3.86566785e-03\n",
      "  9.43471864e-03 -3.86637487e-02 -1.93593390e-02  1.34934075e-02\n",
      " -4.58107069e-02  6.06736802e-02  6.06380515e-02  4.85597476e-02\n",
      " -4.56088036e-02 -5.71936704e-02 -1.55094732e-02  3.40963118e-02\n",
      "  9.48140922e-04 -9.94352624e-03  2.84657311e-02 -3.29024307e-02\n",
      " -2.83210687e-02  3.19907814e-02  2.61299219e-02 -2.74054985e-02\n",
      " -1.36353578e-02  7.47715030e-03  1.19430326e-01 -4.45807055e-02\n",
      "  1.07671004e-02 -8.69424120e-02 -2.19551064e-02  1.83875188e-02\n",
      " -1.06521836e-02 -1.89243387e-02 -3.06513533e-02 -3.04702055e-02\n",
      " -3.22214402e-02  4.12151106e-02  8.95758346e-03 -2.73179654e-02\n",
      "  9.39997006e-03 -9.57059325e-04 -1.94010213e-02 -4.92622741e-02\n",
      " -9.18887649e-03  4.66893986e-02  5.41893281e-02  2.21609809e-02\n",
      " -2.86351684e-02  5.20295948e-02  2.47589331e-02 -7.14268610e-02\n",
      " -1.26206819e-02  7.35519361e-03  2.13784296e-02  2.93514319e-02\n",
      " -2.57651154e-02  5.20562418e-02 -2.74892021e-02 -3.10242623e-02\n",
      " -9.02879983e-02  6.10371605e-02 -5.22609726e-02  2.13111527e-02\n",
      "  4.41735461e-02  3.23371179e-02  1.75647642e-02 -2.39520315e-02\n",
      " -2.69709192e-02  5.11278920e-02  2.69065481e-02 -4.51932885e-02\n",
      "  2.52650329e-03  2.44934373e-02 -2.89540496e-02  2.79993303e-02\n",
      " -1.36022614e-02 -4.32368405e-02  1.85830444e-02  7.63538046e-05\n",
      "  2.43510026e-03 -3.73321911e-03 -1.72279906e-02  1.01292916e-02\n",
      "  1.98437665e-02 -2.60018464e-02 -3.40175047e-03  1.09125385e-02\n",
      " -4.16363962e-02  3.37031893e-02 -2.81635337e-02  1.79126244e-02\n",
      " -4.53095660e-02 -1.09819630e-02 -2.20826035e-03  1.99103076e-02\n",
      "  3.56371813e-02 -3.11800037e-02  3.78752425e-02 -1.41409170e-02\n",
      " -2.16906741e-02  2.73019318e-02  3.69820744e-03  6.35387972e-02\n",
      "  1.22669013e-02 -6.02277461e-03 -7.60759879e-03 -1.86565481e-02\n",
      " -5.64719457e-03 -2.20051664e-03 -1.31824967e-02  1.67724714e-02\n",
      " -3.77264656e-02  2.97895279e-02 -5.01570217e-02  4.89087738e-02\n",
      " -6.07444867e-02 -8.39419141e-02 -5.09002060e-02  1.81768052e-02\n",
      "  6.66732490e-02 -3.30034713e-03 -2.82399356e-03 -5.35407215e-02\n",
      "  3.90341431e-02  2.19852831e-02  3.13555859e-02 -3.36527154e-02\n",
      "  1.96914058e-02  1.67883579e-02  5.04003130e-02  3.08056851e-03\n",
      " -7.24758604e-04  4.42907400e-02 -4.12958581e-03  4.29328680e-02\n",
      " -6.62552640e-02  1.16056588e-03 -2.81716883e-02  1.56886410e-02\n",
      "  9.78133678e-02  5.53594045e-02 -1.39379455e-02  2.12306809e-02\n",
      " -1.30956406e-02 -6.82027861e-02 -8.08985147e-04  4.99292202e-02\n",
      " -2.69265324e-02 -2.97803786e-02  3.84462066e-02  1.97353978e-02\n",
      "  3.37088294e-02  1.65873915e-02  5.77318668e-03 -3.04897949e-02\n",
      " -1.52511410e-02 -3.56158912e-02 -8.69223103e-03 -5.42296870e-33\n",
      "  3.24369734e-03 -3.46330032e-02  3.58932987e-02  1.83771159e-02\n",
      " -2.17505377e-02 -3.26412171e-02  2.88397167e-03  1.50463879e-02\n",
      " -1.75257097e-03 -1.99419074e-02 -6.10348256e-03  2.23847032e-02\n",
      " -8.78957391e-04  2.48684622e-02  3.39736454e-02  2.75592804e-02\n",
      "  3.37792374e-02  3.98565158e-02  2.55545676e-02  1.83041897e-02\n",
      " -2.92878821e-02  5.18086879e-03  8.37783155e-04 -3.66560258e-02\n",
      " -3.46732922e-02  3.82686816e-02  5.50825521e-03 -4.35187854e-02\n",
      "  2.44076960e-02  3.54167186e-02 -2.13442687e-02  2.86623817e-02\n",
      " -2.65382405e-04  3.73409949e-02 -8.68165772e-03  3.04790004e-03\n",
      " -2.71681491e-02 -3.85088623e-02 -6.12388700e-02 -2.00843299e-03\n",
      " -1.22080622e-02 -8.67198333e-02  3.75365512e-03 -1.77706946e-02\n",
      "  8.32478702e-03 -1.69167407e-02  7.02404231e-02  3.32233869e-02\n",
      "  4.34313528e-02  1.47016849e-02 -1.25546902e-01  1.50866183e-02\n",
      " -5.43164127e-02 -1.79151550e-03  4.99600843e-02 -1.53786251e-02\n",
      "  3.32683139e-02 -3.07709835e-02 -1.83896124e-02  9.45798960e-03\n",
      " -4.60291691e-02 -2.03873217e-03  2.62428429e-02 -5.00788987e-02\n",
      "  2.01836098e-02  6.08982705e-02 -2.01180894e-02 -2.60054003e-02\n",
      "  1.05925174e-02 -3.31153758e-02  1.62595473e-02  7.77862519e-02\n",
      " -1.90734316e-03 -5.62886940e-03  1.43716037e-02 -4.06833515e-02\n",
      " -5.14971018e-02  1.66223195e-04 -3.33051966e-03  1.44689279e-02\n",
      "  4.24939732e-04  3.04452814e-02 -1.83636565e-02  1.51190942e-03\n",
      "  2.99861375e-02 -3.68001983e-02  8.35622940e-03 -3.31025198e-02\n",
      "  2.66911406e-02  5.47829689e-03 -1.80523917e-02  2.42577232e-02\n",
      "  5.72704291e-03 -5.93372099e-02  1.04358450e-01 -9.87922214e-03\n",
      " -1.36105409e-02  5.79998195e-02  2.50108466e-02  2.89337300e-02\n",
      " -3.20521556e-02 -3.40233482e-02 -3.41699049e-02 -2.76981704e-02\n",
      "  6.47002459e-02  1.50797972e-02 -1.61925610e-02  3.03266216e-02\n",
      " -2.67187431e-02 -3.67774144e-02 -2.27845386e-02 -5.36433905e-02\n",
      "  1.90500151e-02 -3.42503004e-02  1.32688293e-02 -5.41324588e-03\n",
      "  7.49741495e-03 -7.36962189e-04 -3.08569949e-02  3.82287540e-02\n",
      " -2.08311696e-02 -3.43154743e-02  5.60243707e-03  1.44999903e-02\n",
      " -3.76363583e-02 -5.11782952e-02 -3.51075083e-02  1.71867386e-02\n",
      "  1.50721176e-02 -9.62026566e-02 -1.53544489e-02  1.58376619e-02\n",
      "  2.42940985e-07 -5.88804483e-03  7.68795833e-02  5.86063229e-02\n",
      "  2.21233219e-02 -2.36690715e-02  5.25274314e-02  1.48661723e-02\n",
      "  7.34179281e-03 -4.98921331e-03  4.37413901e-02 -1.28331697e-02\n",
      "  3.37342098e-02 -1.10814627e-02 -1.33937728e-02 -7.80064091e-02\n",
      " -1.36331171e-02  1.94749702e-02  1.91752298e-03 -3.00251786e-02\n",
      "  1.02673082e-04  9.54533294e-02  1.19654007e-01  3.73372808e-02\n",
      "  4.25131014e-03  2.05130614e-02 -3.85415033e-02 -1.90614425e-02\n",
      "  5.88793010e-02  6.81265071e-02 -3.12595665e-02 -6.50440007e-02\n",
      "  2.48044096e-02  3.90084839e-04  7.54762441e-02 -3.46075445e-02\n",
      "  1.32949613e-02  4.14005443e-02  3.07568703e-02  5.50357299e-03\n",
      " -1.53092563e-03  2.75993850e-02  6.46029739e-03  1.05398316e-02\n",
      " -3.09297871e-02  4.60232124e-02 -3.64921093e-02 -1.39539763e-02\n",
      " -3.53720039e-02  7.97857356e-04  1.40632829e-02  1.80258583e-02\n",
      " -1.43368524e-02  2.19216570e-03 -3.96873653e-02 -1.17282374e-02\n",
      " -4.45220359e-02  8.05773679e-03 -4.04862128e-02  3.56148817e-02\n",
      "  5.12853079e-02 -6.64039329e-02 -5.32595515e-02  8.92901793e-03\n",
      "  1.56423785e-02  1.02110945e-01  8.10768548e-03 -4.03857231e-03\n",
      "  2.02352651e-34 -1.38294222e-02 -1.17623396e-02  1.51005918e-02\n",
      "  8.25895742e-02  2.39228383e-02 -1.10377856e-02  3.65664018e-03\n",
      " -7.44782854e-03  2.94555854e-02  3.53004038e-03 -6.10421635e-02]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "###############\n",
    "## EMBEDDING ##\n",
    "###############\n",
    "# Create the embedding model using Sentence Transformers. This is similar to the \n",
    "# embedding layers used in deep learning models. The embedding model here is \n",
    "# all-mpnet-base-v2, which is a general-purpose embedding model that works well\n",
    "# for many tasks. Word2Vec is another popular embedding model, but it is\n",
    "# somewhat outdated compared to more recent models like those in Sentence Transformers.\n",
    "\n",
    "# all-mpnet-base-v2 has a maximum input length of 384 tokens and produces embedded vectors\n",
    "# which have a length of 768, no matter the length of the input text (as long as it is within the limit).\n",
    "\n",
    "# If you have a GPU available, you can change device=\"cpu\" to device=\"cuda\" for faster processing.\n",
    "embedding_model = SentenceTransformer(model_name_or_path=\"all-mpnet-base-v2\",\n",
    "                                      device=\"cpu\") \n",
    "\n",
    "## EXAMPLES\n",
    "# Create a list of sentences to turn into numbers\n",
    "sentences = [\n",
    "    \"The Sentences Transformers library provides an easy and open-source way to create embeddings.\",\n",
    "    \"Sentences can be embedded one by one or as a list of strings.\",\n",
    "    \"Embeddings are one of the most powerful concepts in machine learning!\",\n",
    "    \"Learn to use embeddings well and you'll be well on your way to being an AI engineer.\"\n",
    "]\n",
    "\n",
    "# Sentences are encoded/embedded by calling model.encode()\n",
    "embeddings = embedding_model.encode(sentences)\n",
    "embeddings_dict = dict(zip(sentences, embeddings))\n",
    "\n",
    "# See the embeddings\n",
    "for sentence, embedding in embeddings_dict.items():\n",
    "    print(\"Sentence:\", sentence)\n",
    "    print(\"Embedding:\", embedding)\n",
    "    print(\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "0db19a21",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d24fc7ad37ab44c995fdfd1d8876a9a0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1843 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Send the model to the CPU. If you have a GPU, you can use \"cuda\" instead.\n",
    "# This will speed up the embedding process significantly.\n",
    "embedding_model.to(\"cpu\") \n",
    "\n",
    "# Create embeddings one by one\n",
    "for item in tqdm(pages_and_chunks):\n",
    "    item[\"embedding\"] = embedding_model.encode(item[\"sentence_chunk\"])\n",
    "\n",
    "# Turn text chunks into a single list\n",
    "text_chunks = [item[\"sentence_chunk\"] for item in pages_and_chunks]    \n",
    "\n",
    "# This takes 1.5 minutes on my Mac"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "b7d6b139",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1843, 768])\n",
      "torch.Size([1843, 768])\n"
     ]
    }
   ],
   "source": [
    "# Embed all texts in batches\n",
    "text_chunk_embeddings = embedding_model.encode(text_chunks,\n",
    "                                               batch_size=32, # you can use different batch sizes here for speed/performance, I found 32 works well for this use case\n",
    "                                               convert_to_tensor=True) # optional to return embeddings as tensor instead of array\n",
    "\n",
    "print(text_chunk_embeddings.shape)\n",
    "\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "# Convert embeddings to torch tensor and send to device (note: NumPy arrays are float64, torch tensors are float32 by default)\n",
    "embeddings = torch.tensor(np.array(text_chunk_embeddings.tolist()), dtype=torch.float32).to(device)\n",
    "print(embeddings.shape)\n",
    "\n",
    "## This takes about two minutes on my Mac"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "e9b33cc3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Query: daily value iron\n",
      "Time take to get scores on 1843 embeddings: 0.0015535410020675045 seconds.\n",
      "torch.return_types.topk(\n",
      "values=tensor([0.4900, 0.4880, 0.4782, 0.4698, 0.4340]),\n",
      "indices=tensor([1027, 1509, 1025, 1508, 1031]))\n",
      "Results:\n",
      "Score: tensor(0.4900)\n",
      "Text:\n",
      "Centers for Disease Control and Prevention.http://www.cdc.gov/nutrition/ Iron |\n",
      "661\n",
      "\n",
      "Page number: 661\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Score: tensor(0.4880)\n",
      "Text:\n",
      "Potential Iron Loss in Endurance Athletes” for the potential amounts of iron\n",
      "loss each day in male and female athletes. An increased recommendation for both\n",
      "genders are shown below. These recommendations are based on the assumption that\n",
      "iron has a 10%  absorption efficiency. As noted above, women athletes have a\n",
      "greater iron loss due to menstruation and therefore must increase their dietary\n",
      "needs more than male athletes. Table 16.3 The Potential Iron Loss in Endurance\n",
      "Athletes   Approximate Daily Iron Losses in Endurance Athletes (mg/day)and\n",
      "Increased Dietary Need Male Female Sedentary 1 1.5 Athlete 1.8 2.5 *Increase\n",
      "dietary needs 8 10 *Assumes 10% absorption efficiency Source: Weaver CM, Rajaram\n",
      "S. Exercise and iron status. J Nutr.1992 Mar;122(3\n",
      "Suppl):782-7.https://www.ncbi.nlm.nih.gov/pubmed/ 1542048. Accessed March 23,\n",
      "2018. Sports anemia, which is different from iron deficiency anemia is an\n",
      "adaptation to training for athletes.\n",
      "\n",
      "Page number: 968\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Score: tensor(0.4782)\n",
      "Text:\n",
      "Food Serving Iron (mg) Percent Daily Value Breakfast cereals, fortified 1\n",
      "serving 18 100 Oysters 3 oz. 8 44 Dark chocolate 3 oz. 7 39 Beef liver 3 oz. 5\n",
      "28 Lentils ½ c. 3 17 Spinach, boiled ½ c. 3 17 Tofu, firm ½ c. 3 17 Kidney beans\n",
      "½ c. 2 11 Sardines 3 oz. 2 11 Iron-Deficiency Anemia Iron-deficiency anemia is a\n",
      "condition that develops from having insufficient iron levels in the body\n",
      "resulting in fewer and smaller red blood cells containing lower amounts of\n",
      "hemoglobin. Regardless of the cause (be it from low dietary intake of iron or\n",
      "via excessive blood loss), iron-deficiency anemia has the following signs and\n",
      "symptoms, which are linked to the essential functions of iron in energy\n",
      "metabolism and blood health: • Fatigue • Weakness • Pale skin • Shortness of\n",
      "breath • Dizziness • Swollen, sore tongue 660 | Iron\n",
      "\n",
      "Page number: 660\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Score: tensor(0.4698)\n",
      "Text:\n",
      "Iron Status and Exercise. The American Journal of Clinical Nutrition, 72(2),\n",
      "594S–597S. Sports Nutrition | 967\n",
      "\n",
      "Page number: 967\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Score: tensor(0.4340)\n",
      "Text:\n",
      "The World Bank claims that a million deaths occur every year from anemia and\n",
      "that the majority of those Iron | 663\n",
      "\n",
      "Page number: 663\n",
      "\n",
      "\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "###############################\n",
    "## TEST THE RETRIEVAL SYSTEM ##\n",
    "################################\n",
    "# Define helper function to print wrapped text. Just makes long outputs easier to read.\n",
    "# This uses the textwrap library to wrap long text outputs.\n",
    "def print_wrapped(text, wrap_length=80):\n",
    "    wrapped_text = textwrap.fill(text, wrap_length)\n",
    "    print(wrapped_text)\n",
    "\n",
    "# Define the query, i.e., what we want to search for in the text chunks\n",
    "# Note: This could be anything. But since we're working with a nutrition textbook, \n",
    "# we'll stick with nutrition-based queries.\n",
    "query = \"daily value iron\"\n",
    "print(\"Query:\", query)\n",
    "\n",
    "# Embed the query to the same numerical space as the text examples\n",
    "# Note: It's important to embed your query with the same model you embedded your examples with.\n",
    "# This is how we will determine what chunks are most similar to the query.\n",
    "query_embedding = embedding_model.encode(query, convert_to_tensor=True)\n",
    "\n",
    "# Get similarity scores with the dot product. A dot product is a common way to measure similarity\n",
    "# as it determines how closely aligned two vectors are in space. The higher the dot product, the\n",
    "# closer the vectors are. Two overlapping vectors will have a dot product of 1 (i.e. they point in the \n",
    "# same direction). \n",
    "\n",
    "\n",
    "# Also measure the time taken to compute the scores.\n",
    "start_time = timer()\n",
    "dot_scores = util.dot_score(a=query_embedding, b=embeddings)[0]\n",
    "end_time = timer()\n",
    "\n",
    "print(\"Time take to get scores on\", len(embeddings), \"embeddings:\", end_time-start_time, \"seconds.\")\n",
    "\n",
    "# 4. Get the top-k results (we'll keep this to 5)\n",
    "# You can change k to get more or fewer results.\n",
    "top_results_dot_product = torch.topk(dot_scores, k=5)\n",
    "print(top_results_dot_product)\n",
    "\n",
    "# Print the results (in text not tokens) of the top-k most similar chunks\n",
    "print(\"Results:\")\n",
    "# Loop through zipped together scores and indicies from torch.topk\n",
    "for score, idx in zip(top_results_dot_product[0], top_results_dot_product[1]):\n",
    "    print(\"Score:\", score)\n",
    "    # Print relevant sentence chunk (since the scores are in descending order, the most relevant chunk will be first)\n",
    "    print(\"Text:\")\n",
    "    print_wrapped(pages_and_chunks[idx][\"sentence_chunk\"])\n",
    "    print()\n",
    "    # Print the page number too so we can reference the textbook further (and check the results)\n",
    "    print(\"Page number:\", pages_and_chunks[idx]['page_number'])\n",
    "    print(\"\\n\\n\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d13eee48",
   "metadata": {},
   "source": [
    "### Generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "a073753f",
   "metadata": {},
   "outputs": [],
   "source": [
    "######################\n",
    "## LLM MODEL SET UP ##\n",
    "######################\n",
    "\n",
    "# If you do not have GPU keep this as true, if you do have GPU you can set to false.\n",
    "# This has to do with how the model runs on the reduced resource settings of a CPU.\n",
    "use_quantization_config = True\n",
    "\n",
    "# Below are two model options. The 270M parameter model is smaller and faster, but the\n",
    "# 1B parameter model is more powerful. If you have a GPU you should be able to run a larger\n",
    "# model, just be careful about VRAM usage. The 1B parameter model can use up to 8GB of VRAM\n",
    "# during inference, so make sure you have enough available. If you run into issues, try\n",
    "# the smaller model. If you do not have a GPU, stick with the smaller model (1 billion parameters\n",
    "# or less).\n",
    "\n",
    "# The below models are both from Google and are open-source. You can find them on Hugging Face:\n",
    "# https://huggingface.co/google/functiongemma-270m-it\n",
    "# https://huggingface.co/google/gemma-3-1b-it\n",
    "# If you have never used Hugging Face before, you may need to create a free account\n",
    "# and accept the model license before you can download them. Specifically, use one of the\n",
    "# above links to access the model page, then accept the license terms from Google. You also\n",
    "# need to be logged in via the huggingface_hub library (see the start of this notebook).\n",
    "# Feel free to experiment with other models as well, but make sure they fit within your\n",
    "# hardware limits.\n",
    "\n",
    "#model_id = \"google/functiongemma-270m-it\"\n",
    "model_id = \"google/gemma-3-1b-it\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "aca1b7dc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] Using attention implementation: sdpa\n",
      "[INFO] Using model_id: google/gemma-3-1b-it\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`torch_dtype` is deprecated! Use `dtype` instead!\n"
     ]
    }
   ],
   "source": [
    "# Create quantization config for smaller model loading (optional)\n",
    "# For models that require 4-bit quantization (use this if you have low GPU memory available)\n",
    "# You do not need to use this if you have a more powerful GPU with more VRAM.\n",
    "quantization_config = BitsAndBytesConfig(load_in_4bit=True,\n",
    "                                         bnb_4bit_compute_dtype=torch.float16)\n",
    "\n",
    "# Scaled Dot-Product Attention (other options maybe avaliable)\n",
    "# Flash is avaliable for the Gemma models with an Nvidia GPU\n",
    "attn_implementation = \"sdpa\"\n",
    "print(\"[INFO] Using attention implementation:\", attn_implementation)\n",
    "\n",
    "print(\"[INFO] Using model_id:\", model_id)\n",
    "\n",
    "# Instantiate tokenizer (tokenizer turns text into numbers ready for the model)\n",
    "tokenizer = AutoTokenizer.from_pretrained(pretrained_model_name_or_path=model_id)\n",
    "\n",
    "# 4. Instantiate the model\n",
    "llm_model = AutoModelForCausalLM.from_pretrained(pretrained_model_name_or_path=model_id,\n",
    "                                                 torch_dtype=torch.float16, # datatype to use, we want float16\n",
    "                                                 quantization_config=quantization_config if use_quantization_config else None,\n",
    "                                                 low_cpu_mem_usage=False, # use full memory (CHANGE THIS IF NEEDED)\n",
    "                                                 attn_implementation=attn_implementation) # which attention version to use\n",
    "\n",
    "if not use_quantization_config: # quantization takes care of device setting automatically, so if it's not used, send model to GPU\n",
    "    llm_model.to(\"cuda\")\n",
    "\n",
    "# This takes approximately 5 minutes on my Mac for the 270k parameter model for the first time.\n",
    "# After the first time, loading is much faster since the model is cached locally."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "3ebd0b9e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input text:\n",
      " What are the macronutrients, and what roles do they play in the human body?\n",
      "\n",
      "Prompt (formatted):\n",
      " <bos><start_of_turn>user\n",
      "What are the macronutrients, and what roles do they play in the human body?<end_of_turn>\n",
      "<start_of_turn>model\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#########################################\n",
    "## TEST THE MODEL WITHOUT AUGMENTATION ##\n",
    "##########################################\n",
    "input_text = \"What are the macronutrients, and what roles do they play in the human body?\"\n",
    "print(\"Input text:\\n\", input_text)\n",
    "\n",
    "# Create prompt template for instruction-tuned model\n",
    "# This should be the same for most models chosen form Hugging Face that are instruction-tuned\n",
    "# but check the model card to be sure.\n",
    "dialogue_template = [\n",
    "    {\"role\": \"user\",\n",
    "     \"content\": input_text}\n",
    "]\n",
    "\n",
    "# Apply the chat template to format the prompt (needs to be tokenized)\n",
    "prompt = tokenizer.apply_chat_template(conversation=dialogue_template,\n",
    "                                       tokenize=False, # keep as raw text (not tokenized)\n",
    "                                       add_generation_prompt=True)\n",
    "print(\"\\nPrompt (formatted):\\n\", prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b247e7d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model input (tokenized):\n",
      " {'input_ids': tensor([[     2,      2,    105,   2364,    107,   3689,    659,    506, 216955,\n",
      "         151268, 236764,    532,   1144,  13616,    776,    901,   1441,    528,\n",
      "            506,   3246,   2742, 236881,    106,    107,    105,   4368,    107]]), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1]])} \n",
      "\n",
      "Model output (tokens):\n",
      " tensor([     2,      2,    105,   2364,    107,   3689,    659,    506, 216955,\n",
      "        151268, 236764,    532,   1144,  13616,    776,    901,   1441,    528,\n",
      "           506,   3246,   2742, 236881,    106,    107,    105,   4368,    107,\n",
      "         19058, 236764,   1531, 236789, 236751,   2541,   1679,    506,   3853,\n",
      "           529, 216955, 151268,    528,    506,   3246,   2742, 236888,    108,\n",
      "          1018,  19253,   1863, 151268,    753,    669,  16087,  82801,   1018,\n",
      "           108,  19253,   1863, 151268,    659,    506,  28935,    822,   2742,\n",
      "          3548,    528,   2455,  12136, 236761,   2195,    659,    506,   5905,\n",
      "          2780,   4402,    600,   2847,   2778,    532,   1894,    784,    506,\n",
      "          5151,    529,    822,   2742, 236761,    108,   8291, 236789, 236751,\n",
      "           496,  25890,    529,   1546, 236787,    108,   1018, 236770, 236761,\n",
      "        191286,  14473,   1018,    108, 236829,   5213,   3689,    659,    901,\n",
      "        236881,   1018,    138, 182329,   1731,    568,   5282,  22422, 236764,\n",
      "        122031, 236764,   4044,   2907,    532,   4381,   2391, 236761,    107,\n",
      "        236829,   5213,  21114,  53121,    138,  93000,   2778, 236761,   2195,\n",
      "        236789,    500,  11207,   1679,   1131,  22422, 236764,    837,  37685,\n",
      "           822,   3874,    573,   6376,   5085, 236761,    107, 236829,   5213,\n",
      "         12285,  53121,    107,    140, 236829,   5213,  22575, 191286,  14473,\n",
      "         53121,    568,   5282,   2633,   9792,    753,  22422, 236768,  39525,\n",
      "          3823,   2778,    840,   2729,   2268,  23226,   2778, 236761,    107,\n",
      "           140, 236829,   5213,  48333, 191286,  14473,  53121,    568,   5282,\n",
      "          3697,  27075, 236764,  16062, 236768,  39525,    919,  10385,   2778,\n",
      "           532,  14770, 236761,    107, 236829,   5213,  71122,  53121,    138,\n",
      "        103250,    573,   7875,   1292,    532,   4806,   9792,   2256, 236761,\n",
      "           109,   1018, 236778, 236761, 108486,   1018,    108, 236829,   5213,\n",
      "          3689,    659,    901, 236881,   1018,    138,  42619,    872,    529,\n",
      "         16986,  13834,    753,    506,   3788,  11802,    529,   1972, 236761,\n",
      "           107, 236829,   5213,  21114,  53121,    138, 103250,    573,   3788,\n",
      "           532,  61691,  21346,    568,  16603,  19064, 236764,  26832, 236764,\n",
      "          5324, 236764,   5716, 236764,    532,   1032,  28560,    769,    107,\n",
      "        236829,   5213,  12285,  53121,    107,    140, 236829,   5213,  25146,\n",
      "        108486,  53121,  83167,    784]) \n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Send model to bfloat16 for faster inference (optional, depending on model and hardware)\n",
    "# I got numerical errors for the probability calculations without this.\n",
    "llm_model.bfloat16()\n",
    "\n",
    "# Tokenize the input text (turn it into numbers) and send it to CPU/GPU\n",
    "input_ids = tokenizer(prompt, return_tensors=\"pt\").to(\"cpu\")\n",
    "print(\"Model input (tokenized):\\n\", input_ids, \"\\n\")\n",
    "\n",
    "# Generate outputs passed on the tokenized input\n",
    "outputs = llm_model.generate(**input_ids,\n",
    "                             max_new_tokens=256) # define the maximum number of new tokens to create\n",
    "                                                 # This can be changed as needed.\n",
    "# Print the raw output from the LLM, which is in token form\n",
    "print(\"Model output (tokens):\\n\", outputs[0], \"\\n\")\n",
    "\n",
    "# Takes approximately 6 minutes on my Mac for the 1 billion parameter model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "ea69e180",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model output (decoded):\n",
      " <bos><bos><start_of_turn>user\n",
      "What are the macronutrients, and what roles do they play in the human body?<end_of_turn>\n",
      "<start_of_turn>model\n",
      "Okay, let's break down the role of macronutrients in the human body!\n",
      "\n",
      "**Macronutrients - The Building Blocks**\n",
      "\n",
      "Macronutrients are the nutrients your body needs in large amounts. They are the primary food groups that provide energy and support all the functions of your body.\n",
      "\n",
      "Here's a breakdown of each:\n",
      "\n",
      "**1. Carbohydrates**\n",
      "\n",
      "* **What are they?**  Sugars (like glucose, fructose, etc.) and starches.\n",
      "* **Role:**  Provide energy. They're broken down into glucose, which fuels your cells for daily activities.\n",
      "* **Types:**\n",
      "    * **Simple Carbohydrates:** (like table sugar - glucose) Provide quick energy but offer little sustained energy.\n",
      "    * **Complex Carbohydrates:** (like whole grains, vegetables) Provide more stable energy and fiber.\n",
      "* **Important:**  Essential for brain function and blood sugar control.\n",
      "\n",
      "\n",
      "**2. Proteins**\n",
      "\n",
      "* **What are they?**  Made up of amino acids - the building blocks of life.\n",
      "* **Role:**  Essential for building and repairing tissues (including muscles, bones, hair, skin, and other organs).\n",
      "* **Types:**\n",
      "    * **Complete Proteins:** Contain all \n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Decode the output tokens to text (so we can read it)\n",
    "# Note that most LLMs produce output using markdown formatting, so the output\n",
    "# may contain markdown syntax (e.g., **bold**, _italic_, etc.). There are also\n",
    "# special characters in brackets that tell the model to do certain things. The\n",
    "# user prompts is also here.\n",
    "outputs_decoded = tokenizer.decode(outputs[0])\n",
    "print(\"Model output (decoded):\\n\", outputs_decoded, \"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3960437f",
   "metadata": {},
   "source": [
    "### Augmentation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "1ea94e4b",
   "metadata": {},
   "outputs": [],
   "source": [
    "###########################################\n",
    "## TEST THE MODEL WITH AUGMENTED INPUTS  ##\n",
    "###########################################\n",
    "# Define a list of nutrition-style questions to test the retrieval-augmented generation (RAG\n",
    "\n",
    "# Nutrition-style questions generated with GPT4\n",
    "gpt4_questions = [\n",
    "    \"What are the macronutrients, and what roles do they play in the human body?\",\n",
    "    \"How do vitamins and minerals differ in their roles and importance for health?\",\n",
    "    \"Describe the process of digestion and absorption of nutrients in the human body.\",\n",
    "    \"What role does fibre play in digestion? Name five fibre containing foods.\",\n",
    "    \"Explain the concept of energy balance and its importance in weight management.\"\n",
    "]\n",
    "\n",
    "# Manually created question list\n",
    "manual_questions = [\n",
    "    \"How often should infants be breastfed?\",\n",
    "    \"What are symptoms of pellagra?\",\n",
    "    \"How does saliva help with digestion?\",\n",
    "    \"What is the RDI for protein per day?\",\n",
    "    \"water soluble vitamins\"\n",
    "]\n",
    "\n",
    "query_list = gpt4_questions + manual_questions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "459a94e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a function to do the retrieval and print the results\n",
    "# Note that you can change n_resources_to_return to get more or fewer results.\n",
    "# All returned results will be used to augment the prompt for the LLM.\n",
    "def retrieve_relevant_resources(query: str,\n",
    "                                embeddings: torch.tensor,\n",
    "                                model: SentenceTransformer=embedding_model,\n",
    "                                n_resources_to_return: int=5,\n",
    "                                print_time: bool=True):\n",
    "    \"\"\"\n",
    "    Embeds a query with model and returns top k scores and indices from embeddings.\n",
    "    \"\"\"\n",
    "\n",
    "    # Embed the query\n",
    "    query_embedding = model.encode(query,\n",
    "                                   convert_to_tensor=True)\n",
    "\n",
    "    # Get dot product scores on embeddings\n",
    "    start_time = timer()\n",
    "    dot_scores = util.dot_score(query_embedding, embeddings)[0]\n",
    "    end_time = timer()\n",
    "\n",
    "    if print_time:\n",
    "        print(\"[INFO] Time taken to get scores on\", len(embeddings), \"embeddings:\", end_time-start_time, \"seconds.\")\n",
    "\n",
    "    scores, indices = torch.topk(input=dot_scores,\n",
    "                                 k=n_resources_to_return)\n",
    "\n",
    "    return scores, indices\n",
    "\n",
    "# Function to format and print the top results in a nice manner\n",
    "def print_top_results_and_scores(query: str,\n",
    "                                 embeddings: torch.tensor,\n",
    "                                 pages_and_chunks: list[dict]=pages_and_chunks,\n",
    "                                 n_resources_to_return: int=5):\n",
    "    \"\"\"\n",
    "    Takes a query, retrieves most relevant resources and prints them out in descending order.\n",
    "\n",
    "    Note: Requires pages_and_chunks to be formatted in a specific way (see above for reference).\n",
    "    \"\"\"\n",
    "\n",
    "    scores, indices = retrieve_relevant_resources(query=query,\n",
    "                                                  embeddings=embeddings,\n",
    "                                                  n_resources_to_return=n_resources_to_return)\n",
    "\n",
    "    print(\"Query:\", query, \"\\n\")\n",
    "    print(\"Results:\")\n",
    "    # Loop through zipped together scores and indicies\n",
    "    for score, index in zip(scores, indices):\n",
    "        print(\"Score:\", score, \"\\n\")\n",
    "        # Print relevant sentence chunk (since the scores are in descending order, the most relevant chunk will be first)\n",
    "        print_wrapped(pages_and_chunks[index][\"sentence_chunk\"])\n",
    "        # Print the page number too so we can reference the textbook further and check the results\n",
    "        print(\"Page number:\", pages_and_chunks[index][\"page_number\"])\n",
    "        print(\"\\n\")\n",
    "\n",
    "# Define a prompt formatter function to create augmented prompts based on the retrieved information\n",
    "def prompt_formatter(query: str,\n",
    "                     context_items: list[dict]) -> str:\n",
    "    \"\"\"\n",
    "    Augments query with text-based context from context_items.\n",
    "    \"\"\"\n",
    "    # Join context items into one dotted paragraph\n",
    "    context = \"- \" + \"\\n- \".join([item[\"sentence_chunk\"] for item in context_items])\n",
    "\n",
    "    # Create a base prompt with examples to help the model\n",
    "    # Note: this is very customizable, I've chosen to use 3 examples of the answer style we'd like.\n",
    "    # We could also write this in a txt file and import it in if we wanted.\n",
    "\n",
    "    # Change this as you like to get different answer styles. Look up examples of prompt engineering for more ideas.\n",
    "    base_prompt = \"\"\"Based on the following context items, please answer the query.\n",
    "Give yourself room to think by extracting relevant passages from the context before answering the query.\n",
    "Don't return the thinking, only return the answer.\n",
    "Make sure your answers are as explanatory as possible.\n",
    "Use the following examples as reference for the ideal answer style.\n",
    "\\nExample 1:\n",
    "Query: What are the fat-soluble vitamins?\n",
    "Answer: The fat-soluble vitamins include Vitamin A, Vitamin D, Vitamin E, and Vitamin K. These vitamins are absorbed along with fats in the diet and can be stored in the body's fatty tissue and liver for later use. Vitamin A is important for vision, immune function, and skin health. Vitamin D plays a critical role in calcium absorption and bone health. Vitamin E acts as an antioxidant, protecting cells from damage. Vitamin K is essential for blood clotting and bone metabolism.\n",
    "\\nExample 2:\n",
    "Query: What are the causes of type 2 diabetes?\n",
    "Answer: Type 2 diabetes is often associated with overnutrition, particularly the overconsumption of calories leading to obesity. Factors include a diet high in refined sugars and saturated fats, which can lead to insulin resistance, a condition where the body's cells do not respond effectively to insulin. Over time, the pancreas cannot produce enough insulin to manage blood sugar levels, resulting in type 2 diabetes. Additionally, excessive caloric intake without sufficient physical activity exacerbates the risk by promoting weight gain and fat accumulation, particularly around the abdomen, further contributing to insulin resistance.\n",
    "\\nExample 3:\n",
    "Query: What is the importance of hydration for physical performance?\n",
    "Answer: Hydration is crucial for physical performance because water plays key roles in maintaining blood volume, regulating body temperature, and ensuring the transport of nutrients and oxygen to cells. Adequate hydration is essential for optimal muscle function, endurance, and recovery. Dehydration can lead to decreased performance, fatigue, and increased risk of heat-related illnesses, such as heat stroke. Drinking sufficient water before, during, and after exercise helps ensure peak physical performance and recovery.\n",
    "\\nNow use the following context items to answer the user query:\n",
    "{context}\n",
    "\\nRelevant passages: <extract relevant passages from the context here>\n",
    "User query: {query}\n",
    "Answer:\"\"\"\n",
    "\n",
    "    # Update base prompt with context items and query\n",
    "    base_prompt = base_prompt.format(context=context, query=query)\n",
    "\n",
    "    # Create prompt template for instruction-tuned model\n",
    "    dialogue_template = [\n",
    "        {\"role\": \"user\",\n",
    "        \"content\": base_prompt}\n",
    "    ]\n",
    "\n",
    "    # Apply the chat template\n",
    "    prompt = tokenizer.apply_chat_template(conversation=dialogue_template,\n",
    "                                          tokenize=False,\n",
    "                                          add_generation_prompt=True)\n",
    "    return prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "468978e9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Query: What is the RDI for protein per day?\n",
      "[INFO] Time taken to get scores on 1843 embeddings: 0.0010961250009131618 seconds.\n",
      "<bos><start_of_turn>user\n",
      "Based on the following context items, please answer the query.\n",
      "Give yourself room to think by extracting relevant passages from the context before answering the query.\n",
      "Don't return the thinking, only return the answer.\n",
      "Make sure your answers are as explanatory as possible.\n",
      "Use the following examples as reference for the ideal answer style.\n",
      "\n",
      "Example 1:\n",
      "Query: What are the fat-soluble vitamins?\n",
      "Answer: The fat-soluble vitamins include Vitamin A, Vitamin D, Vitamin E, and Vitamin K. These vitamins are absorbed along with fats in the diet and can be stored in the body's fatty tissue and liver for later use. Vitamin A is important for vision, immune function, and skin health. Vitamin D plays a critical role in calcium absorption and bone health. Vitamin E acts as an antioxidant, protecting cells from damage. Vitamin K is essential for blood clotting and bone metabolism.\n",
      "\n",
      "Example 2:\n",
      "Query: What are the causes of type 2 diabetes?\n",
      "Answer: Type 2 diabetes is often associated with overnutrition, particularly the overconsumption of calories leading to obesity. Factors include a diet high in refined sugars and saturated fats, which can lead to insulin resistance, a condition where the body's cells do not respond effectively to insulin. Over time, the pancreas cannot produce enough insulin to manage blood sugar levels, resulting in type 2 diabetes. Additionally, excessive caloric intake without sufficient physical activity exacerbates the risk by promoting weight gain and fat accumulation, particularly around the abdomen, further contributing to insulin resistance.\n",
      "\n",
      "Example 3:\n",
      "Query: What is the importance of hydration for physical performance?\n",
      "Answer: Hydration is crucial for physical performance because water plays key roles in maintaining blood volume, regulating body temperature, and ensuring the transport of nutrients and oxygen to cells. Adequate hydration is essential for optimal muscle function, endurance, and recovery. Dehydration can lead to decreased performance, fatigue, and increased risk of heat-related illnesses, such as heat stroke. Drinking sufficient water before, during, and after exercise helps ensure peak physical performance and recovery.\n",
      "\n",
      "Now use the following context items to answer the user query:\n",
      "- Most nitrogen is lost as urea in the urine, but urea is also excreted in the feces. Proteins are also lost in sweat and as hair and nails grow. The RDA, therefore, is the amount of protein a person should consume in their diet to balance the amount of protein used up and lost from the body. For healthy adults, this amount of protein was determined to be 0.8 grams of protein per kilogram of body weight. You can calculate 410 | Proteins, Diet, and Personal Choices\n",
      "- Proteins, Diet, and Personal Choices UNIVERSITY OF HAWAI‘I AT MĀNOA FOOD SCIENCE AND HUMAN NUTRITION PROGRAM AND HUMAN NUTRITION PROGRAM We have discussed what proteins are, how they are made, how they are digested and absorbed, the many functions of proteins in the body, and the consequences of having too little or too much protein in the diet. This section will provide you with information on how to determine the recommended amount of protein for you, and your many choices in designing an optimal diet with high-quality protein sources. How Much Protein Does a Person Need in Their Diet? The recommendations set by the IOM for the Recommended Daily Allowance (RDA) and AMDR for protein for different age groups are listed in Table 6.2 “Dietary Reference Intakes for Protein”. A Tolerable Upper Intake Limit for protein has not been set, but it is recommended that you do not exceed the upper end of the AMDR. Table 6.2 Dietary Reference Intakes for Protein Proteins, Diet, and Personal Choices | 409\n",
      "- Age Group Protein (%) Carbohydrates (%) Fat (%) Children (1–3) 5–20 45–65 30–40 Children and Adolescents (4–18) 10–30 45–65 25–35 Adults (>19) 10–35 45–65 20–35 Source: Food and Nutrition Board of the Institute of Medicine. Dietary Reference Intakes for Energy, Carbohydrate, Fat, Fatty Acids, Cholesterol, Protein, and Amino Acids. http://www.nationalacademies.org/hmd/~/media/Files/ Activity%20Files/Nutrition/DRI-Tables/ 8_Macronutrient%20Summary.pdf?la=en. Published 2002. Accessed November 22, 2017.   Tips for Using the Dietary Reference Intakes to Plan Your Diet You can use the DRIs to help assess and plan your diet. Keep in mind when evaluating your nutritional intake that the values established have been devised with an ample safety margin and should be used as guidance for optimal intakes. Also, the values are meant to assess and plan average intake over time; that is, you don’t need to meet these recommendations every single day—meeting them on average over several days is sufficient. Understanding Dietary Reference Intakes | 715\n",
      "- Age Group RDA (g/day) AMDR (% calories) Infants (0–6 mo) 9.1* Not determined Infants (7–12 mo) 11.0 Not determined Children (1–3) 13.0 5–20 Children (4–8) 19.0 10–30 Children (9–13) 34.0 10–30 Males (14–18) 52.0 10–30 Females (14–18) 46.0 10–30 Adult Males (19+) 56.0 10–35 Adult Females (19+) 46.0 10–35 * Denotes Adequate Intake Source: Dietary Reference Intakes: Macronutrients. Dietary Reference Intakes for Energy, Carbohydrate, Fiber, Fat, Fatty Acids, Cholesterol, Protein, and Amino Acids. Institute of Medicine. September 5, 2002. Accessed September 28, 2017. Protein Input = Protein Used by the Body + Protein Excreted The appropriate amount of protein in a person’s diet is that which maintains a balance between what is taken in and what is used. The RDAs for protein were determined by assessing nitrogen balance. Nitrogen is one of the four basic elements contained in all amino acids. When proteins are broken down and amino acids are catabolized, nitrogen is released. Remember that when the liver breaks down amino acids, it produces ammonia, which is rapidly converted to nontoxic, nitrogen-containing urea, which is then transported to the kidneys for excretion.\n",
      "- percent of the population meets their nutrient need is the EAR, and the point at which 97 to 98 percent of the population meets their needs is the RDA. The UL is the highest level at which you can consume a nutrient without it being too much—as nutrient intake increases beyond the UL, the risk of health problems resulting from that nutrient increases. Source: Dietary Reference Intakes Tables and Application. The National Academies of Science, Engineering, and Medicine.  Health and Medicine Division. http://nationalacademies.org/HMD/ Activities/Nutrition/SummaryDRIs/DRI-Tables.aspx. Accessed November 22, 2017. The Acceptable Macronutrient Distribution Range (AMDR) is the calculated range of how much energy from carbohydrates, fats, and protein is recommended for a healthy diet adequate of the essential nutrients and is associated with a reduced risk of chronic disease. The ranges listed in Table 12.1 “Acceptable Macronutrient Distribution Ranges (AMDR) For Various Age Groups” allows individuals to personalize their diets taking into consideration that different subgroups in a population often require different requirements. The DRI committee recommends using the midpoint of the AMDRs as an approach to focus on moderation2.\n",
      "\n",
      "Relevant passages: <extract relevant passages from the context here>\n",
      "User query: What is the RDI for protein per day?\n",
      "Answer:<end_of_turn>\n",
      "<start_of_turn>model\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "RAG answer:\n",
      " <bos>The recommendation for protein intake per day is based on the amount of protein used up and lost from the body. The RDA, or Recommended Daily Allowance, is the amount of protein a person should consume to balance the amount of protein used up and lost from the body. The AMDR for adults is 0.8 grams of protein per kilogram of body weight. This amount is recommended for healthy adults. The AMDR for children is 10–30 grams of protein per day. It's important to note that the AMDR for adults is for healthy individuals, and the recommendations are based on the assumption that adults are healthy.<end_of_turn>\n"
     ]
    }
   ],
   "source": [
    "# Choose a random query from the list\n",
    "query = random.choice(query_list)\n",
    "print(\"Query:\", query)\n",
    "\n",
    "# Get relevant text for the query using the default values, so five\n",
    "# pieces of context will be returned.\n",
    "scores, indices = retrieve_relevant_resources(query=query,\n",
    "                                              embeddings=embeddings)\n",
    "\n",
    "# Create a list of context items\n",
    "context_items = [pages_and_chunks[i] for i in indices]\n",
    "\n",
    "# Format prompt with context items\n",
    "prompt = prompt_formatter(query=query,\n",
    "                          context_items=context_items)\n",
    "print(prompt)\n",
    "print(\"\\n\\n\\n\\n\")\n",
    "\n",
    "# Tokenize the prompt and send to CPU/GPU\n",
    "input_ids = tokenizer(prompt, return_tensors=\"pt\").to(\"cpu\")\n",
    "\n",
    "# Generate an output of tokens\n",
    "outputs = llm_model.generate(**input_ids,\n",
    "                             temperature=0.7, # lower temperature = more deterministic outputs, higher temperature = more creative outputs\n",
    "                             do_sample=True, # whether or not to use sampling, see https://huyenchip.com/2024/01/16/sampling.html for more\n",
    "                             max_new_tokens=256) # how many new tokens to generate from prompt\n",
    "\n",
    "# Turn the output tokens into text\n",
    "output_text = tokenizer.decode(outputs[0])\n",
    "\n",
    "print(\"RAG answer:\\n\", output_text.replace(prompt, ''))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54f0cfc7",
   "metadata": {},
   "source": [
    "## Additional Information\n",
    "* [Retrieval-Augmented Generation for Knowledge-Intensive NLP Tasks (Original Paper)](https://arxiv.org/abs/2005.11401)\n",
    "* [Simple Local RAG](https://github.com/dinocodesx/simple-local-rag/tree/master)\n",
    "* [Building a RAG Pipeline from Scratch with PyTorch and Transformers](https://python.plainenglish.io/building-a-rag-pipeline-from-scratch-with-pytorch-and-transformers-b52e5504cde2)\n",
    "* [Local Retrieval Augmented Generation (RAG) from Scratch (step by step tutorial) (Video)](https://www.youtube.com/watch?v=qN_2fnOPY-M)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86c8f2c9",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
