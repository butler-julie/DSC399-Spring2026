{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1c7f7cdc",
   "metadata": {},
   "source": [
    "# Large Language Models\n",
    "\n",
    "Author: Julie Butler\n",
    "\n",
    "Date Created: November 25, 2025\n",
    "\n",
    "Last Modified: December 1, 2025\n",
    "\n",
    "In this assignment we will attempt to create a (very small) large language model (LLM) which can generate text similar to Shakespeare. The data set we will be using today comes from Andrej Karpathy and is used in his blog post [The Unreasonable Effectiveness of Recurrent Neural Networks](https://karpathy.github.io/2015/05/21/rnn-effectiveness/). Note that this blog post is included in Ilya Sutskever's Top 30 Machine Learning Papers, a list of papers covering 90% of the current field of machine learning and a good primer for advanced studies in modern machine learning and artificial intelligence. Summaries of the papers can be found [here](https://aman.ai/primers/ai/top-30-papers/), though I do suggest that you read them yourself if you plan on pursuing this field (anyone want to join me in reading them over Christmas break?). Andrej Karpathy also has an excellent [YouTube channel](https://www.youtube.com/@AndrejKarpathy) that includes some very good (and very long) videos on making rather complex LLMs from scratch."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8083bc29",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/butlerju/Library/Python/3.9/lib/python/site-packages/urllib3/__init__.py:35: NotOpenSSLWarning: urllib3 v2 only supports OpenSSL 1.1.1+, currently the 'ssl' module is compiled with 'LibreSSL 2.8.3'. See: https://github.com/urllib3/urllib3/issues/3020\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "#############\n",
    "## IMPORTS ##\n",
    "#############\n",
    "import tensorflow as tf\n",
    "import tensorflow_datasets as tfds\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "01a9fc16",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tfds.core.DatasetInfo(\n",
      "    name='tiny_shakespeare',\n",
      "    full_name='tiny_shakespeare/1.0.0',\n",
      "    description=\"\"\"\n",
      "    40,000 lines of Shakespeare from a variety of Shakespeare's plays. Featured in\n",
      "    Andrej Karpathy's blog post 'The Unreasonable Effectiveness of Recurrent Neural\n",
      "    Networks': http://karpathy.github.io/2015/05/21/rnn-effectiveness/.\n",
      "    \n",
      "    To use for e.g. character modelling:\n",
      "    \n",
      "    ```\n",
      "    d = tfds.load(name='tiny_shakespeare')['train']\n",
      "    d = d.map(lambda x: tf.strings.unicode_split(x['text'], 'UTF-8'))\n",
      "    # train split includes vocabulary for other splits\n",
      "    vocabulary = sorted(set(next(iter(d)).numpy()))\n",
      "    d = d.map(lambda x: {'cur_char': x[:-1], 'next_char': x[1:]})\n",
      "    d = d.unbatch()\n",
      "    seq_len = 100\n",
      "    batch_size = 2\n",
      "    d = d.batch(seq_len)\n",
      "    d = d.batch(batch_size)\n",
      "    ```\n",
      "    \"\"\",\n",
      "    homepage='https://github.com/karpathy/char-rnn/blob/master/data/tinyshakespeare/input.txt',\n",
      "    data_dir='/Users/butlerju/tensorflow_datasets/tiny_shakespeare/1.0.0',\n",
      "    file_format=tfrecord,\n",
      "    download_size=1.06 MiB,\n",
      "    dataset_size=1.06 MiB,\n",
      "    features=FeaturesDict({\n",
      "        'text': Text(shape=(), dtype=string),\n",
      "    }),\n",
      "    supervised_keys=None,\n",
      "    disable_shuffling=False,\n",
      "    splits={\n",
      "        'test': <SplitInfo num_examples=1, num_shards=1>,\n",
      "        'train': <SplitInfo num_examples=1, num_shards=1>,\n",
      "        'validation': <SplitInfo num_examples=1, num_shards=1>,\n",
      "    },\n",
      "    citation=\"\"\"@misc{\n",
      "      author={Karpathy, Andrej},\n",
      "      title={char-rnn},\n",
      "      year={2015},\n",
      "      howpublished={\\url{https://github.com/karpathy/char-rnn}}\n",
      "    }\"\"\",\n",
      ")\n",
      "\n",
      "First Citizen:\n",
      "Before we proceed any further, hear me speak.\n",
      "\n",
      "All:\n",
      "Speak, speak.\n",
      "\n",
      "First Citizen:\n",
      "You\n",
      "\n",
      "1003854\n"
     ]
    }
   ],
   "source": [
    "#####################\n",
    "## IMPORT THE DATA ##\n",
    "#####################\n",
    "ds, info = tfds.load(\"tiny_shakespeare\", split=\"train\", with_info=True)\n",
    "text = ds.take(1).as_numpy_iterator().__next__()[\"text\"].decode()\n",
    "\n",
    "print(info)\n",
    "print()\n",
    "\n",
    "# Print a sample and the length of the data set.\n",
    "print(text[0:100])\n",
    "print()\n",
    "print(len(text))\n",
    "\n",
    "# Truncate the data set to make for faster training. You can change\n",
    "# or remove this it get better performance.\n",
    "truncate = 50000\n",
    "text = text[:truncate]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "5bd6cb06",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['', '[UNK]', 'the', 'and', 'to', 'you', 'i', 'of', 'a', 'in', 'that', 'he', 'marcius', 'not', 'for', 'your', 'him', 'with', 'my', 'it', 'is', 'have', 'as', 'they', 'we', 'be', 'his', 'are', 'their', 'our', 'first', 'but', 'menenius', 'me', 'all', 'what', 'good', 'shall', 'this', 'will', 'than', 'if', 'o', 'no', 'well', 'cominius', 'at', 'us', 'them', 'so']\n"
     ]
    }
   ],
   "source": [
    "##################\n",
    "## TOKENIZATION ##\n",
    "#################\n",
    "seq_length = 128\n",
    "\n",
    "tokenizer = tf.keras.layers.TextVectorization(\n",
    "    max_tokens=8000,\n",
    "    output_sequence_length=None,  # <-- we want full-length tokens\n",
    "    standardize=\"lower_and_strip_punctuation\",\n",
    ")\n",
    "tokenizer.adapt([text])\n",
    "\n",
    "vocab = tokenizer.get_vocabulary()\n",
    "vocab_size = len(vocab)\n",
    "\n",
    "full_tokens = tokenizer(tf.constant([text]))\n",
    "full_tokens = tf.squeeze(full_tokens, axis=0)  \n",
    "full_tokens = tf.cast(full_tokens, tf.int32)\n",
    "\n",
    "# Print a sample of the vocab list\n",
    "print(vocab[:50])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "949815d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "#########################\n",
    "## FORMAT THE DATA SET ##\n",
    "#########################\n",
    "def make_dataset(tokens, seq_len):\n",
    "    # Manually doing time series formatting, could change this to use\n",
    "    # TimeSeriesGenerator.\n",
    "    N = len(tokens)\n",
    "    X = []\n",
    "    Y = []\n",
    "    for i in range(N - seq_len - 1):\n",
    "        X.append(tokens[i : i + seq_len])\n",
    "        Y.append(tokens[i + 1 : i + seq_len + 1])\n",
    "    X = tf.stack(X)\n",
    "    Y = tf.stack(Y)\n",
    "    return tf.data.Dataset.from_tensor_slices((X, Y))\n",
    "\n",
    "dataset = make_dataset(full_tokens, seq_length)\n",
    "dataset = dataset.shuffle(2000).batch(32).prefetch(tf.data.AUTOTUNE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "695def78",
   "metadata": {},
   "outputs": [],
   "source": [
    "##########################\n",
    "## TRANSFORMER FUNCTION ##\n",
    "#########################\n",
    "def transformer(x, embed_dim, num_heads, ff_dim):\n",
    "    # Attention Network\n",
    "    attn_out = tf.keras.layers.MultiHeadAttention(\n",
    "        num_heads=num_heads,\n",
    "        key_dim=embed_dim // num_heads\n",
    "    )(x, x)\n",
    "    x = tf.keras.layers.LayerNormalization()(x + attn_out)\n",
    "\n",
    "    # Feedforward Neural Network\n",
    "    ff = tf.keras.Sequential([\n",
    "        tf.keras.layers.Dense(ff_dim, activation=\"relu\"),\n",
    "        tf.keras.layers.Dense(embed_dim),\n",
    "    ])\n",
    "    x = tf.keras.layers.LayerNormalization()(x + ff(x))\n",
    "    return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "b6882203",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"functional_5\"</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1mModel: \"functional_5\"\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━┓\n",
       "┃<span style=\"font-weight: bold\"> Layer (type)        </span>┃<span style=\"font-weight: bold\"> Output Shape      </span>┃<span style=\"font-weight: bold\">    Param # </span>┃<span style=\"font-weight: bold\"> Connected to      </span>┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━┩\n",
       "│ input_layer_3       │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)       │          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ -                 │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">InputLayer</span>)        │                   │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ embedding_1         │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">48</span>)   │     <span style=\"color: #00af00; text-decoration-color: #00af00\">99,360</span> │ input_layer_3[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]… │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Embedding</span>)         │                   │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ multi_head_attenti… │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">48</span>)   │      <span style=\"color: #00af00; text-decoration-color: #00af00\">9,408</span> │ embedding_1[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>… │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">MultiHeadAttentio…</span> │                   │            │ embedding_1[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>] │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ add_4 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Add</span>)         │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">48</span>)   │          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ embedding_1[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>… │\n",
       "│                     │                   │            │ multi_head_atten… │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ layer_normalizatio… │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">48</span>)   │         <span style=\"color: #00af00; text-decoration-color: #00af00\">96</span> │ add_4[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]       │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">LayerNormalizatio…</span> │                   │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ sequential_2        │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">48</span>)   │      <span style=\"color: #00af00; text-decoration-color: #00af00\">7,032</span> │ layer_normalizat… │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Sequential</span>)        │                   │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ add_5 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Add</span>)         │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">48</span>)   │          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ layer_normalizat… │\n",
       "│                     │                   │            │ sequential_2[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">…</span> │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ layer_normalizatio… │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">48</span>)   │         <span style=\"color: #00af00; text-decoration-color: #00af00\">96</span> │ add_5[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]       │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">LayerNormalizatio…</span> │                   │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ multi_head_attenti… │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">48</span>)   │      <span style=\"color: #00af00; text-decoration-color: #00af00\">9,408</span> │ layer_normalizat… │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">MultiHeadAttentio…</span> │                   │            │ layer_normalizat… │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ add_6 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Add</span>)         │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">48</span>)   │          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ layer_normalizat… │\n",
       "│                     │                   │            │ multi_head_atten… │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ layer_normalizatio… │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">48</span>)   │         <span style=\"color: #00af00; text-decoration-color: #00af00\">96</span> │ add_6[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]       │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">LayerNormalizatio…</span> │                   │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ sequential_3        │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">48</span>)   │      <span style=\"color: #00af00; text-decoration-color: #00af00\">7,032</span> │ layer_normalizat… │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Sequential</span>)        │                   │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ add_7 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Add</span>)         │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">48</span>)   │          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ layer_normalizat… │\n",
       "│                     │                   │            │ sequential_3[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">…</span> │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ layer_normalizatio… │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">48</span>)   │         <span style=\"color: #00af00; text-decoration-color: #00af00\">96</span> │ add_7[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]       │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">LayerNormalizatio…</span> │                   │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ dense_9 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)     │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">2070</span>) │    <span style=\"color: #00af00; text-decoration-color: #00af00\">101,430</span> │ layer_normalizat… │\n",
       "└─────────────────────┴───────────────────┴────────────┴───────────────────┘\n",
       "</pre>\n"
      ],
      "text/plain": [
       "┏━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━┓\n",
       "┃\u001b[1m \u001b[0m\u001b[1mLayer (type)       \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape     \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m   Param #\u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mConnected to     \u001b[0m\u001b[1m \u001b[0m┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━┩\n",
       "│ input_layer_3       │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m128\u001b[0m)       │          \u001b[38;5;34m0\u001b[0m │ -                 │\n",
       "│ (\u001b[38;5;33mInputLayer\u001b[0m)        │                   │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ embedding_1         │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m128\u001b[0m, \u001b[38;5;34m48\u001b[0m)   │     \u001b[38;5;34m99,360\u001b[0m │ input_layer_3[\u001b[38;5;34m0\u001b[0m]… │\n",
       "│ (\u001b[38;5;33mEmbedding\u001b[0m)         │                   │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ multi_head_attenti… │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m128\u001b[0m, \u001b[38;5;34m48\u001b[0m)   │      \u001b[38;5;34m9,408\u001b[0m │ embedding_1[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m… │\n",
       "│ (\u001b[38;5;33mMultiHeadAttentio…\u001b[0m │                   │            │ embedding_1[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m] │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ add_4 (\u001b[38;5;33mAdd\u001b[0m)         │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m128\u001b[0m, \u001b[38;5;34m48\u001b[0m)   │          \u001b[38;5;34m0\u001b[0m │ embedding_1[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m… │\n",
       "│                     │                   │            │ multi_head_atten… │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ layer_normalizatio… │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m128\u001b[0m, \u001b[38;5;34m48\u001b[0m)   │         \u001b[38;5;34m96\u001b[0m │ add_4[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]       │\n",
       "│ (\u001b[38;5;33mLayerNormalizatio…\u001b[0m │                   │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ sequential_2        │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m128\u001b[0m, \u001b[38;5;34m48\u001b[0m)   │      \u001b[38;5;34m7,032\u001b[0m │ layer_normalizat… │\n",
       "│ (\u001b[38;5;33mSequential\u001b[0m)        │                   │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ add_5 (\u001b[38;5;33mAdd\u001b[0m)         │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m128\u001b[0m, \u001b[38;5;34m48\u001b[0m)   │          \u001b[38;5;34m0\u001b[0m │ layer_normalizat… │\n",
       "│                     │                   │            │ sequential_2[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m…\u001b[0m │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ layer_normalizatio… │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m128\u001b[0m, \u001b[38;5;34m48\u001b[0m)   │         \u001b[38;5;34m96\u001b[0m │ add_5[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]       │\n",
       "│ (\u001b[38;5;33mLayerNormalizatio…\u001b[0m │                   │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ multi_head_attenti… │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m128\u001b[0m, \u001b[38;5;34m48\u001b[0m)   │      \u001b[38;5;34m9,408\u001b[0m │ layer_normalizat… │\n",
       "│ (\u001b[38;5;33mMultiHeadAttentio…\u001b[0m │                   │            │ layer_normalizat… │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ add_6 (\u001b[38;5;33mAdd\u001b[0m)         │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m128\u001b[0m, \u001b[38;5;34m48\u001b[0m)   │          \u001b[38;5;34m0\u001b[0m │ layer_normalizat… │\n",
       "│                     │                   │            │ multi_head_atten… │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ layer_normalizatio… │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m128\u001b[0m, \u001b[38;5;34m48\u001b[0m)   │         \u001b[38;5;34m96\u001b[0m │ add_6[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]       │\n",
       "│ (\u001b[38;5;33mLayerNormalizatio…\u001b[0m │                   │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ sequential_3        │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m128\u001b[0m, \u001b[38;5;34m48\u001b[0m)   │      \u001b[38;5;34m7,032\u001b[0m │ layer_normalizat… │\n",
       "│ (\u001b[38;5;33mSequential\u001b[0m)        │                   │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ add_7 (\u001b[38;5;33mAdd\u001b[0m)         │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m128\u001b[0m, \u001b[38;5;34m48\u001b[0m)   │          \u001b[38;5;34m0\u001b[0m │ layer_normalizat… │\n",
       "│                     │                   │            │ sequential_3[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m…\u001b[0m │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ layer_normalizatio… │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m128\u001b[0m, \u001b[38;5;34m48\u001b[0m)   │         \u001b[38;5;34m96\u001b[0m │ add_7[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]       │\n",
       "│ (\u001b[38;5;33mLayerNormalizatio…\u001b[0m │                   │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ dense_9 (\u001b[38;5;33mDense\u001b[0m)     │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m128\u001b[0m, \u001b[38;5;34m2070\u001b[0m) │    \u001b[38;5;34m101,430\u001b[0m │ layer_normalizat… │\n",
       "└─────────────────────┴───────────────────┴────────────┴───────────────────┘\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">234,054</span> (914.27 KB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Total params: \u001b[0m\u001b[38;5;34m234,054\u001b[0m (914.27 KB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">234,054</span> (914.27 KB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m234,054\u001b[0m (914.27 KB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "###########################################################\n",
    "## BUILD LARGE LANGUAGE MODEL (I.E. LINKED TRANSFORMERS) ##\n",
    "###########################################################\n",
    "# Parameters\n",
    "embed_dim = 36\n",
    "num_heads = 2\n",
    "ff_dim = 64\n",
    "\n",
    "inputs = tf.keras.Input(shape=(seq_length,), dtype=tf.int32)\n",
    "\n",
    "x = tf.keras.layers.Embedding(vocab_size, embed_dim)(inputs)\n",
    "\n",
    "x = transformer(x, embed_dim, num_heads, ff_dim)\n",
    "\n",
    "x = transformer(x, embed_dim, num_heads, ff_dim)\n",
    "\n",
    "outputs = tf.keras.layers.Dense(vocab_size)(x)\n",
    "\n",
    "model = tf.keras.Model(inputs, outputs)\n",
    "model.compile(optimizer=\"adam\", loss=\"sparse_categorical_crossentropy\")\n",
    "\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "b70c4290",
   "metadata": {},
   "outputs": [],
   "source": [
    "#######################\n",
    "## GENERATE NEW TEXT ##\n",
    "#######################\n",
    "def generate(model, start_text, max_tokens=50):\n",
    "    text_so_far = start_text\n",
    "\n",
    "    for _ in range(max_tokens):\n",
    "        # Tokenize the current text\n",
    "        token_ids = tokenizer([text_so_far]) \n",
    "        token_ids = tf.squeeze(token_ids, axis=0).numpy().tolist() \n",
    "        # Keep only last 128 tokens\n",
    "        token_ids = token_ids[-seq_length:]\n",
    "        # LEFT-PAD with zeros if shorter than seq_length\n",
    "        # Can replace this with pad_sequences if you want.\n",
    "        if len(token_ids) < seq_length:\n",
    "            pad_len = seq_length - len(token_ids)\n",
    "            token_ids = [0] * pad_len + token_ids\n",
    "        token_tensor = tf.constant([token_ids], dtype=tf.int32)\n",
    "        # Predict next token\n",
    "        preds = model.predict(token_tensor, verbose=0)\n",
    "        next_id = int(tf.argmax(preds[0, -1]).numpy())\n",
    "        # Convert id → word\n",
    "        next_word = vocab[next_id]\n",
    "        text_so_far += \" \" + next_word\n",
    "\n",
    "    return text_so_far\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7697dd34",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/3\n",
      "\u001b[1m276/276\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m16s\u001b[0m 56ms/step - loss: 7.6335\n",
      "Epoch 2/3\n",
      "\u001b[1m201/276\u001b[0m \u001b[32m━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━\u001b[0m \u001b[1m4s\u001b[0m 57ms/step - loss: 7.6332"
     ]
    }
   ],
   "source": [
    "###########\n",
    "## TRAIN ##\n",
    "###########\n",
    "model.fit(dataset, epochs=3)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07ecfdef",
   "metadata": {},
   "outputs": [],
   "source": [
    "#############\n",
    "## EXAMPLE ##\n",
    "#############\n",
    "print(generate(model, \"To be, or not to be\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "558c6619",
   "metadata": {},
   "source": [
    "## Assignment\n",
    "\n",
    "### Part 1: Understanding the Code\n",
    "1. Go through the code and provide comments as to what is happening.\n",
    "\n",
    "Answer the following questions thoroughly, including adding citations to any information learned outside of course materials.\n",
    "\n",
    "2. What does the tokenizer do? What is its vocabulary?\n",
    "3. How does the code create input/target sequences for a language model?\n",
    "4. Describe the self-attention mechanism in the model. Why does it take (x, x) as arguments?\n",
    "5. Explain the purpose of:\n",
    "    * Embedding layer\n",
    "    * Multi-head attention\n",
    "    * Residual connections\n",
    "    * LayerNormalization\n",
    "    * Feedforward block\n",
    "5. Why does the model use SparseCategoricalCrossentropy?\n",
    "6. How does the generation function work? What is greedy decoding?\n",
    "\n",
    "### Part 2: Modify the Code\n",
    "1. Attempt to make the model better by adjusting the following parameters:`embed_dim`, `num_heads`, `ff_dim`, `output_sequence_length`, `seq_length`, and `epochs`. You can also change the truncation of the training data, activation functions, and any other parameters you like. Comment on how changing these parameters changes the model's performance. Note, I would advise against drastically increasing the parameters without testing first as the run times can easily sky rocket.\n",
    "2. The above LLM only has two transformer blocks. Increase this to three and comment on the change in performance.\n",
    "3. Increasing the above parameters and architecture still results in a simple LLM. Make two or more of the above changes to the model which will make it more complex, and hopefully better able to generate text. Explain what each change does to the model in a comment or markdown cell.\n",
    "    * Change the tokenization strategy from the word level to the sub-word or character level. Note the tiny-shakespeare dataset is usually tokenized at the character level.\n",
    "    * Add positional embeddings (see last week's assignment).\n",
    "    * Change the attention to casual (masked).\n",
    "    * Add an adaptive learning rate or a learning rate scheduler.\n",
    "    * Add dropout.\n",
    "    * Add temperature, top-k, or top-p sampling.\n",
    "    * Change the decoding strategy (related to the sampling above).\n",
    "    \n",
    "### Part 3: Text Generation\n",
    "What combination of changes made in Part 2 gave you the best output (i.e. that sounds like it could be Shakespeare)? What was your favorite output?\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b542488",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
