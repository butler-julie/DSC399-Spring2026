{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "35ea8d08",
   "metadata": {},
   "source": [
    "# Homwork: Recurrent Neural Networks for Natural Language Processing\n",
    "\n",
    "The goal of the below RNN network is to predict the _sentiment_ of a movie review (rather it is positive or negative) based on the text from the movie review. The data set is already split into training and test sets and the words from the movie reviews have already been tokenized."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e5862138",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/juliehartley/Library/Python/3.9/lib/python/site-packages/urllib3/__init__.py:35: NotOpenSSLWarning: urllib3 v2 only supports OpenSSL 1.1.1+, currently the 'ssl' module is compiled with 'LibreSSL 2.8.3'. See: https://github.com/urllib3/urllib3/issues/3020\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "#############\n",
    "## IMPORTS ##\n",
    "#############\n",
    "# A data set of movie reviews from tensorflow which is already\n",
    "# formatted for natural language processing (it has been \n",
    "# pre-processed for you)\n",
    "from tensorflow.keras.datasets import imdb\n",
    "\n",
    "# This is used to reduce the length of the training data to \n",
    "# decrease the ammount of time needed for training\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "\n",
    "# Mostly tensorflow NN imports that have been used before\n",
    "# except for embedding which will be discussed later in the\n",
    "# notebook\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Embedding, LSTM, Dense"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4dc1885",
   "metadata": {},
   "outputs": [],
   "source": [
    "##############################\n",
    "## FORMATTING THE DATA SET  ##\n",
    "#############################\n",
    "# What the the maximum number of unique words which are allowed\n",
    "# in a review, ranked by the popularity of the words. The default\n",
    "# value in this notebook is to only have reviews that include the \n",
    "# 10,000 most used words. Limiting the number of words in reviews\n",
    "# limits the number of tokens and thus the training time.\n",
    "vocab_size = 10000\n",
    "maxlen = 200\n",
    "\n",
    "(X_train, y_train), (X_test, y_test) = imdb.load_data(num_words=vocab_size)\n",
    "\n",
    "# Pad sequences (Look up what this does.)\n",
    "X_train = pad_sequences(X_train, maxlen=maxlen)\n",
    "X_test = pad_sequences(X_test, maxlen=maxlen)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "4d55564c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[   5   25  100   43  838  112   50  670    2    9   35  480  284    5\n",
      "  150    4  172  112  167    2  336  385   39    4  172 4536 1111   17\n",
      "  546   38   13  447    4  192   50   16    6  147 2025   19   14   22\n",
      "    4 1920 4613  469    4   22   71   87   12   16   43  530   38   76\n",
      "   15   13 1247    4   22   17  515   17   12   16  626   18    2    5\n",
      "   62  386   12    8  316    8  106    5    4 2223 5244   16  480   66\n",
      " 3785   33    4  130   12   16   38  619    5   25  124   51   36  135\n",
      "   48   25 1415   33    6   22   12  215   28   77   52    5   14  407\n",
      "   16   82    2    8    4  107  117 5952   15  256    4    2    7 3766\n",
      "    5  723   36   71   43  530  476   26  400  317   46    7    4    2\n",
      " 1029   13  104   88    4  381   15  297   98   32 2071   56   26  141\n",
      "    6  194 7486   18    4  226   22   21  134  476   26  480    5  144\n",
      "   30 5535   18   51   36   28  224   92   25  104    4  226   65   16\n",
      "   38 1334   88   12   16  283    5   16 4472  113  103   32   15   16\n",
      " 5345   19  178   32]\n",
      "200\n"
     ]
    }
   ],
   "source": [
    "# Display the first point in the X component of the \n",
    "# training data. Also print its length.\n",
    "# The X data are just lists of numbers representing\n",
    "# the movie review text\n",
    "print(X_train[0])\n",
    "print(len(X_train[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "cb525a54",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n"
     ]
    }
   ],
   "source": [
    "# Display the corresponding y value\n",
    "# Which is just a single number\n",
    "print(y_train[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f19fddde",
   "metadata": {},
   "source": [
    "To get an idea as to what the movie reviews actually look like, we can use the following chunk of code modified from [the tensorflow documentation page for the data set](https://www.tensorflow.org/api_docs/python/tf/keras/datasets/imdb/get_word_index). Play around with the below code to get an idea of what the reviews look like before they are tokenized."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1369eae6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"[START] this film was just brilliant casting location scenery story direction everyone's really suited the part they played and you could just imagine being there robert redford's is an amazing actor and now the same being director norman's father came from the same scottish island as myself so i loved the fact there was a real connection with this film the witty remarks throughout the film were great it was just brilliant so much that i bought the film as soon as it was released for retail and would recommend it to everyone to watch and the fly fishing was amazing really cried at the end it was so sad and you know what they say if you cry at a film it must have been good and this definitely was also congratulations to the two little boy's that played the part's of norman and paul they were just brilliant children are often left out of the praising list i think because the stars that play them all grown up are such a big profile for the whole film but these children are amazing and should be praised for what they have done don't you think the whole story was so lovely because it was true and was someone's life after all that was shared with us all\""
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Use the default parameters to keras.datasets.imdb.load_data\n",
    "# Try different values and see how they change the code.\n",
    "start_char = 1\n",
    "oov_char = 2\n",
    "index_from = 3\n",
    "# Retrieve the training sequences.\n",
    "(x_train, _), _ = imdb.load_data(\n",
    "    start_char=start_char, oov_char=oov_char, index_from=index_from\n",
    ")\n",
    "# Retrieve the word index file mapping words to indices\n",
    "word_index = imdb.get_word_index()\n",
    "# Reverse the word index to obtain a dict mapping indices to words\n",
    "# And add `index_from` to indices to sync with `x_train`\n",
    "inverted_word_index = dict(\n",
    "    (i + index_from, word) for (word, i) in word_index.items()\n",
    ")\n",
    "# Update `inverted_word_index` to include `start_char` and `oov_char`\n",
    "inverted_word_index[start_char] = \"[START]\"\n",
    "inverted_word_index[oov_char] = \"[OOV]\"\n",
    "# Decode the first sequence in the dataset\n",
    "decoded_sequence = \" \".join(inverted_word_index[i] for i in x_train[0])\n",
    "\n",
    "decoded_sequence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c1ab313",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " embedding (Embedding)       (None, 200, 128)          1280000   \n",
      "                                                                 \n",
      " lstm (LSTM)                 (None, 64)                49408     \n",
      "                                                                 \n",
      " dense (Dense)               (None, 1)                 65        \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 1329473 (5.07 MB)\n",
      "Trainable params: 1329473 (5.07 MB)\n",
      "Non-trainable params: 0 (0.00 Byte)\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "####################\n",
    "## CREATE THE RNN ##\n",
    "####################\n",
    "# Define the RNN as a sequential neural network\n",
    "model_lstm = Sequential()\n",
    "\n",
    "# Add an embedding layer (Look this up)\n",
    "# This converts word indices into dense vectors of fixed size\n",
    "# input_dim = vocabulary size\n",
    "# output_dim = size of embedding vectors\n",
    "# input_length = length of input sequences\n",
    "model_lstm.add(Embedding(vocab_size, 128, input_length=maxlen))\n",
    "\n",
    "# Add an LSTM RNN layer with 64 neurons\n",
    "model_lstm.add(LSTM(64))\n",
    "\n",
    "# Add a dense layer to post-process the results of the LSTM\n",
    "# layer and produce the output. Needs to have only a single \n",
    "# neuron as the output is one dimensional.\n",
    "model_lstm.add(Dense(1, activation='sigmoid'))\n",
    "\n",
    "# Use binary cross-entropy and accuracy as this is a classification\n",
    "# problem and the adam optimizer\n",
    "model_lstm.compile(loss='binary_crossentropy', optimizer='adam', \n",
    "                   metrics=['accuracy'])\n",
    "\n",
    "# Print a summary of the model\n",
    "model_lstm.summary()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "766f5898",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/3\n",
      "625/625 [==============================] - 56s 88ms/step - loss: 0.4205 - accuracy: 0.8041 - val_loss: 0.3111 - val_accuracy: 0.8726\n",
      "Epoch 2/3\n",
      "625/625 [==============================] - 54s 86ms/step - loss: 0.2446 - accuracy: 0.9052 - val_loss: 0.3242 - val_accuracy: 0.8666\n",
      "Epoch 3/3\n",
      "625/625 [==============================] - 54s 87ms/step - loss: 0.1685 - accuracy: 0.9365 - val_loss: 0.3763 - val_accuracy: 0.8638\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.src.callbacks.History at 0x3147c3970>"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "##############\n",
    "## TRAINING ##\n",
    "#############\n",
    "# Use a validation data set to improve the accuracy of the model\n",
    "# Start with a small number of epochs to attempt to prevent \n",
    "# overfitting\n",
    "model_lstm.fit(X_train, y_train, validation_split=0.2, epochs=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "21902119",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "782/782 [==============================] - 17s 22ms/step - loss: 0.3959 - accuracy: 0.8564\n",
      "LSTM Accuracy: 0.8563600182533264\n"
     ]
    }
   ],
   "source": [
    "###############\n",
    "## ACCURACY ##\n",
    "##############\n",
    "loss, accuracy = model_lstm.evaluate(X_test, y_test)\n",
    "print(f\"LSTM Accuracy:\", accuracy)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51c604c4",
   "metadata": {},
   "source": [
    "## Assignment\n",
    "0. Go through the above code and add more comments to more throughly explain what everything does, how everything works, and why it is there.\n",
    "1. What is natural language processing? What is the goal and where is it used?\n",
    "2. Research the IMDB data set and describe what is present in the X data, what is represented by the y data, and what the numerical encodings mean? How did tensorflow process the data set from movie reviews to numbers?\n",
    "3. Perform hyperparameter tuning on the above network to maximize your accuracy. Play around with the parameters of the embedding layer. Adjust the number of LSTM layers or dense layers and any numeric values. Try different activation and loss function. You can also try formatting the data set using one-hot encoding. What is your highest accuracy and what model gave you that accuracy?\n",
    "4. Redo the above network with GRU units instead of LSTM units. Add this network down below as to not alter your aboved work. Which type of RNN layer performs better?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "420c07d1",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
