{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "WprpkMcniZXw"
   },
   "source": [
    "# Attention and Transformer Networks\n",
    "\n",
    "In this notebook you will use a transformer to perform natural language processing. The below code is modified from [this example](https://keras.io/examples/nlp/text_classification_with_transformer/) from the Keras documentation. Note that the website is using an older version, and thus some of the syntax is outdated. The concepts are still valid.\n",
    "\n",
    "Read through the code and then see the assignment at the bottom of the notebook. Note that training a transformer model (even a simple one) can take a bit of time.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "id": "HFrVwO1mgIii"
   },
   "outputs": [],
   "source": [
    "#############\n",
    "## IMPORTS ##\n",
    "############\n",
    "# Keras imports to create the transformer\n",
    "import keras\n",
    "from keras import ops\n",
    "from keras import layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "id": "SM1KPM8kgLI0"
   },
   "outputs": [],
   "source": [
    "############################\n",
    "## CREATE THE TRANSFORMER ##\n",
    "############################\n",
    "# Create a class that defines a transformer. It\n",
    "# inherits from the keras Layer class so it will\n",
    "# have all of the functionality of that class\n",
    "# as well such as fit and predict. We will overwrite\n",
    "# the initialization function and the call function but\n",
    "# the other will stay the same.\n",
    "\n",
    "# If you are unfamiliar with inheritance in Python (or\n",
    "# inheritance in general) I recommend you do a bit of reseach\n",
    "# into the topic before continuing\n",
    "class TransformerBlock(layers.Layer):\n",
    "    # Define the initialization function which takes three\n",
    "    # arguments:\n",
    "    # embed_dim is the embedding size for the tokens,\n",
    "    # num_heads is the number of heads for the attention layer,\n",
    "    # ff_dim is the size of the feedforward neural network.\n",
    "    # The dropout rate of the model can also be changed though a\n",
    "    # default is provided.\n",
    "    # QUESTION 1: Do some research and provide context for the\n",
    "    # embedding size and the number of heads for the attention layer.\n",
    "    # The Keras or Tensorflow documentation may be helpful in addition\n",
    "    # to a general internet search. Remember to cite any sources which are\n",
    "    # external to course materials (i.e. provided lecture notes and the textbook).\n",
    "    def __init__(self, embed_dim, num_heads, ff_dim, rate=0.1):\n",
    "        # Initialize the parent class\n",
    "        super().__init__()\n",
    "        # Define a multi-head attention network using the provided\n",
    "        # parameters\n",
    "        self.att = layers.MultiHeadAttention(num_heads=num_heads,\n",
    "                                             key_dim=embed_dim)\n",
    "        # Define a feedforward neural network with the provided\n",
    "        # parameters and a relu activation function. Note you can\n",
    "        # change the activation function to be passed\n",
    "        # in the arguments (hint for assignment question 4)\n",
    "        self.ffn = keras.Sequential(\n",
    "            [layers.Dense(ff_dim, activation=\"relu\"),\n",
    "             layers.Dense(embed_dim),])\n",
    "\n",
    "        # Add normalization and dropout to improve performance. For\n",
    "        # assignment question 4 you will likely want to change these\n",
    "        # from the default values or remove them entirely\n",
    "        self.layernorm1 = layers.LayerNormalization(epsilon=1e-6)\n",
    "        self.layernorm2 = layers.LayerNormalization(epsilon=1e-6)\n",
    "        self.dropout1 = layers.Dropout(rate)\n",
    "        self.dropout2 = layers.Dropout(rate)\n",
    "\n",
    "    # Define the call function which will create the network in the order\n",
    "    # that is needed to define a transformer (i.e. an attention layer and\n",
    "    # then a feedfoward neural network). Normalization and dropout are\n",
    "    # also added but these are optional but may improve performance.\n",
    "    def call(self, inputs):\n",
    "        attn_output = self.att(inputs, inputs)\n",
    "        attn_output = self.dropout1(attn_output)\n",
    "        out1 = self.layernorm1(inputs + attn_output)\n",
    "        ffn_output = self.ffn(out1)\n",
    "        ffn_output = self.dropout2(ffn_output)\n",
    "        return self.layernorm2(out1 + ffn_output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "id": "JQ9WSTAAgOTf"
   },
   "outputs": [],
   "source": [
    "#########################################\n",
    "## TOKEN AND POSISTION EMBEDDING CLASS ##\n",
    "#########################################\n",
    "# Create a class, which also inherits from the Keras Layers class that\n",
    "# will tokenize and then embedd the data. Tokenizing and embedding here\n",
    "# are similar (if not exactly the same) to concepts we covered last week.\n",
    "# Question 2: Compare tokenization and position embedding as they relate\n",
    "# to training a transformer to tokenization and sequence length as they\n",
    "# relate to recurrent neural networks. You may need to do some outside\n",
    "# reasearch here. If you use external resources please cite them.\n",
    "class TokenAndPositionEmbedding(layers.Layer):\n",
    "    # Define the initilization function. We will only consider phrases\n",
    "    # who have a tokenized length under a maximum length (maxlen). This\n",
    "    # is not required but does speed up the process.\n",
    "    def __init__(self, maxlen, vocab_size, embed_dim):\n",
    "        # Initialize the parent class\n",
    "        super().__init__()\n",
    "        # Define tokenization and position embedding functions from\n",
    "        # Keras layers\n",
    "        self.token_emb = layers.Embedding(input_dim=vocab_size,\n",
    "                                          output_dim=embed_dim)\n",
    "        self.pos_emb = layers.Embedding(input_dim=maxlen,\n",
    "                                        output_dim=embed_dim)\n",
    "\n",
    "    # Rewrite the call function.\n",
    "    # Question 3: Determine the function and reason for this function?\n",
    "    def call(self, x):\n",
    "        maxlen = ops.shape(x)[-1]\n",
    "        positions = ops.arange(start=0, stop=maxlen, step=1)\n",
    "        positions = self.pos_emb(positions)\n",
    "        x = self.token_emb(x)\n",
    "        return x + positions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "kkwBgzvZgQou",
    "outputId": "e4318ccd-e419-47c3-e863-28189ae677c0"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "25000 Training sequences\n",
      "25000 Validation sequences\n"
     ]
    }
   ],
   "source": [
    "#########################\n",
    "## IMPORT THE DATA SET ##\n",
    "#########################\n",
    "# Here we will be looking at the IMDB dataset:\n",
    "# https://keras.io/api/datasets/imdb/\n",
    "# The x data are tokenized reviews of movies and the y data\n",
    "# are the sentiment expressed through the review (positive\n",
    "# or negative). Thus this is a classification problem.\n",
    "\n",
    "# Only consider the top 20k words\n",
    "vocab_size = 20000\n",
    "# Only consider the first 200 words of each movie review\n",
    "maxlen = 200\n",
    "\n",
    "# Import the data which is automatically split into two datasets,\n",
    "# here a training and validation data set.\n",
    "(x_train, y_train),(x_val, y_val) = keras.datasets.imdb.load_data(num_words=vocab_size)\n",
    "print(len(x_train), \"Training sequences\")\n",
    "print(len(x_val), \"Validation sequences\")\n",
    "\n",
    "# Trim the X data to only consider the first 200 words/tokens\n",
    "# in each review. This is optional but does decrease the time.\n",
    "x_train = keras.utils.pad_sequences(x_train, maxlen=maxlen)\n",
    "x_val = keras.utils.pad_sequences(x_val, maxlen=maxlen)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "2tUw9wlVgSxw"
   },
   "outputs": [],
   "source": [
    "# Define the hyperparameters of the model\n",
    "# Embedding size for each token\n",
    "embed_dim = 32\n",
    "# Number of attention heads\n",
    "num_heads = 2\n",
    "# Hidden layer size in feed forward network\n",
    "# inside transformer\n",
    "ff_dim = 32\n",
    "\n",
    "# Create the model, we start with an input layer\n",
    "inputs = layers.Input(shape=(maxlen,))\n",
    "# next we add the embedding layer\n",
    "embedding_layer = TokenAndPositionEmbedding(maxlen,\n",
    "                                            vocab_size,embed_dim)\n",
    "x = embedding_layer(inputs)\n",
    "# Now the transformer layer\n",
    "transformer_block = TransformerBlock(embed_dim,\n",
    "                                     num_heads, ff_dim)\n",
    "x = transformer_block(x)\n",
    "# And we finish with a pooling layer and a dense\n",
    "# layer before adding the output layer\n",
    "x = layers.GlobalAveragePooling1D()(x)\n",
    "x = layers.Dropout(0.1)(x)\n",
    "x = layers.Dense(20, activation=\"relu\")(x)\n",
    "x = layers.Dropout(0.1)(x)\n",
    "outputs = layers.Dense(2, activation=\"softmax\")(x)\n",
    "\n",
    "# Question 4: What would be the purpose of the pooling\n",
    "# and dense layers between defining the transformer and\n",
    "# creating the output layer?\n",
    "\n",
    "# Define the model\n",
    "model = keras.Model(inputs=inputs, outputs=outputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "47slKTHdgVcu",
    "outputId": "da637eea-310c-43ef-a1b1-69a8a378900f"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/2\n",
      "\u001b[1m782/782\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m26s\u001b[0m 32ms/step - accuracy: 0.7096 - loss: 0.5178 - val_accuracy: 0.8614 - val_loss: 0.3085\n",
      "Epoch 2/2\n",
      "\u001b[1m782/782\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m30s\u001b[0m 39ms/step - accuracy: 0.9293 - loss: 0.1874 - val_accuracy: 0.8728 - val_loss: 0.3116\n"
     ]
    }
   ],
   "source": [
    "# Compile and train the model as a classification problem\n",
    "model.compile(optimizer=\"adam\",\n",
    "              loss=\"sparse_categorical_crossentropy\",\n",
    "              metrics=[\"accuracy\"])\n",
    "history = model.fit(\n",
    "    x_train, y_train, batch_size=32, epochs=2,\n",
    "    validation_data=(x_val, y_val)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m782/782\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 12ms/step - accuracy: 0.8736 - loss: 0.3129\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[0.31158268451690674, 0.872759997844696]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.evaluate(x_val, y_val)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "LIH9j2fxg7ua"
   },
   "source": [
    "## Assignment:\n",
    "1. Last week we learned how to use recurrent neural networks for a similar natural language processing task. Do some research and compare and contrast recurrent neural networks and transformer models as they relate to natural language processing. Cite your sources if they are external to the course materials.\n",
    "2. For each of the above code cells, add a 1-2 sentence minimum comment explaining what the cell does, how it relates to the information learned this week, and how it relates to information learned prior in the course. Additionally, answer any questions asked in the existing comments either in the code cell or below. Label each answer below with the question number.\n",
    "3. Rewrite the data import cell so that there are three data sets: training, validation, and test. You can decide how large each of the sets are. Redo the above cell such that after the training the accuracy of the trained model is determined with the test set. Comment on the initial accuracy.\n",
    "4. Perform hyperparameter tuning in order to attempt to improve the accuracy of the model. In addition to the \"normal\" hyperparamters of loss function, activation function, and number of hidden layers, yoou can now also adjust embedding size and the number of heads for each token."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "4OAZLvGRgZMt"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
