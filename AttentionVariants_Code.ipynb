{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa2d5b42",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/butlerju/Library/Python/3.9/lib/python/site-packages/urllib3/__init__.py:35: NotOpenSSLWarning: urllib3 v2 only supports OpenSSL 1.1.1+, currently the 'ssl' module is compiled with 'LibreSSL 2.8.3'. See: https://github.com/urllib3/urllib3/issues/3020\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "#############\n",
    "## IMPORTS ##\n",
    "#############\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import requests, math"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "1b7e6b4b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#####################\n",
    "## IMPORT THE DATA ##\n",
    "######################\n",
    "\n",
    "# Instead of using tenorflow to import the data set, we will be pulling it\n",
    "# directly from Karpath's GitHub repository. He is the one who created the \n",
    "# data set and made it famous in his RNN blog post.\n",
    "url = \"https://raw.githubusercontent.com/karpathy/char-rnn/master/data/tinyshakespeare/input.txt\"\n",
    "text = requests.get(url).text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "63891fde",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total words: 202651\n",
      "Vocab size: 25670\n",
      "Sample tokens: ['First', 'Citizen:', 'Before', 'we', 'proceed', 'any', 'further,', 'hear', 'me', 'speak.', 'All:', 'Speak,', 'speak.', 'First', 'Citizen:', 'You', 'are', 'all', 'resolved', 'rather']\n",
      "Train words: 182385\n",
      "Val words: 20266\n"
     ]
    }
   ],
   "source": [
    "#################\n",
    "## TOKENIZAION ##\n",
    "#################\n",
    "# Here we are performing the tokenization. For simplicity, we will use a\n",
    "# word tokenization (i.e. each unique word is a token). This means that the\n",
    "# model will learn more quickly but will have a limited vocabulary. In practice,\n",
    "# this data set, and other natural language data sets, are often tokenized using\n",
    "# sub-word or character tokenization to allow for a larger vocabulary and better\n",
    "# handling of rare words.\n",
    "\n",
    "# Split the text pulled from the GitHub repo into words using the whitespace to\n",
    "# separate the words.\n",
    "words = text.split()\n",
    "\n",
    "# Generate a list of the unique words (vocabulary) and create mappings from\n",
    "# word to index and index to word.\n",
    "vocab = sorted(set(words))\n",
    "# Word to index mapping\n",
    "stoi = {w: i for i, w in enumerate(vocab)}    \n",
    "# Index to word mapping \n",
    "itos = {i: w for w, i in stoi.items()}    \n",
    "# Size of the vocabulary      \n",
    "vocab_size = len(vocab)\n",
    "\n",
    "# Print some statistics about the tokenization\n",
    "print(\"Total words:\", len(words))\n",
    "print(\"Vocab size:\", vocab_size)\n",
    "print(\"Sample tokens:\", words[:20])\n",
    "\n",
    "# Convert the entire text into a tensor of word indices (note that we need to\n",
    "# use tensor here to feed the data into a PyTorch model later).\n",
    "data = torch.tensor([stoi[w] for w in words], dtype=torch.long)\n",
    "\n",
    "# Train/validation split. Here we are using 90% of the data for training and 10%\n",
    "# for validation. This can be adjusted as needed.\n",
    "split = int(0.9 * len(data))\n",
    "train_data = data[:split]\n",
    "val_data   = data[split:]\n",
    "\n",
    "print(\"Train words:\", len(train_data))\n",
    "print(\"Val words:\", len(val_data))\n",
    "\n",
    "# Function to generate a batch of data for training/validation. The batch will\n",
    "# consist of input sequences of length block_size and the corresponding target\n",
    "# sequences (which are the input sequences shifted by one word). Block size and\n",
    "# batch size are hyperparameters that can be adjusted.\n",
    "def get_batch(split, block_size, batch_size, device):\n",
    "    \"\"\"\n",
    "    Inputs:    \n",
    "        split: \"train\" or \"val\" to indicate which data split to use\n",
    "        block_size: Length of each input sequence\n",
    "        batch_size: Number of sequences in the batch\n",
    "        device: Device to place the tensors on (e.g., \"cpu\" or \"cuda\")\n",
    "    Outputs:\n",
    "        x: Input tensor of shape (batch_size, block_size)\n",
    "        y: Target tensor of shape (batch_size, block_size)\n",
    "    Formats the train/validation data into batches.\n",
    "    \"\"\" \n",
    "    # select the appropriate data split\n",
    "    source = train_data if split == \"train\" else val_data\n",
    "    # pick random starting word positions\n",
    "    ids = torch.randint(0, len(source) - block_size - 1, (batch_size,))\n",
    "    # construct the input and target sequences\n",
    "    x = torch.stack([source[i:i+block_size] for i in ids]).to(device)\n",
    "    y = torch.stack([source[i+1:i+block_size+1] for i in ids]).to(device)\n",
    "    return x, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8cafb4b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "########################\n",
    "## ATTENTION VARIANTS ##\n",
    "########################\n",
    "\n",
    "#############################################\n",
    "## SCALED DOT-PRODUCT ATTENTION (STANDARD) ##\n",
    "##############################################\n",
    "class ScaledDotProductAttention(nn.Module):\n",
    "    \"\"\"\n",
    "        Standard scaled dot-product attention with causal masking. \n",
    "    \"\"\"\n",
    "    def __init__(self, n_embd, n_head):\n",
    "        \"\"\"\n",
    "        Inputs:\n",
    "            n_embd: Embedding dimension (length of input vectors)\n",
    "            n_head: Number of attention heads\n",
    "        Returns:\n",
    "            None\n",
    "        Initializes the components of the attention mechanism with one \n",
    "        or more heads\n",
    "        \"\"\"\n",
    "        # Initialize the nn.Module superclass\n",
    "        super().__init__()\n",
    "        # Define the number of heads and the dimension per head. The dimension\n",
    "        # per head is the embedding dimension divided by the number of heads and \n",
    "        # must be an integer.\n",
    "        assert n_embd % n_head == 0\n",
    "        self.n_head = n_head\n",
    "        self.d = n_embd // n_head\n",
    "        # Define the linear layers for query, key, and value projections\n",
    "        self.qkv = nn.Linear(n_embd, 3*n_embd, bias=False)\n",
    "        # Define the output linear layer\n",
    "        self.out = nn.Linear(n_embd, n_embd)\n",
    "\n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        Inputs:        \n",
    "            x: Input tensor of shape (B, T, C) where\n",
    "               B = batch size\n",
    "               T = sequence length (number of tokens)\n",
    "               C = embedding dimension\n",
    "        Returns:\n",
    "            Output tensor of shape (B, T, C) after applying attention\n",
    "        Performs the forward pass of the attention mechanism.\n",
    "        \"\"\"\n",
    "        # Get the batch size (B), sequence length (T), and embedding dimension (C)\n",
    "        B, T, C = x.shape\n",
    "        # Send the input through the query, key, and value linear layer and reshapes\n",
    "        # into multiple heads. Then permute the dimensions to get the shape\n",
    "        # (3, B, n_head, T, d) where d = C / n_head. Permute reorders the dimensions.\n",
    "        qkv = self.qkv(x).reshape(B, T, 3, self.n_head, self.d).permute(2,0,3,1,4)\n",
    "        # Split the qkv tensor into separate query, key, and value tensors\n",
    "        q, k, v = qkv[0], qkv[1], qkv[2]\n",
    "        # Compute the raw attention scores by performing the dot product between\n",
    "        # the query and key tensors, scaled by the square root of the dimension\n",
    "        # per head. The result has shape (B, n_head, T, T).\n",
    "        att = (q @ k.transpose(-2,-1)) / math.sqrt(self.d)\n",
    "        # Create a causal mask to ensure that each position can only attend to\n",
    "        # previous positions (including itself). The mask has shape (T, T). Causal\n",
    "        # masking is important for autoregressive models to prevent information\n",
    "        # leakage from future tokens.\n",
    "        mask = torch.tril(torch.ones(T, T, device=x.device)) == 1\n",
    "        # Apply the causal mask to the attention scores, setting masked positions\n",
    "        # to negative infinity. This ensures that after applying softmax, these\n",
    "        # positions will have zero attention weight.\n",
    "        att = att.masked_fill(~mask, float('-inf'))\n",
    "        # Apply the softmax function to the attention scores to obtain the\n",
    "        # attention weights. The softmax is applied along the last dimension\n",
    "        # (the sequence length dimension).\n",
    "        att = att.softmax(dim=-1)\n",
    "        # Compute the output by performing the weighted sum of the value tensor\n",
    "        # using the attention weights. The result has shape (B, n_head, T, d).\n",
    "        out = att @ v\n",
    "        # Reshape and permute the output tensor back to shape (B, T, C).\n",
    "        out = out.transpose(1,2).reshape(B,T,C)\n",
    "        # Send the output through the final linear layer and return.\n",
    "        return self.out(out)\n",
    "\n",
    "#####################\n",
    "## LOCAL ATTENTION ##\n",
    "#####################\n",
    "class LocalAttention(nn.Module):\n",
    "    \"\"\"\n",
    "    Simple causal *local* attention with φ(x)=ELU(x)+1 feature map.\n",
    "    Pedagogical sliding-window implementation (not the most optimized).\n",
    "    \"\"\"\n",
    "    def __init__(self, n_embd, n_head, window_size=64):\n",
    "        \"\"\"\n",
    "        Inputs:\n",
    "            n_embd: Embedding dimension (length of input vectors)\n",
    "            n_head: Number of attention heads\n",
    "            window_size: Size of the local attention window\n",
    "        Returns:\n",
    "            None\n",
    "        Initializes the components of the local attention mechanism.\n",
    "        \"\"\"\n",
    "        # Initialize the nn.Module superclass\n",
    "        super().__init__()\n",
    "        # Define the number of heads and the dimension per head. The dimension\n",
    "        # per head is the embedding dimension divided by the number of heads and \n",
    "        # must be an integer.\n",
    "        assert n_embd % n_head == 0\n",
    "        self.n_head = n_head\n",
    "        self.d = n_embd // n_head\n",
    "        # Define the local attention window size\n",
    "        self.window_size = int(window_size)\n",
    "        # Define the linear layers for query, key, and value projections\n",
    "        self.Wq = nn.Linear(n_embd, n_embd, bias=False)\n",
    "        self.Wk = nn.Linear(n_embd, n_embd, bias=False)\n",
    "        self.Wv = nn.Linear(n_embd, n_embd, bias=False)\n",
    "        # Define the output linear layer\n",
    "        self.out = nn.Linear(n_embd, n_embd)\n",
    "        # Define the scaling factor for stability. This is a common practice\n",
    "        # in attention mechanisms to prevent numerical instability.\n",
    "        self.scale = 1.0 / math.sqrt(self.d)  # standard scaling for stability\n",
    "\n",
    "    # Define the feature map phi(x) used in the attention mechanism. staticmethod\n",
    "    # decorator indicates that this method does not depend on the instance\n",
    "    # (self) and can be called on the class itself.\n",
    "    @staticmethod\n",
    "    def phi(x):\n",
    "        \"\"\"\n",
    "        Inputs:\n",
    "            x: Input tensor\n",
    "        Returns:\n",
    "            Transformed tensor after applying the feature map\n",
    "        Applies the feature map φ(x) = ELU(x) + 1.\"\"\"\n",
    "        return F.elu(x, alpha=1.0) + 1.0\n",
    "\n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        Inputs:\n",
    "            x: (B, T, C)\n",
    "        Returns:\n",
    "          y: (B, T, C)\n",
    "        Performs the forward pass of the local attention mechanism.\n",
    "        \"\"\"\n",
    "        # Get the batch size (B), sequence length (T), and embedding dimension (C)\n",
    "        B, T, C = x.shape\n",
    "        # Get the number of heads (H) and dimension per head (Dh)\n",
    "        H, Dh = self.n_head, self.d\n",
    "\n",
    "        # project and split into heads: (B,H,T,Dh)\n",
    "        q = self.Wq(x).reshape(B, T, H, Dh).transpose(1, 2)\n",
    "        k = self.Wk(x).reshape(B, T, H, Dh).transpose(1, 2)\n",
    "        v = self.Wv(x).reshape(B, T, H, Dh).transpose(1, 2)\n",
    "\n",
    "        # apply feature map\n",
    "        q_phi = self.phi(q)  # (B,H,T,Dh)\n",
    "        k_phi = self.phi(k)  # (B,H,T,Dh)\n",
    "\n",
    "        # slide over time steps\n",
    "        outs = []\n",
    "        for t in range(T):\n",
    "            # causal window: [max(0, t-window+1) .. t]\n",
    "            j0 = max(0, t - self.window_size + 1)\n",
    "            j1 = t + 1\n",
    "\n",
    "            # slice keys/values in window: (B,H,W,Dh)\n",
    "            k_win = k_phi[:, :, j0:j1, :]      # φ(K)\n",
    "            v_win = v[:,  :, j0:j1, :]      # V (no φ on values)\n",
    "            q_t   = q_phi[:, :, t, :]          # (B,H,Dh)\n",
    "\n",
    "            # similarities: (B,H,W) using φ(q)·φ(k)\n",
    "            # add scale factor to stabilize softmax (common with dot-product attention)\n",
    "            sim = torch.einsum(\"bhd,bhwd->bhw\", q_t, k_win) * self.scale\n",
    "\n",
    "            # softmax over window positions\n",
    "            a = F.softmax(sim, dim=-1)      # (B,H,W)\n",
    "\n",
    "            # weighted sum: (B,H,Dh)\n",
    "            y_t = torch.einsum(\"bhw,bhwd->bhd\", a, v_win)\n",
    "\n",
    "            outs.append(y_t)\n",
    "\n",
    "        # stack back to (B,T,C)\n",
    "        y = torch.stack(outs, dim=2).transpose(1, 2).reshape(B, T, C)\n",
    "        return self.out(y)\n",
    "\n",
    "#######################\n",
    "## LINEAR ATTENTION ##\n",
    "#######################\n",
    "class LinearAttention(nn.Module):\n",
    "    \"\"\"\n",
    "    Simple causal linear attention with φ(x)=ELU(x)+1 feature map.\n",
    "    Pedagogical prefix-scan implementation (not the most optimized).\n",
    "    \"\"\"\n",
    "    def __init__(self, n_embd, n_head):\n",
    "        \"\"\"\n",
    "        Inputs:\n",
    "            n_embd: Embedding dimension (length of input vectors)\n",
    "            n_head: Number of attention heads\n",
    "        Returns:\n",
    "            None\n",
    "        Initializes the components of the linear attention mechanism.\n",
    "        \"\"\"\n",
    "        # Initialize the nn.Module superclass\n",
    "        super().__init__()\n",
    "        # Define the number of heads and the dimension per head. The dimension\n",
    "        # per head is the embedding dimension divided by the number of heads and \n",
    "        # must be an integer.\n",
    "        assert n_embd % n_head == 0\n",
    "        self.n_head = n_head\n",
    "        self.d = n_embd // n_head\n",
    "        # Define the linear layers for query, key, and value projections\n",
    "        self.Wq = nn.Linear(n_embd, n_embd, bias=False)\n",
    "        self.Wk = nn.Linear(n_embd, n_embd, bias=False)\n",
    "        self.Wv = nn.Linear(n_embd, n_embd, bias=False)\n",
    "        # Define the output linear layer\n",
    "        self.out = nn.Linear(n_embd, n_embd)\n",
    "        # Small epsilon value for numerical stability in division\n",
    "        self.eps = 1e-6\n",
    "\n",
    "    # See the local attention class for explanation of staticmethod decorator\n",
    "    @staticmethod\n",
    "    def phi(x):\n",
    "        \"\"\"\n",
    "        Inputs:\n",
    "            x: Input tensor\n",
    "        Returns:\n",
    "            Transformed tensor after applying the feature map\n",
    "        Applies the feature map φ(x) = ELU(x) + 1.\"\"\"\n",
    "        return F.elu(x, alpha=1.0) + 1.0\n",
    "\n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        Inputs:\n",
    "            x: (B, T, C)\n",
    "        Returns:\n",
    "          y: (B, T, C)\n",
    "        Performs the forward pass of the linear attention mechanism.\n",
    "        \"\"\"\n",
    "        # Get the batch size (B), sequence length (T), and embedding dimension (C)\n",
    "        B, T, C = x.shape\n",
    "        # Get the number of heads (H) and dimension per head (Dh)\n",
    "        H, Dh = self.n_head, self.d\n",
    "\n",
    "        # project and split into heads: (B,H,T,Dh)\n",
    "        q = self.Wq(x).reshape(B, T, H, Dh).transpose(1, 2)   # (B,H,T,Dh)\n",
    "        k = self.Wk(x).reshape(B, T, H, Dh).transpose(1, 2)\n",
    "        v = self.Wv(x).reshape(B, T, H, Dh).transpose(1, 2)\n",
    "\n",
    "        # apply feature map\n",
    "        q_phi = self.phi(q)\n",
    "        k_phi = self.phi(k)\n",
    "\n",
    "        # prefix accumulators\n",
    "        K_accum  = torch.zeros(B, H, Dh, device=x.device, dtype=x.dtype)\n",
    "        KV_accum = torch.zeros(B, H, Dh, Dh, device=x.device, dtype=x.dtype)\n",
    "\n",
    "        # slide over time steps\n",
    "        outs = []\n",
    "        for t in range(T):\n",
    "            # keys/values at time t\n",
    "            kt = k_phi[:, :, t, :]                                    # (B,H,Dh)\n",
    "            vt = v[:, :, t, :]                                     # (B,H,Dh)\n",
    "            # accumulate prefix sums\n",
    "            KV_accum = KV_accum + torch.einsum(\"bhd,bhe->bhde\", kt, vt)\n",
    "            K_accum  = K_accum  + kt\n",
    "            # query\n",
    "            qt = q_phi[:, :, t, :]                                    # (B,H,Dh)\n",
    "            # compute output at time t\n",
    "            num = torch.einsum(\"bhd,bhde->bhe\", qt, KV_accum)      # (B,H,Dh)\n",
    "            # compute denominator\n",
    "            den = torch.einsum(\"bhd,bhd->bh\", qt, K_accum).unsqueeze(-1) + self.eps\n",
    "            # final output at time t\n",
    "            yt = num / den\n",
    "            outs.append(yt)\n",
    "        # stack back to (B,T,C)\n",
    "        y = torch.stack(outs, dim=2).transpose(1, 2).reshape(B, T, C)  # (B,T,C)\n",
    "        return self.out(y)\n",
    "\n",
    "# ============================\n",
    "# Multi-Query Attention (MQA)\n",
    "# ============================\n",
    "class MultiQueryAttention(nn.Module):\n",
    "    \"\"\"\n",
    "    Multi-Query Attention: many query heads, shared K and V across heads.\n",
    "    Cuts memory bandwidth for K/V compared with full MHA.\n",
    "    \"\"\"\n",
    "    def __init__(self, n_embd, n_head):\n",
    "        \"\"\"\n",
    "        Inputs:\n",
    "            n_embd: Embedding dimension (length of input vectors)\n",
    "            n_head: Number of attention heads\n",
    "        Returns:\n",
    "            None\n",
    "        Initializes the components of the multi-query attention mechanism.\n",
    "        \"\"\"\n",
    "        # Initialize the nn.Module superclass\n",
    "        super().__init__()\n",
    "        # Define the number of heads and the dimension per head. The dimension\n",
    "        # per head is the embedding dimension divided by the number of heads and \n",
    "        # must be an integer.\n",
    "        assert n_embd % n_head == 0\n",
    "        self.n_head = n_head\n",
    "        self.d = n_embd // n_head\n",
    "        # Define the linear layers for query, key, and value projections\n",
    "        self.Wq = nn.Linear(n_embd, n_embd, bias=False)  # (B,T,C) -> (B,T,H*Dh)\n",
    "        self.Wk = nn.Linear(n_embd, self.d, bias=False)  # shared K: (B,T,Dh)\n",
    "        self.Wv = nn.Linear(n_embd, self.d, bias=False)  # shared V: (B,T,Dh)\n",
    "        # Define the output linear layer\n",
    "        self.out = nn.Linear(n_embd, n_embd)\n",
    "\n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        Inputs:\n",
    "            x: (B, T, C)\n",
    "        Returns:\n",
    "          y: (B, T, C)\n",
    "        Performs the forward pass of the multi-query attention mechanism.\n",
    "        \"\"\"\n",
    "        # Get the batch size (B), sequence length (T), and embedding dimension (C)\n",
    "        B, T, C = x.shape\n",
    "        # Get the number of heads (H) and dimension per head (Dh)\n",
    "        H, Dh = self.n_head, self.d\n",
    "        # project and split into heads: (B,H,T,Dh)\n",
    "        q = self.Wq(x).reshape(B, T, H, Dh).transpose(1, 2)      # (B,H,T,Dh)\n",
    "\n",
    "        # shared K/V (expand over heads)\n",
    "        k = self.Wk(x).unsqueeze(1).expand(B, H, T, Dh).contiguous()\n",
    "        v = self.Wv(x).unsqueeze(1).expand(B, H, T, Dh).contiguous()\n",
    "\n",
    "        # PyTorch's fused scaled dot-product attention (Flash when available)\n",
    "        y = F.scaled_dot_product_attention(q, k, v, is_causal=True)  # (B,H,T,Dh)\n",
    "        y = y.transpose(1, 2).reshape(B, T, C)\n",
    "        return self.out(y)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "de7cc866",
   "metadata": {},
   "outputs": [],
   "source": [
    "##########################\n",
    "## LARGE LANGUAGE MODEL ##\n",
    "###########################\n",
    "\n",
    "##############################\n",
    "## SINGLE TRANSFORMER BLOCK ##\n",
    "###############################\n",
    "class TransformerBlock(nn.Module):\n",
    "    # Initialization method for the Transformer block.\n",
    "    def __init__(self, n_embd, n_head, attn_class):\n",
    "        \"\"\"\n",
    "        Inputs:        \n",
    "            n_embd: Length of the embedding vector\n",
    "            n_head: Number of attention heads\n",
    "            attn_class: Attention class to use (must be the name of one of the above classes)\n",
    "        Returns:\n",
    "            None.\n",
    "        Initializes a Transformer block with layer normalization, attention, and feed-forward network.\n",
    "        \"\"\" \n",
    "        # Initialize the nn.Module superclass\n",
    "        super().__init__()\n",
    "        # Layer normalization   \n",
    "        self.ln1 = nn.LayerNorm(n_embd)\n",
    "        # Attention mechanism\n",
    "        self.attn = attn_class(n_embd, n_head)\n",
    "        # Second layer normalization\n",
    "        self.ln2 = nn.LayerNorm(n_embd)\n",
    "        # Feed-forward network\n",
    "        self.ff = nn.Sequential(\n",
    "            nn.Linear(n_embd, 4*n_embd),\n",
    "            nn.GELU(),\n",
    "            nn.Linear(4*n_embd, n_embd)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        Inputs:\n",
    "            x: Input tensor of shape (B, T, C) where\n",
    "               B = batch size\n",
    "               T = sequence length (number of tokens)\n",
    "               C = embedding dimension\n",
    "        Returns:\n",
    "            Output tensor of shape (B, T, C) after applying the Transformer block\n",
    "        Performs the forward pass of the Transformer block.\n",
    "        \"\"\"\n",
    "        # Apply layer normalization, attention, and residual connection\n",
    "        x = x + self.attn(self.ln1(x))\n",
    "        # Apply layer normalization, feed-forward network, and residual connection\n",
    "        x = x + self.ff(self.ln2(x))\n",
    "        # Return the output tensor\n",
    "        return x\n",
    "\n",
    "class LLM(nn.Module):\n",
    "    \"\"\"\n",
    "    Creates an LLM model using multiple Transformer blocks. Allows for different attention mechanisms.\n",
    "    \"\"\"\n",
    "    def __init__(self, vocab_size, block_size, n_layer=4, n_embd=256, n_head=4, \n",
    "                 attn_class=ScaledDotProductAttention):\n",
    "        \"\"\"\n",
    "        Inputs:\n",
    "            vocab_size: Size of the vocabulary (number of unique tokens)\n",
    "            block_size: Length of each input sequence\n",
    "            n_layer: Number of Transformer blocks\n",
    "            n_embd: Length of the embedding vector\n",
    "            n_head: Number of attention heads\n",
    "            attn_class: Attention class to use (must be the name of one of the above classes)\n",
    "        Returns:\n",
    "            None\n",
    "        Initializes the LLM model with token and positional embeddings, multiple Transformer blocks,\n",
    "        layer normalization, and a final linear layer for output.\n",
    "        \"\"\"\n",
    "        # Initialize the nn.Module superclass\n",
    "        super().__init__()\n",
    "        # Store the block size (length of input sequences)\n",
    "        self.block_size = block_size\n",
    "        # Token embedding layer\n",
    "        self.token = nn.Embedding(vocab_size, n_embd)\n",
    "        # Positional embedding layer\n",
    "        self.pos   = nn.Embedding(block_size, n_embd)\n",
    "        # Stack of Transformer blocks, number of block is n_layer\n",
    "        self.blocks = nn.Sequential(*[TransformerBlock(n_embd, n_head, attn_class) for _ in range(n_layer)])\n",
    "        # Final layer normalization\n",
    "        self.ln_f = nn.LayerNorm(n_embd)\n",
    "        # Final linear layer \n",
    "        self.head = nn.Linear(n_embd, vocab_size)\n",
    "\n",
    "    def forward(self, x, targets=None):\n",
    "        \"\"\"\n",
    "        Inputs:\n",
    "            x: Input tensor of shape (B, T) where\n",
    "                 B = batch size\n",
    "                 T = sequence length (number of tokens)\n",
    "            targets: Optional target tensor of shape (B, T) for computing loss\n",
    "        Returns:\n",
    "            If targets is None, returns logits tensor of shape (B, T, vocab_size).\n",
    "            If targets is provided, returns a tuple (logits, loss) where loss is the\n",
    "            cross-entropy loss between logits and targets.\n",
    "        Performs the forward pass of the LLM model.\n",
    "        \"\"\"\n",
    "        # Get the batch size (B) and sequence length (T)\n",
    "        B, T = x.shape\n",
    "        # Create position indices for the sequence length\n",
    "        pos = torch.arange(0, T, device=x.device)\n",
    "        # Combine token and positional embeddings\n",
    "        x = self.token(x) + self.pos(pos)[None,:,:]\n",
    "        # Pass through the stack of Transformer blocks\n",
    "        x = self.blocks(x)\n",
    "        # Apply final layer normalization\n",
    "        x = self.ln_f(x)\n",
    "        # Compute logits using the final linear layer\n",
    "        logits = self.head(x)\n",
    "        # If targets are not provided, return logits\n",
    "        if targets is None:\n",
    "            return logits\n",
    "        # If targets are provided, compute cross-entropy loss and return logits and loss\n",
    "        loss = F.cross_entropy(logits.view(-1, vocab_size), targets.view(-1))\n",
    "        return logits, loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "e11f7310",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training...\n",
      "step 0 | loss 10.318\n",
      "step 100 | loss 6.917\n",
      "step 200 | loss 5.677\n",
      "step 300 | loss 4.692\n",
      "step 400 | loss 3.690\n",
      "step 500 | loss 2.796\n",
      "step 600 | loss 1.860\n",
      "step 700 | loss 1.211\n",
      "step 800 | loss 0.778\n",
      "step 900 | loss 0.510\n"
     ]
    }
   ],
   "source": [
    "#$########################\n",
    "## TRAIN THE LLM MODEL ##\n",
    "#########################\n",
    "\n",
    "# Check to see if a GPU is available and set the device accordingly. This only works\n",
    "# if you have a compatible GPU and the necessary CUDA libraries installed. The GPU will\n",
    "# significantly speed up the training process.\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "# Define block size and batch size for training. The block sie is the length of\n",
    "# each input sequence and the batch size is the number of sequences in each batch. The\n",
    "# values can be adjusted as needed.\n",
    "block_size = 256\n",
    "batch_size = 64\n",
    "\n",
    "# Choose the attention mechanism to use in the model. You can switch between\n",
    "# different attention classes defined above. Use the name exactly as defined.\n",
    "attn_type = ScaledDotProductAttention   \n",
    "\n",
    "# Define the model using the specified attention mechanism and move it to the\n",
    "# selected device (CPU or GPU).\n",
    "model = LLM(vocab_size, block_size, attn_class=attn_type).to(device)\n",
    "\n",
    "# Define the optimizer for training. Here we are using the AdamW optimizer,\n",
    "# which is a variant of the Adam optimizer that includes weight decay for\n",
    "# regularization. The optimizer will update the model parameters during training.\n",
    "optimizer = torch.optim.AdamW(model.parameters())\n",
    "\n",
    "# Training loop. Steps is the number of training iterations. In each iteration,\n",
    "# we get a batch of data, perform a forward pass through the model, compute the\n",
    "# loss, perform backpropagation, and update the model parameters using the optimizer.\n",
    "steps = 1000\n",
    "print(\"Training...\")\n",
    "\n",
    "for step in range(steps):\n",
    "    # Get a batch of training data\n",
    "    xb, yb = get_batch(\"train\", block_size, batch_size, device)\n",
    "    # Perform a forward pass through the model to get logits and loss\n",
    "    logits, loss = model(xb, yb)\n",
    "    # Backpropagation and optimization step\n",
    "    optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    # Print the loss every 100 steps\n",
    "    if step % 100 == 0:\n",
    "        print(f\"step {step} | loss {loss.item():.3f}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "d74f907a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "To be, or not to be, The which is ta'en! First Gentleman: It is a King Edward. She would the benefit of all the king a himself, this is to be done, and that her to be true, that the lord of Paulina,--a piece of Paulina,--a piece of Paulina,--a piece of Paulina,--a piece the court? for\n"
     ]
    }
   ],
   "source": [
    "############################\n",
    "## TEST THE TRAINED MODEL ##\n",
    "############################\n",
    "\n",
    "def sample(model, start=\"ROMEO:\", steps=100):\n",
    "    \"\"\"\n",
    "    Inputs:    \n",
    "        model: Trained LLM model\n",
    "        start: Starting prompt string\n",
    "        steps: Number of words to generate\n",
    "    Returns:\n",
    "        Generated text string after sampling from the model\n",
    "    Generates text by sampling from the trained LLM model starting from the\n",
    "    given prompt.\n",
    "    \"\"\"\n",
    "    # Set the model to evaluation mode (testing mode)\n",
    "    model.eval()\n",
    "\n",
    "    # Tokenize start prompt into words\n",
    "    start_words = start.split()\n",
    "    start_ids = [stoi.get(w, 0) for w in start_words]\n",
    "\n",
    "    # Build initial sequence tensor\n",
    "    idx = torch.tensor([start_ids], device=device)\n",
    "\n",
    "    # Generate words one at a time\n",
    "    for i in range(steps):\n",
    "        # Crop context to block_size so position embeddings never overflow\n",
    "        idx_cond = idx[:, -model.block_size:]\n",
    "        # Get the model's logits for the current sequence\n",
    "        with torch.no_grad():\n",
    "            logits = model(idx_cond)[:, -1, :]  # final word's logits\n",
    "        # Greedy decoding: pick highest‑probability word. This can be changed as desired.\n",
    "        next_id = torch.argmax(logits, dim=-1, keepdim=True)\n",
    "        # Append to sequence\n",
    "        idx = torch.cat([idx, next_id], dim=1)\n",
    "\n",
    "    # Convert token IDs back to words, and join into a single string, and return in\n",
    "    result_words = [itos[int(i)] for i in idx[0]]\n",
    "    return \" \".join(result_words)\n",
    "\n",
    "\n",
    "# Generate and print sample text from the trained model\n",
    "print(sample(model, \"To be, or not to be,\", steps=50))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d763297c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
