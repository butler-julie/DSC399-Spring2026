{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c512abc8",
   "metadata": {},
   "source": [
    "# Homework 1 Week 1\n",
    "\n",
    "## Part 1: Introduction to PyTorch\n",
    "\n",
    "### Concept Questions\n",
    "\n",
    "Answer the following questions in complete sentences, providing as much detail as neccessary to thoroughly answer.\n",
    "\n",
    "* What is the PyTorch equivalent of:\n",
    "\n",
    "    * `tf.keras.Model`\n",
    "\n",
    "    * `model.compile()`\n",
    "\n",
    "    * `model.fit()`\n",
    "\n",
    "* In Keras, where is the forward pass defined? In PyTorch, where is it defined?\n",
    "\n",
    "* Why does PyTorch require calling `loss. backward()` explicitly?\n",
    "\n",
    "* What does `optimizer.step()` do, conceptually?\n",
    "\n",
    "### Create Neural Networks in Python\n",
    "For this assignment you are to create a convolutional neural network to classify the Fashion MNIST data set.\n",
    "* Import the data set from keras, similar to how the MNSIT data set is imported in lecture notes.\n",
    "* Plan a CNN that could be used to classify the data. You can use the one from the lecture notes, your solution from DSC 340, or create one from scratch. Note that for this assignment accuracy does not matter. Implement this neural network using both formats shown in the lecture notes (with and without `nn.Sequential`).\n",
    "* Make the following changes to one of your base neural network. Make sure that all changes are shown, either in one network or across several networks:\n",
    "    * Add at least one more layer, of any type.\n",
    "    * Change the activation function of at least one layer.\n",
    "    * Change the learning rate of the optimizer.\n",
    "    * Add dropout between any layers of the network.\n",
    "    * Add a batch size to your training loop. In the examples shown in the lecture notes, all of the data is passed at once.\n",
    "    * Add early stopping to the training process."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e817173",
   "metadata": {},
   "source": [
    "## Part 2: Natural Language Processing with Transformers and Large Language Models\n",
    "\n",
    "In this assignment we will attempt to create a (very small) large language model (LLM) which can generate text similar to Shakespeare. The data set we will be using today comes from Andrej Karpathy and is used in his blog post [The Unreasonable Effectiveness of Recurrent Neural Networks](https://karpathy.github.io/2015/05/21/rnn-effectiveness/). Note that this blog post is included in Ilya Sutskever's Top 30 Machine Learning Papers, a list of papers covering 90% of the current field of machine learning and a good primer for advanced studies in modern machine learning and artificial intelligence. Summaries of the papers can be found [here](https://aman.ai/primers/ai/top-30-papers/), though I do suggest that you read them yourself if you plan on pursuing this field. Andrej Karpathy also has an excellent [YouTube channel](https://www.youtube.com/@AndrejKarpathy) that includes some very good (and very long) videos on making rather complex LLMs from scratch.\n",
    "\n",
    "Note that while this assignment is a repeat of DSC 340's last homework assignment, I do suggest that you work through it again to remind yourself of how it works."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "cb7d249e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/butlerju/Library/Python/3.9/lib/python/site-packages/urllib3/__init__.py:35: NotOpenSSLWarning: urllib3 v2 only supports OpenSSL 1.1.1+, currently the 'ssl' module is compiled with 'LibreSSL 2.8.3'. See: https://github.com/urllib3/urllib3/issues/3020\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "#############\n",
    "## IMPORTS ##\n",
    "#############\n",
    "import tensorflow as tf\n",
    "import tensorflow_datasets as tfds\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "799128d0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tfds.core.DatasetInfo(\n",
      "    name='tiny_shakespeare',\n",
      "    full_name='tiny_shakespeare/1.0.0',\n",
      "    description=\"\"\"\n",
      "    40,000 lines of Shakespeare from a variety of Shakespeare's plays. Featured in\n",
      "    Andrej Karpathy's blog post 'The Unreasonable Effectiveness of Recurrent Neural\n",
      "    Networks': http://karpathy.github.io/2015/05/21/rnn-effectiveness/.\n",
      "    \n",
      "    To use for e.g. character modelling:\n",
      "    \n",
      "    ```\n",
      "    d = tfds.load(name='tiny_shakespeare')['train']\n",
      "    d = d.map(lambda x: tf.strings.unicode_split(x['text'], 'UTF-8'))\n",
      "    # train split includes vocabulary for other splits\n",
      "    vocabulary = sorted(set(next(iter(d)).numpy()))\n",
      "    d = d.map(lambda x: {'cur_char': x[:-1], 'next_char': x[1:]})\n",
      "    d = d.unbatch()\n",
      "    seq_len = 100\n",
      "    batch_size = 2\n",
      "    d = d.batch(seq_len)\n",
      "    d = d.batch(batch_size)\n",
      "    ```\n",
      "    \"\"\",\n",
      "    homepage='https://github.com/karpathy/char-rnn/blob/master/data/tinyshakespeare/input.txt',\n",
      "    data_dir='/Users/butlerju/tensorflow_datasets/tiny_shakespeare/1.0.0',\n",
      "    file_format=tfrecord,\n",
      "    download_size=1.06 MiB,\n",
      "    dataset_size=1.06 MiB,\n",
      "    features=FeaturesDict({\n",
      "        'text': Text(shape=(), dtype=string),\n",
      "    }),\n",
      "    supervised_keys=None,\n",
      "    disable_shuffling=False,\n",
      "    splits={\n",
      "        'test': <SplitInfo num_examples=1, num_shards=1>,\n",
      "        'train': <SplitInfo num_examples=1, num_shards=1>,\n",
      "        'validation': <SplitInfo num_examples=1, num_shards=1>,\n",
      "    },\n",
      "    citation=\"\"\"@misc{\n",
      "      author={Karpathy, Andrej},\n",
      "      title={char-rnn},\n",
      "      year={2015},\n",
      "      howpublished={\\url{https://github.com/karpathy/char-rnn}}\n",
      "    }\"\"\",\n",
      ")\n",
      "\n",
      "First Citizen:\n",
      "Before we proceed any further, hear me speak.\n",
      "\n",
      "All:\n",
      "Speak, speak.\n",
      "\n",
      "First Citizen:\n",
      "You\n",
      "\n",
      "1003854\n"
     ]
    }
   ],
   "source": [
    "#####################\n",
    "## IMPORT THE DATA ##\n",
    "#####################\n",
    "ds, info = tfds.load(\"tiny_shakespeare\", split=\"train\", with_info=True)\n",
    "text = ds.take(1).as_numpy_iterator().__next__()[\"text\"].decode()\n",
    "\n",
    "print(info)\n",
    "print()\n",
    "\n",
    "# Print a sample and the length of the data set.\n",
    "print(text[0:100])\n",
    "print()\n",
    "print(len(text))\n",
    "\n",
    "# Truncate the data set to make for faster training. You can change\n",
    "# or remove this it get better performance.\n",
    "truncate = 50000\n",
    "text = text[:truncate]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c099ca2c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['', '[UNK]', 'the', 'and', 'to', 'you', 'i', 'of', 'a', 'in', 'that', 'he', 'marcius', 'not', 'for', 'your', 'him', 'with', 'my', 'it', 'is', 'have', 'as', 'they', 'we', 'be', 'his', 'are', 'their', 'our', 'first', 'but', 'menenius', 'me', 'all', 'what', 'good', 'shall', 'this', 'will', 'than', 'if', 'o', 'no', 'well', 'cominius', 'at', 'us', 'them', 'so']\n"
     ]
    }
   ],
   "source": [
    "##################\n",
    "## TOKENIZATION ##\n",
    "#################\n",
    "seq_length = 128\n",
    "\n",
    "tokenizer = tf.keras.layers.TextVectorization(\n",
    "    max_tokens=8000,\n",
    "    output_sequence_length=None,  # <-- we want full-length tokens\n",
    "    standardize=\"lower_and_strip_punctuation\",\n",
    ")\n",
    "tokenizer.adapt([text])\n",
    "\n",
    "vocab = tokenizer.get_vocabulary()\n",
    "vocab_size = len(vocab)\n",
    "\n",
    "full_tokens = tokenizer(tf.constant([text]))\n",
    "full_tokens = tf.squeeze(full_tokens, axis=0)  \n",
    "full_tokens = tf.cast(full_tokens, tf.int32)\n",
    "\n",
    "# Print a sample of the vocab list\n",
    "print(vocab[:50])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "1c1c8942",
   "metadata": {},
   "outputs": [],
   "source": [
    "#########################\n",
    "## FORMAT THE DATA SET ##\n",
    "#########################\n",
    "def make_dataset(tokens, seq_len):\n",
    "    # Manually doing time series formatting, could change this to use\n",
    "    # TimeSeriesGenerator.\n",
    "    N = len(tokens)\n",
    "    X = []\n",
    "    Y = []\n",
    "    for i in range(N - seq_len - 1):\n",
    "        X.append(tokens[i : i + seq_len])\n",
    "        Y.append(tokens[i + 1 : i + seq_len + 1])\n",
    "    X = tf.stack(X)\n",
    "    Y = tf.stack(Y)\n",
    "    return tf.data.Dataset.from_tensor_slices((X, Y))\n",
    "\n",
    "dataset = make_dataset(full_tokens, seq_length)\n",
    "dataset = dataset.shuffle(2000).batch(32).prefetch(tf.data.AUTOTUNE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "9dd1c2bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "##########################\n",
    "## TRANSFORMER FUNCTION ##\n",
    "#########################\n",
    "def transformer(x, embed_dim, num_heads, ff_dim):\n",
    "    # Attention Network\n",
    "    attn_out = tf.keras.layers.MultiHeadAttention(\n",
    "        num_heads=num_heads,\n",
    "        key_dim=embed_dim // num_heads\n",
    "    )(x, x)\n",
    "    x = tf.keras.layers.LayerNormalization()(x + attn_out)\n",
    "\n",
    "    # Feedforward Neural Network\n",
    "    ff = tf.keras.Sequential([\n",
    "        tf.keras.layers.Dense(ff_dim, activation=\"relu\"),\n",
    "        tf.keras.layers.Dense(embed_dim),\n",
    "    ])\n",
    "    x = tf.keras.layers.LayerNormalization()(x + ff(x))\n",
    "    return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "6e025b06",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"functional_2\"</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1mModel: \"functional_2\"\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━┓\n",
       "┃<span style=\"font-weight: bold\"> Layer (type)        </span>┃<span style=\"font-weight: bold\"> Output Shape      </span>┃<span style=\"font-weight: bold\">    Param # </span>┃<span style=\"font-weight: bold\"> Connected to      </span>┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━┩\n",
       "│ input_layer         │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)       │          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ -                 │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">InputLayer</span>)        │                   │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ embedding           │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">36</span>)   │     <span style=\"color: #00af00; text-decoration-color: #00af00\">74,520</span> │ input_layer[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>] │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Embedding</span>)         │                   │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ multi_head_attenti… │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">36</span>)   │      <span style=\"color: #00af00; text-decoration-color: #00af00\">5,328</span> │ embedding[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>],  │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">MultiHeadAttentio…</span> │                   │            │ embedding[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ add (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Add</span>)           │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">36</span>)   │          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ embedding[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>],  │\n",
       "│                     │                   │            │ multi_head_atten… │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ layer_normalization │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">36</span>)   │         <span style=\"color: #00af00; text-decoration-color: #00af00\">72</span> │ add[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]         │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">LayerNormalizatio…</span> │                   │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ sequential          │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">36</span>)   │      <span style=\"color: #00af00; text-decoration-color: #00af00\">4,708</span> │ layer_normalizat… │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Sequential</span>)        │                   │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ add_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Add</span>)         │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">36</span>)   │          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ layer_normalizat… │\n",
       "│                     │                   │            │ sequential[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]  │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ layer_normalizatio… │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">36</span>)   │         <span style=\"color: #00af00; text-decoration-color: #00af00\">72</span> │ add_1[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]       │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">LayerNormalizatio…</span> │                   │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ multi_head_attenti… │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">36</span>)   │      <span style=\"color: #00af00; text-decoration-color: #00af00\">5,328</span> │ layer_normalizat… │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">MultiHeadAttentio…</span> │                   │            │ layer_normalizat… │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ add_2 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Add</span>)         │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">36</span>)   │          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ layer_normalizat… │\n",
       "│                     │                   │            │ multi_head_atten… │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ layer_normalizatio… │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">36</span>)   │         <span style=\"color: #00af00; text-decoration-color: #00af00\">72</span> │ add_2[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]       │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">LayerNormalizatio…</span> │                   │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ sequential_1        │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">36</span>)   │      <span style=\"color: #00af00; text-decoration-color: #00af00\">4,708</span> │ layer_normalizat… │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Sequential</span>)        │                   │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ add_3 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Add</span>)         │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">36</span>)   │          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ layer_normalizat… │\n",
       "│                     │                   │            │ sequential_1[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">…</span> │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ layer_normalizatio… │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">36</span>)   │         <span style=\"color: #00af00; text-decoration-color: #00af00\">72</span> │ add_3[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]       │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">LayerNormalizatio…</span> │                   │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ dense_4 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)     │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">2070</span>) │     <span style=\"color: #00af00; text-decoration-color: #00af00\">76,590</span> │ layer_normalizat… │\n",
       "└─────────────────────┴───────────────────┴────────────┴───────────────────┘\n",
       "</pre>\n"
      ],
      "text/plain": [
       "┏━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━┓\n",
       "┃\u001b[1m \u001b[0m\u001b[1mLayer (type)       \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape     \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m   Param #\u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mConnected to     \u001b[0m\u001b[1m \u001b[0m┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━┩\n",
       "│ input_layer         │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m128\u001b[0m)       │          \u001b[38;5;34m0\u001b[0m │ -                 │\n",
       "│ (\u001b[38;5;33mInputLayer\u001b[0m)        │                   │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ embedding           │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m128\u001b[0m, \u001b[38;5;34m36\u001b[0m)   │     \u001b[38;5;34m74,520\u001b[0m │ input_layer[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m] │\n",
       "│ (\u001b[38;5;33mEmbedding\u001b[0m)         │                   │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ multi_head_attenti… │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m128\u001b[0m, \u001b[38;5;34m36\u001b[0m)   │      \u001b[38;5;34m5,328\u001b[0m │ embedding[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m],  │\n",
       "│ (\u001b[38;5;33mMultiHeadAttentio…\u001b[0m │                   │            │ embedding[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ add (\u001b[38;5;33mAdd\u001b[0m)           │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m128\u001b[0m, \u001b[38;5;34m36\u001b[0m)   │          \u001b[38;5;34m0\u001b[0m │ embedding[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m],  │\n",
       "│                     │                   │            │ multi_head_atten… │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ layer_normalization │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m128\u001b[0m, \u001b[38;5;34m36\u001b[0m)   │         \u001b[38;5;34m72\u001b[0m │ add[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]         │\n",
       "│ (\u001b[38;5;33mLayerNormalizatio…\u001b[0m │                   │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ sequential          │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m128\u001b[0m, \u001b[38;5;34m36\u001b[0m)   │      \u001b[38;5;34m4,708\u001b[0m │ layer_normalizat… │\n",
       "│ (\u001b[38;5;33mSequential\u001b[0m)        │                   │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ add_1 (\u001b[38;5;33mAdd\u001b[0m)         │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m128\u001b[0m, \u001b[38;5;34m36\u001b[0m)   │          \u001b[38;5;34m0\u001b[0m │ layer_normalizat… │\n",
       "│                     │                   │            │ sequential[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]  │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ layer_normalizatio… │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m128\u001b[0m, \u001b[38;5;34m36\u001b[0m)   │         \u001b[38;5;34m72\u001b[0m │ add_1[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]       │\n",
       "│ (\u001b[38;5;33mLayerNormalizatio…\u001b[0m │                   │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ multi_head_attenti… │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m128\u001b[0m, \u001b[38;5;34m36\u001b[0m)   │      \u001b[38;5;34m5,328\u001b[0m │ layer_normalizat… │\n",
       "│ (\u001b[38;5;33mMultiHeadAttentio…\u001b[0m │                   │            │ layer_normalizat… │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ add_2 (\u001b[38;5;33mAdd\u001b[0m)         │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m128\u001b[0m, \u001b[38;5;34m36\u001b[0m)   │          \u001b[38;5;34m0\u001b[0m │ layer_normalizat… │\n",
       "│                     │                   │            │ multi_head_atten… │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ layer_normalizatio… │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m128\u001b[0m, \u001b[38;5;34m36\u001b[0m)   │         \u001b[38;5;34m72\u001b[0m │ add_2[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]       │\n",
       "│ (\u001b[38;5;33mLayerNormalizatio…\u001b[0m │                   │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ sequential_1        │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m128\u001b[0m, \u001b[38;5;34m36\u001b[0m)   │      \u001b[38;5;34m4,708\u001b[0m │ layer_normalizat… │\n",
       "│ (\u001b[38;5;33mSequential\u001b[0m)        │                   │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ add_3 (\u001b[38;5;33mAdd\u001b[0m)         │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m128\u001b[0m, \u001b[38;5;34m36\u001b[0m)   │          \u001b[38;5;34m0\u001b[0m │ layer_normalizat… │\n",
       "│                     │                   │            │ sequential_1[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m…\u001b[0m │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ layer_normalizatio… │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m128\u001b[0m, \u001b[38;5;34m36\u001b[0m)   │         \u001b[38;5;34m72\u001b[0m │ add_3[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]       │\n",
       "│ (\u001b[38;5;33mLayerNormalizatio…\u001b[0m │                   │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ dense_4 (\u001b[38;5;33mDense\u001b[0m)     │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m128\u001b[0m, \u001b[38;5;34m2070\u001b[0m) │     \u001b[38;5;34m76,590\u001b[0m │ layer_normalizat… │\n",
       "└─────────────────────┴───────────────────┴────────────┴───────────────────┘\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">171,470</span> (669.80 KB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Total params: \u001b[0m\u001b[38;5;34m171,470\u001b[0m (669.80 KB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">171,470</span> (669.80 KB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m171,470\u001b[0m (669.80 KB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "###########################################################\n",
    "## BUILD LARGE LANGUAGE MODEL (I.E. LINKED TRANSFORMERS) ##\n",
    "###########################################################\n",
    "# Parameters\n",
    "embed_dim = 36\n",
    "num_heads = 2\n",
    "ff_dim = 64\n",
    "\n",
    "inputs = tf.keras.Input(shape=(seq_length,), dtype=tf.int32)\n",
    "\n",
    "x = tf.keras.layers.Embedding(vocab_size, embed_dim)(inputs)\n",
    "\n",
    "x = transformer(x, embed_dim, num_heads, ff_dim)\n",
    "\n",
    "x = transformer(x, embed_dim, num_heads, ff_dim)\n",
    "\n",
    "outputs = tf.keras.layers.Dense(vocab_size, activation=\"softmax\")(x)\n",
    "\n",
    "model = tf.keras.Model(inputs, outputs)\n",
    "model.compile(optimizer=\"adam\", loss=\"sparse_categorical_crossentropy\")\n",
    "\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "79ce45f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "#######################\n",
    "## GENERATE NEW TEXT ##\n",
    "#######################\n",
    "def generate(model, start_text, max_tokens=50):\n",
    "    text_so_far = start_text\n",
    "\n",
    "    for _ in range(max_tokens):\n",
    "        # Tokenize the current text\n",
    "        token_ids = tokenizer([text_so_far]) \n",
    "        token_ids = tf.squeeze(token_ids, axis=0).numpy().tolist() \n",
    "        # Keep only last 128 tokens\n",
    "        token_ids = token_ids[-seq_length:]\n",
    "        # LEFT-PAD with zeros if shorter than seq_length\n",
    "        # Can replace this with pad_sequences if you want.\n",
    "        if len(token_ids) < seq_length:\n",
    "            pad_len = seq_length - len(token_ids)\n",
    "            token_ids = [0] * pad_len + token_ids\n",
    "        token_tensor = tf.constant([token_ids], dtype=tf.int32)\n",
    "        # Predict next token\n",
    "        preds = model.predict(token_tensor, verbose=0)\n",
    "        next_id = int(tf.argmax(preds[0, -1]).numpy())\n",
    "        # Convert id → word\n",
    "        next_word = vocab[next_id]\n",
    "        text_so_far += \" \" + next_word\n",
    "\n",
    "    return text_so_far\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "caa9d8be",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/3\n",
      "\u001b[1m276/276\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 35ms/step - loss: 6.0229\n",
      "Epoch 2/3\n",
      "\u001b[1m276/276\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 36ms/step - loss: 3.3670\n",
      "Epoch 3/3\n",
      "\u001b[1m276/276\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 37ms/step - loss: 1.9581\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.src.callbacks.history.History at 0x10582c760>"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "###########\n",
    "## TRAIN ##\n",
    "###########\n",
    "model.fit(dataset, epochs=3)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "5bbe312e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "To be, or not to be or of his nor sleep sicinius sicinius sicinius sicinius sicinius sicinius sicinius sicinius sicinius sicinius sicinius sicinius sicinius sicinius sicinius sicinius sicinius sicinius sicinius sicinius sicinius sicinius sicinius sicinius sicinius sicinius sicinius sicinius sicinius sicinius sicinius sicinius sicinius sicinius sicinius sicinius sicinius sicinius sicinius sicinius sicinius sicinius sicinius sicinius sicinius\n"
     ]
    }
   ],
   "source": [
    "#############\n",
    "## EXAMPLE ##\n",
    "#############\n",
    "print(generate(model, \"To be, or not to be\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9725248e",
   "metadata": {},
   "source": [
    "## Assignment\n",
    "\n",
    "### Part A: Understanding the Code\n",
    "1. Go through the code and provide comments as to what is happening.\n",
    "\n",
    "Answer the following questions thoroughly, including adding citations to any information learned outside of course materials.\n",
    "\n",
    "2. What does the tokenizer do? What is its vocabulary?\n",
    "3. How does the code create input/target sequences for a language model?\n",
    "4. Describe the self-attention mechanism in the model. Why does it take (x, x) as arguments?\n",
    "5. Explain the purpose of:\n",
    "    * Embedding layer\n",
    "    * Multi-head attention\n",
    "    * Residual connections\n",
    "    * LayerNormalization\n",
    "    * Feedforward block\n",
    "5. Why does the model use SparseCategoricalCrossentropy?\n",
    "6. How does the generation function work? What is greedy decoding?\n",
    "\n",
    "### Part B: Modify the Code\n",
    "1. Attempt to make the model better by adjusting the following parameters:`embed_dim`, `num_heads`, `ff_dim`, `output_sequence_length`, `seq_length`, and `epochs`. You can also change the truncation of the training data, activation functions, and any other parameters you like. Comment on how changing these parameters changes the model's performance. Note, I would advise against drastically increasing the parameters without testing first as the run times can easily sky rocket.\n",
    "2. The above LLM only has two transformer blocks. Increase this to three and comment on the change in performance.\n",
    "3. Increasing the above parameters and architecture still results in a simple LLM. Make two or more of the above changes to the model which will make it more complex, and hopefully better able to generate text. Explain what each change does to the model in a comment or markdown cell.\n",
    "    * Change the tokenization strategy from the word level to the sub-word or character level. Note the tiny-shakespeare dataset is usually tokenized at the character level.\n",
    "    * Add positional embeddings (see last week's assignment).\n",
    "    * Change the attention to casual (masked).\n",
    "    * Add an adaptive learning rate or a learning rate scheduler.\n",
    "    * Add dropout.\n",
    "    * Add temperature, top-k, or top-p sampling.\n",
    "    * Change the decoding strategy (related to the sampling above).\n",
    "    \n",
    "### Part C: Text Generation\n",
    "What combination of changes made in Part 2 gave you the best output (i.e. that sounds like it could be Shakespeare)? What was your favorite output?\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a78ba3f9",
   "metadata": {},
   "source": [
    "## Part 3: Attention Variants\n",
    "\n",
    "Take the final LLM created in Part 2 of this assignment and try one or more different attention variants. Discuss how this architecture differs from your previous version and how its performance differs."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1acf46c7",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
