{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b828d422",
   "metadata": {},
   "source": [
    "# Introduction to PyTorch\n",
    "### DSC 399: Advanced Applications and Interpretability of Neural Networks \n",
    "\n",
    "## Imports and Data Set Up"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "84b303e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "#############\n",
    "## IMPORTS ##\n",
    "#############\n",
    "\n",
    "# For Data Set Loading and Preprocessing\n",
    "# Data set for the feedforward neural networks\n",
    "from sklearn.datasets import load_breast_cancer\n",
    "# Train test split function from sklearn\n",
    "from sklearn.model_selection import train_test_split\n",
    "# Standard scaler to normalize from sklearn\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "# Data sets for CNNs and RNNs from Tensorflow\n",
    "from tensorflow.keras.datasets import mnist, imdb\n",
    "# Pad sequences from Keras for NLP data formatting\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "\n",
    "# Tensorflow/Keras Imports\n",
    "# Import base tensorflow\n",
    "import tensorflow as tf\n",
    "# Import the sequential model to create neural networks\n",
    "from tensorflow.keras.models import Sequential\n",
    "# Import different layer types needed from Keras\n",
    "from tensorflow.keras.layers import Dense, Conv2D, MaxPooling2D, Flatten, LSTM, Embedding\n",
    "\n",
    "# PyTorch Imports\n",
    "# Import base PyTorch\n",
    "import torch\n",
    "# Import the neural network capabilities\n",
    "import torch.nn as nn\n",
    "# Import the optimizers\n",
    "import torch.optim as optim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73df7d3e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#################################\n",
    "## IMPORT AND FORMAT DATA SETS ##\n",
    "#################################\n",
    "# This data set, which predicts if a person does or does not have breast cancer,\n",
    "# is from sklearn. We will import it, perform a train test split, and format the \n",
    "# data set to be used in a neural network before returning the X and y components\n",
    "# of the training and test data set.\n",
    "def load_breast_cancer_data(test_size=0.2):\n",
    "    # Load the breast cancer dataset from sklearn\n",
    "    X,y = load_breast_cancer(return_X_y=True)\n",
    "\n",
    "    # Split the dataset into training and testing sets\n",
    "    # Using the test size specified in the arguments (default of 20%)\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=test_size)\n",
    "\n",
    "    # Standardize/Normalize the features\n",
    "    # Neural networks perform better with normalized features\n",
    "    scaler = StandardScaler()\n",
    "    X_train = scaler.fit_transform(X_train)\n",
    "    X_test = scaler.transform(X_test)\n",
    "\n",
    "    # Return the data\n",
    "    return X_train, X_test, y_train, y_test\n",
    "\n",
    "# This data set, containing images of hand written digits and the number\n",
    "# being shown, will be used for convolutional neural networks\n",
    "def load_mnist_data():\n",
    "    # Load the MNIST dataset from tensorflow. It is already split into training\n",
    "    # and test data\n",
    "    (X_train, y_train), (X_test, y_test) = mnist.load_data()\n",
    "\n",
    "    # Normalize the images to the range [0, 1] (note that for grayscale\n",
    "    # images pixels range from 0 to 255).\n",
    "    X_train = X_train / 255.0\n",
    "    X_test = X_test / 255.0\n",
    "\n",
    "    # Return the needed data\n",
    "    return X_train, X_test, y_train, y_test\n",
    "\n",
    "# This daat set, which contains the text from movie reviews and rather the review\n",
    "# is positive or negative, will be used for the recurrent neural networks to perform\n",
    "# natural language processing. Note that the data set is already tokenized. The \n",
    "# arguments are the number of unique words allowed across all review and the number of \n",
    "# words allowed in each review.\n",
    "def load_imdb_data(num_words=10000, maxlen=500):\n",
    "    # Load the IMDB dataset from tensorflow\n",
    "    (X_train, y_train), (X_test, y_test) = imdb.load_data(num_words=num_words)\n",
    "\n",
    "    # Pad sequences to ensure uniform input size (i.e. all reviews have exactly\n",
    "    # maxlen words in them). Adds zeros if the review is too short and truncates\n",
    "    # if it is too long.\n",
    "    X_train = pad_sequences(X_train, maxlen=maxlen)\n",
    "    X_test = pad_sequences(X_test, maxlen=maxlen)\n",
    "\n",
    "    # Return the needed data\n",
    "    return X_train, X_test, y_train, y_test\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "58f4c3cb",
   "metadata": {},
   "source": [
    "## Feedforward Neural Networks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "964bdff3",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/butlerju/Library/Python/3.9/lib/python/site-packages/keras/src/layers/core/dense.py:93: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"sequential_6\"</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1mModel: \"sequential_6\"\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
       "┃<span style=\"font-weight: bold\"> Layer (type)                    </span>┃<span style=\"font-weight: bold\"> Output Shape           </span>┃<span style=\"font-weight: bold\">       Param # </span>┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
       "│ dense_13 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)             │         <span style=\"color: #00af00; text-decoration-color: #00af00\">1,984</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_14 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)             │         <span style=\"color: #00af00; text-decoration-color: #00af00\">4,160</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_15 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1</span>)              │            <span style=\"color: #00af00; text-decoration-color: #00af00\">65</span> │\n",
       "└─────────────────────────────────┴────────────────────────┴───────────────┘\n",
       "</pre>\n"
      ],
      "text/plain": [
       "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
       "┃\u001b[1m \u001b[0m\u001b[1mLayer (type)                   \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape          \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m      Param #\u001b[0m\u001b[1m \u001b[0m┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
       "│ dense_13 (\u001b[38;5;33mDense\u001b[0m)                │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m64\u001b[0m)             │         \u001b[38;5;34m1,984\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_14 (\u001b[38;5;33mDense\u001b[0m)                │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m64\u001b[0m)             │         \u001b[38;5;34m4,160\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_15 (\u001b[38;5;33mDense\u001b[0m)                │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m1\u001b[0m)              │            \u001b[38;5;34m65\u001b[0m │\n",
       "└─────────────────────────────────┴────────────────────────┴───────────────┘\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">6,209</span> (24.25 KB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Total params: \u001b[0m\u001b[38;5;34m6,209\u001b[0m (24.25 KB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">6,209</span> (24.25 KB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m6,209\u001b[0m (24.25 KB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Accuracy: 96.49122953414917 %\n"
     ]
    }
   ],
   "source": [
    "####################################################\n",
    "## Feedforward Neural Network in Tensorflow/Keras ##\n",
    "####################################################\n",
    "# Create the model using Keras. It has an input layer of 30 neurons, a first hidden\n",
    "# layer of 64 neurons with a relu activation function, a second hidden layer of 64\n",
    "# neurons and a relu activation function, and an output layer with one neuron and\n",
    "# a sigmoid activation function. This will perform the binary classification.\n",
    "model = Sequential([\n",
    "    Dense(64, activation='relu', input_dim=30),\n",
    "    Dense(64, activation='relu'),\n",
    "    Dense(1, activation='sigmoid')  \n",
    "])\n",
    "\n",
    "# Compile the model using the Adam optimizer, the binary cross entropy loss function\n",
    "# (for binary classification), and display the accuracy when training.\n",
    "model.compile(optimizer=\"adam\",loss='binary_crossentropy',metrics=['accuracy'])\n",
    "\n",
    "# Show model summary\n",
    "model.summary()\n",
    "\n",
    "# Pull the training and test data from the previously defined function.\n",
    "X_train, X_test, y_train, y_test = load_breast_cancer_data()\n",
    "\n",
    "# Train the neural network using 20 epoch, no batch size, no validation split\n",
    "# and no information printed each iteration/epoch\n",
    "model.fit(X_train, y_train, epochs=20, batch_size=None, validation_split=None, \n",
    "          verbose=0)\n",
    "\n",
    "# Test the model and print the test accuracy\n",
    "test_loss, test_acc = model.evaluate(X_test, y_test, verbose=0)\n",
    "print(\"Test Accuracy:\", test_acc*100, \"%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2a5f473",
   "metadata": {},
   "outputs": [],
   "source": [
    "###########################################\n",
    "## Feedforward Nerual Network in PyTorch ##\n",
    "###########################################\n",
    "# Neural networks in PyTorch are created with a class that inherits from the\n",
    "# neural network functionality from PyTorch. There are two ways to create the class.\n",
    "# The first is the most similar to Keras, but also has the least ability to \n",
    "# customize the network.\n",
    "class FNN(nn.Module):\n",
    "    # Define the initialization function of the class. It takes no arguments in\n",
    "    # this case, but in general it can have as many arguments as needed.\n",
    "    def __init__(self):\n",
    "        # Initialize everything that is inherited from the parent class\n",
    "        super().__init__()\n",
    "        # Create a sequential model, this behaves like the Sequential model from\n",
    "        # Keras. We name it self.net so that the variable belongs to the class\n",
    "        # and not the function.\n",
    "        self.net = nn.Sequential(\n",
    "            # This is the first hidden layer, the numbers define the size of\n",
    "            # the weights matrix. 30 is the number of values going into the layer,\n",
    "            # in this case the number of input neurons. 64 is the number of neurons\n",
    "            # in this layer. Linear is a simple feedfoward neural network layer.\n",
    "            nn.Linear(30, 64),\n",
    "            # Add a relu activation function to the output of the first hidden layer.\n",
    "            nn.ReLU(),\n",
    "            # Second hidden layer gets 64 inputs from the first hidden layer and has\n",
    "            # 64 neurons. It also has a relu activation function.\n",
    "            nn.Linear(64, 64),\n",
    "            nn.ReLU(),\n",
    "            # The output layer receives 64 inputs from the second hidden layer and\n",
    "            # needs one neuron to make a binary classification with the sigmoid\n",
    "            # loss function.\n",
    "            nn.Linear(64, 1),\n",
    "            nn.Sigmoid()   \n",
    "        )\n",
    "    # This creates the forward pass of the neural network, which runs data through\n",
    "    # the sequential model in the order it is defined.\n",
    "    def forward(self, x):\n",
    "        return self.net(x)\n",
    "    \n",
    "# The below class makes the same neural network as the above class, but without\n",
    "# using Sequential. This means that the user has more control over how data flows\n",
    "# through the model, but it is more involved.\n",
    "class FNN_V2(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        # Create the layers and activiation functions that will be used to construct\n",
    "        # the network, but none of them are connected at this point.\n",
    "        self.hidden_layer1 = nn.Linear(30, 64)\n",
    "        self.hidden_layer2 = nn.Linear(64, 64)\n",
    "        self.output = nn.Linear(64, 1)\n",
    "\n",
    "        self.relu = nn.ReLU()\n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "\n",
    "    def forward(self, x):\n",
    "        # The forward pass will define how data moves through the network. First into\n",
    "        # the first hidden layer, with a relu activation function, then into the second\n",
    "        # hidden layer with a relu activation function, and finally into the output layer\n",
    "        # with a sigmoid activation function.\n",
    "        x = self.relu(self.hidden_layer1(x))\n",
    "        x = self.relu(self.hidden_layer2(x))\n",
    "        x = self.sigmoid(self.output(x))\n",
    "        return x\n",
    "\n",
    "# Get the training and test data from the function defined above.\n",
    "X_train, X_test, y_train, y_test = load_breast_cancer_data()\n",
    "\n",
    "# Unlike with Keras, we cannot use Numpy arrays for PyTorch. Before we pass\n",
    "# the data to the network we need to convert it ot a torch tensor.\n",
    "X_train = torch.tensor(X_train, dtype=torch.float32)\n",
    "X_test = torch.tensor(X_test, dtype=torch.float32)\n",
    "y_train = torch.tensor(y_train, dtype=torch.float32).unsqueeze(1)\n",
    "y_test = torch.tensor(y_test, dtype=torch.float32).unsqueeze(1)\n",
    "\n",
    "# Create an instance of one of the classes\n",
    "model = FNN()\n",
    "\n",
    "# Define the loss function (binary cross entropy) and the optimizer (Adam)\n",
    "loss_func = nn.BCELoss()\n",
    "optimizer = optim.Adam(model.parameters())\n",
    "\n",
    "# This is the training process. We will have 20 epochs and will train the model\n",
    "# one per epoch, with no batch size or validation data set.\n",
    "epochs = 20\n",
    "for epoch in range(epochs):\n",
    "    # Train the model\n",
    "    model.train()\n",
    "    # Clear the old gradients so they do not affect the current pass\n",
    "    optimizer.zero_grad()\n",
    "    # Get the outputs of the model.\n",
    "    outputs = model(X_train)\n",
    "    # Compute values of the loss function and then do the backpropagation\n",
    "    loss = loss_func(outputs, y_train)\n",
    "    loss.backward()\n",
    "    # Update the parameters of the model according to the backpropagation\n",
    "    optimizer.step()\n",
    "\n",
    "# This will allow us to test the trained model.\n",
    "model.eval()\n",
    "# Use no_grad to not track gradients and make the computation faster\n",
    "with torch.no_grad():\n",
    "    # Pass the test data to the model and get the probabilities\n",
    "    probs = model(X_test)\n",
    "    # If the probability is greater than or equal than 0.5 then predict a 1,\n",
    "    # if lower then predict a 0.\n",
    "    preds = (probs >= 0.5).float()\n",
    "    # Comput the accuracy\n",
    "    accuracy = (preds == y_test).sum() / y_test.size(0)\n",
    "# Print the accuracy as a percent.\n",
    "print(\"Test Accuracy:\", test_acc*100, \"%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "a14ab8c9",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/butlerju/Library/Python/3.9/lib/python/site-packages/keras/src/layers/convolutional/base_conv.py:113: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"sequential_8\"</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1mModel: \"sequential_8\"\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
       "┃<span style=\"font-weight: bold\"> Layer (type)                    </span>┃<span style=\"font-weight: bold\"> Output Shape           </span>┃<span style=\"font-weight: bold\">       Param # </span>┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
       "│ conv2d_4 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv2D</span>)               │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">26</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">26</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>)     │           <span style=\"color: #00af00; text-decoration-color: #00af00\">320</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ max_pooling2d_4 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">MaxPooling2D</span>)  │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">13</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">13</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>)     │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ conv2d_5 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv2D</span>)               │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">11</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">11</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)     │        <span style=\"color: #00af00; text-decoration-color: #00af00\">18,496</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ max_pooling2d_5 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">MaxPooling2D</span>)  │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">5</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">5</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)       │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ flatten_2 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Flatten</span>)             │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1600</span>)           │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_18 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)            │       <span style=\"color: #00af00; text-decoration-color: #00af00\">204,928</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_19 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">10</span>)             │         <span style=\"color: #00af00; text-decoration-color: #00af00\">1,290</span> │\n",
       "└─────────────────────────────────┴────────────────────────┴───────────────┘\n",
       "</pre>\n"
      ],
      "text/plain": [
       "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
       "┃\u001b[1m \u001b[0m\u001b[1mLayer (type)                   \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape          \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m      Param #\u001b[0m\u001b[1m \u001b[0m┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
       "│ conv2d_4 (\u001b[38;5;33mConv2D\u001b[0m)               │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m26\u001b[0m, \u001b[38;5;34m26\u001b[0m, \u001b[38;5;34m32\u001b[0m)     │           \u001b[38;5;34m320\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ max_pooling2d_4 (\u001b[38;5;33mMaxPooling2D\u001b[0m)  │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m13\u001b[0m, \u001b[38;5;34m13\u001b[0m, \u001b[38;5;34m32\u001b[0m)     │             \u001b[38;5;34m0\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ conv2d_5 (\u001b[38;5;33mConv2D\u001b[0m)               │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m11\u001b[0m, \u001b[38;5;34m11\u001b[0m, \u001b[38;5;34m64\u001b[0m)     │        \u001b[38;5;34m18,496\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ max_pooling2d_5 (\u001b[38;5;33mMaxPooling2D\u001b[0m)  │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m5\u001b[0m, \u001b[38;5;34m5\u001b[0m, \u001b[38;5;34m64\u001b[0m)       │             \u001b[38;5;34m0\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ flatten_2 (\u001b[38;5;33mFlatten\u001b[0m)             │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m1600\u001b[0m)           │             \u001b[38;5;34m0\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_18 (\u001b[38;5;33mDense\u001b[0m)                │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m128\u001b[0m)            │       \u001b[38;5;34m204,928\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_19 (\u001b[38;5;33mDense\u001b[0m)                │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m10\u001b[0m)             │         \u001b[38;5;34m1,290\u001b[0m │\n",
       "└─────────────────────────────────┴────────────────────────┴───────────────┘\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">225,034</span> (879.04 KB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Total params: \u001b[0m\u001b[38;5;34m225,034\u001b[0m (879.04 KB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">225,034</span> (879.04 KB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m225,034\u001b[0m (879.04 KB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Accuracy: 97.71999716758728 %\n"
     ]
    }
   ],
   "source": [
    "######################################################\n",
    "## Convolutional Neural Network in Tensorflow/Keras ##\n",
    "######################################################\n",
    "# Create a convolutional neural network (CNN) using keras to predict the values \n",
    "# shown in the images of the MNIST data set.\n",
    "model = Sequential([\n",
    "    # Start with a convolutional layer with 32 filters, a 3x3 kernel size, and a \n",
    "    # relu activation function which receives images from the input layer that \n",
    "    # are 28x28 and are grayscale (1 value per pixel). The convolutional layer is \n",
    "    # followed by a max pooling layer with a pool size of 2x2.\n",
    "    Conv2D(32, kernel_size=(3, 3), activation=\"relu\", input_shape=(28, 28,1)),\n",
    "    MaxPooling2D(pool_size=(2, 2)),\n",
    "\n",
    "    # This is followed by another set of convolutional and pooling layers, but in \n",
    "    # this case the convolutional layer as 64 filters.\n",
    "    Conv2D(64, kernel_size=(3, 3), activation=\"relu\"),\n",
    "    MaxPooling2D(pool_size=(2, 2)),\n",
    "\n",
    "    # Flatten the data as it is currently two dimensional but needs to be one \n",
    "    # dimensional to be processed by the dense layers.\n",
    "    Flatten(),\n",
    "    # Add a post-processing dense layer with 128 neurons and a relu activation\n",
    "    # function. Then an output layer with 10 neurons (10 classes) and a softmax\n",
    "    # activation function.\n",
    "    Dense(128, activation=\"relu\"),\n",
    "    Dense(10, activation=\"softmax\")\n",
    "])\n",
    "\n",
    "# Compile the model with the Adam optimizer, the sparse categorical cross entropy\n",
    "# loss function, and show the accuracy during training.\n",
    "model.compile(optimizer=\"adam\",loss=\"sparse_categorical_crossentropy\",\n",
    "    metrics=[\"accuracy\"])\n",
    "\n",
    "# Print a summary of the model.\n",
    "model.summary()\n",
    "\n",
    "# Retrieve the training and test data from the previously defined function\n",
    "X_train, X_test, y_train, y_test = load_mnist_data()\n",
    "\n",
    "# Truncate the size of the training data to reduce the number of images and thus\n",
    "# the training time\n",
    "X_train = X_train[:10000]\n",
    "y_train = y_train[:10000]\n",
    "\n",
    "# Fit the neural network with the training data, 5 epochs, no batch size, and no \n",
    "# validation split.\n",
    "model.fit(X_train, y_train, epochs=5, batch_size=None, validation_split=None, verbose=0)\n",
    "\n",
    "# Create predictions using the test data and print the test accuracy\n",
    "test_loss, test_acc = model.evaluate(X_test, y_test, verbose=0)\n",
    "print(\"Test Accuracy:\", test_acc*100, \"%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e59b5b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "#############################################\n",
    "## Convolutional Neural Network in PyTorch ##\n",
    "#############################################\n",
    "# Create a class to recreate the above Keras CNN using PyTorch syntax.\n",
    "class CNN(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.model = nn.Sequential(\n",
    "            nn.Conv2d(1, 32, kernel_size=3),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(2),\n",
    "\n",
    "            nn.Conv2d(32, 64, kernel_size=3),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(2),\n",
    "\n",
    "            nn.Flatten(),\n",
    "            # 64*5*5 is how big the images are at the end as they are now\n",
    "            # three dimensional tensors. This can be figured out with \n",
    "            # mathematics or by printing a summary of the model with just the\n",
    "            # CNN layers.\n",
    "            nn.Linear(64 * 5 * 5, 128),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(128, 10)   # logits\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.model(x)\n",
    "    \n",
    "# Load data\n",
    "X_train, X_test, y_train, y_test = load_mnist_data()\n",
    "\n",
    "X_train = X_train[:10000]\n",
    "y_train = y_train[:10000]\n",
    "\n",
    "# Convert to PyTorch tensors and add channel dimension\n",
    "X_train = torch.tensor(X_train).unsqueeze(1)  # (N, 1, 28, 28)\n",
    "X_test  = torch.tensor(X_test).unsqueeze(1)\n",
    "\n",
    "y_train = torch.tensor(y_train, dtype=torch.long)\n",
    "y_test  = torch.tensor(y_test, dtype=torch.long)\n",
    "\n",
    "model = CNN()\n",
    "\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters())\n",
    "\n",
    "epochs = 5\n",
    "for epoch in range(epochs):\n",
    "    model.train()\n",
    "    optimizer.zero_grad()\n",
    "\n",
    "    outputs = model(X_train)\n",
    "    loss = criterion(outputs, y_train)\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    outputs = model(X_test)\n",
    "    preds = torch.argmax(outputs, dim=1)\n",
    "    accuracy = (preds == y_test).float().mean()\n",
    "\n",
    "print(\"Test Accuracy:\", accuracy*100, \"%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a3be4f0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test accuracy: 0.691\n"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "a7832299",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/butlerju/Library/Python/3.9/lib/python/site-packages/keras/src/layers/core/embedding.py:97: UserWarning: Argument `input_length` is deprecated. Just remove it.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"sequential_4\"</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1mModel: \"sequential_4\"\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
       "┃<span style=\"font-weight: bold\"> Layer (type)                    </span>┃<span style=\"font-weight: bold\"> Output Shape           </span>┃<span style=\"font-weight: bold\">       Param # </span>┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
       "│ embedding_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Embedding</span>)         │ ?                      │   <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (unbuilt) │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ lstm_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">LSTM</span>)                   │ ?                      │   <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (unbuilt) │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_9 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                 │ ?                      │   <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (unbuilt) │\n",
       "└─────────────────────────────────┴────────────────────────┴───────────────┘\n",
       "</pre>\n"
      ],
      "text/plain": [
       "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
       "┃\u001b[1m \u001b[0m\u001b[1mLayer (type)                   \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape          \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m      Param #\u001b[0m\u001b[1m \u001b[0m┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
       "│ embedding_1 (\u001b[38;5;33mEmbedding\u001b[0m)         │ ?                      │   \u001b[38;5;34m0\u001b[0m (unbuilt) │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ lstm_1 (\u001b[38;5;33mLSTM\u001b[0m)                   │ ?                      │   \u001b[38;5;34m0\u001b[0m (unbuilt) │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_9 (\u001b[38;5;33mDense\u001b[0m)                 │ ?                      │   \u001b[38;5;34m0\u001b[0m (unbuilt) │\n",
       "└─────────────────────────────────┴────────────────────────┴───────────────┘\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Total params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "##################################################\n",
    "## Recurrent Neural Network in Tensorflow/Keras ##\n",
    "##################################################\n",
    "\n",
    "num_words = 1000\n",
    "maxlen = 100\n",
    "X_train, X_test, y_train, y_test = load_imdb_data(num_words, maxlen)\n",
    "\n",
    "model = Sequential([\n",
    "    Embedding(input_dim=num_words, output_dim=128, input_length=maxlen),\n",
    "    LSTM(64),\n",
    "    Dense(1, activation='sigmoid')\n",
    "])\n",
    "\n",
    "model.compile(\n",
    "    optimizer='adam',\n",
    "    loss='binary_crossentropy',\n",
    "    metrics=['accuracy']\n",
    ")\n",
    "\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d739dc6f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n",
      "\u001b[1m782/782\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 11ms/step - accuracy: 0.7084 - loss: 0.5480\n",
      "Epoch 2/5\n",
      "\u001b[1m782/782\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 11ms/step - accuracy: 0.8271 - loss: 0.3825\n",
      "Epoch 3/5\n",
      "\u001b[1m782/782\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 12ms/step - accuracy: 0.8530 - loss: 0.3386\n",
      "Epoch 4/5\n",
      "\u001b[1m782/782\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 11ms/step - accuracy: 0.8679 - loss: 0.3050\n",
      "Epoch 5/5\n",
      "\u001b[1m782/782\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 11ms/step - accuracy: 0.8775 - loss: 0.2938\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.src.callbacks.history.History at 0x3c6a2cfd0>"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(X_train, y_train,epochs=5,batch_size=None,validation_split=None,verbose=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "11feddba",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test accuracy: 0.841\n"
     ]
    }
   ],
   "source": [
    "test_loss, test_acc = model.evaluate(X_test, y_test, verbose=0)\n",
    "print(f\"Test accuracy: {test_acc:.3f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "deaf4688",
   "metadata": {},
   "outputs": [],
   "source": [
    "#########################################\n",
    "## Recurrent Neural Network in PyTorch ##\n",
    "#########################################\n",
    "class IMDBRNN(nn.Module):\n",
    "    def __init__(self, vocab_size, embed_dim=128, hidden_dim=64):\n",
    "        super().__init__()\n",
    "        self.embedding = nn.Embedding(vocab_size, embed_dim)\n",
    "        self.lstm = nn.LSTM(embed_dim, hidden_dim, batch_first=True)\n",
    "        self.fc = nn.Linear(hidden_dim, 1)\n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.embedding(x)           # (batch, seq_len, embed_dim)\n",
    "        _, (h_n, _) = self.lstm(x)      # final hidden state\n",
    "        x = h_n[-1]                     # (batch, hidden_dim)\n",
    "        x = self.sigmoid(self.fc(x))    # probability\n",
    "        return x\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "ede70914",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load data\n",
    "num_words = 1000\n",
    "maxlen = 100\n",
    "X_train, X_test, y_train, y_test = load_imdb_data(num_words, maxlen)\n",
    "\n",
    "# Convert to PyTorch tensors\n",
    "X_train = torch.tensor(X_train, dtype=torch.long)\n",
    "X_test  = torch.tensor(X_test, dtype=torch.long)\n",
    "\n",
    "y_train = torch.tensor(y_train, dtype=torch.float32).unsqueeze(1)\n",
    "y_test  = torch.tensor(y_test, dtype=torch.float32).unsqueeze(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "46620625",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/5], Loss: 0.6940\n",
      "Epoch [2/5], Loss: 0.6920\n",
      "Epoch [3/5], Loss: 0.6901\n",
      "Epoch [4/5], Loss: 0.6883\n",
      "Epoch [5/5], Loss: 0.6865\n"
     ]
    }
   ],
   "source": [
    "model = IMDBRNN(num_words)\n",
    "\n",
    "criterion = nn.BCELoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "epochs = 5\n",
    "for epoch in range(epochs):\n",
    "    model.train()\n",
    "\n",
    "    optimizer.zero_grad()\n",
    "    outputs = model(X_train)\n",
    "    loss = criterion(outputs, y_train)\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "    print(f\"Epoch [{epoch+1}/{epochs}], Loss: {loss.item():.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "8f2f0129",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test accuracy: 0.558\n"
     ]
    }
   ],
   "source": [
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    outputs = model(X_test)\n",
    "    preds = (outputs >= 0.5).float()\n",
    "    accuracy = (preds == y_test).float().mean()\n",
    "\n",
    "print(f\"Test accuracy: {accuracy.item():.3f}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
